<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2ec44e43e07040f7a7efefc16255e17a</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="dele-ca1-part-b" class="cell markdown">
<h1><strong>DELE CA1 Part B</strong></h1>
<p>Name: Lim Zhen Yang</p>
<p>Admission Number: 2214506</p>
<p>Class: DAAA/FT/2B/04</p>
</section>
<section id="imports" class="cell markdown">
<h1><strong>IMPORTS</strong></h1>
</section>
<div class="cell code" data-execution_count="465">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> K</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> to_categorical</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential, Model, load_model</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.activations <span class="im">import</span> softmax</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> (</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    Embedding,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    Reshape,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    Input,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    SimpleRNN,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    Dense,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    LSTM,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    GRU,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    Dropout,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    Bidirectional,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    TextVectorization,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    MultiHeadAttention,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    LayerNormalization,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    Layer,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.callbacks <span class="im">import</span> (</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    ModelCheckpoint,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    EarlyStopping,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    ReduceLROnPlateau,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    TensorBoard,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    LearningRateScheduler,</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras_tuner <span class="im">import</span> RandomSearch</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras.optimizers <span class="im">as</span> optimizers</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras_preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> wordnet</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tag <span class="im">import</span> pos_tag</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="co"># from keras_nlp.metrics import Perplexity</span></span></code></pre></div>
</div>
<div class="cell markdown">
<p>Check and limit GPU usage</p>
</div>
<div class="cell code" data-execution_count="297">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if TensorFlow can access GPUs</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> tf.test.gpu_device_name():</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Default GPU Device: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(tf.test.gpu_device_name()))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    gpus <span class="op">=</span> tf.config.experimental.list_physical_devices(<span class="st">&quot;GPU&quot;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gpus:</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Currently, memory growth needs to be the same across GPUs</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> gpu <span class="kw">in</span> gpus:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                tf.config.experimental.set_memory_growth(gpu, <span class="va">True</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            logical_gpus <span class="op">=</span> tf.config.experimental.list_logical_devices(<span class="st">&quot;GPU&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="bu">len</span>(gpus), <span class="st">&quot;Physical GPUs,&quot;</span>, <span class="bu">len</span>(logical_gpus), <span class="st">&quot;Logical GPUs&quot;</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Memory growth must be set before GPUs have been initialized</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(e)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No GPU devices found. TensorFlow will use CPU.&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>No GPU devices found. TensorFlow will use CPU.
</code></pre>
</div>
</div>
<section id="background-research" class="cell markdown">
<h1><strong>BACKGROUND RESEARCH</strong></h1>
</section>
<section id="breakthroughs-in-nlp" class="cell markdown">
<h3>Breakthroughs in NLP</h3>
<p>1990s: Sepp Hochreiter and Jurgen Schmidhuber's introduction of Long
Short-Term Memory (LSTM) networks addressed limitations in RNNs,
significantly enhancing the AI's ability to analyze longer text
sequences (TechTarget, 2023).</p>
<p>2000s: Landmark papers by teams including those at Bell
Communications and the University of Montreal introduced techniques like
Latent Semantic Analysis and neural probabilistic language models,
setting the stage for current deep learning approaches (TechTarget,
2023).</p>
<p>2010s: The advent of transformer models, particularly Google's
development of the "Attention is all you need" paper and BERT
(Bidirectional Encoder Representations from Transformers),
revolutionized text generation, offering better context understanding
and efficiency (TechTarget, 2023).</p>
<p>OpenAI's GPT Series: Starting with GPT in 2018, followed by more
advanced versions like GPT-3 and GPT-4, these models have become the
cornerstone of modern text generation, with applications in generating
coherent and contextually relevant text (TechTarget, 2023).</p>
<p>Reference:</p>
<p>TechTarget. (2023). History of generative AI innovations spans 9
decades. Retrieved from <a
href="https://www.techtarget.com/searchenterpriseai/tip/History-of-generative-AI-innovations-spans-9-decades"
class="uri">https://www.techtarget.com/searchenterpriseai/tip/History-of-generative-AI-innovations-spans-9-decades</a></p>
</section>
<section id="limitations-of-text-generation" class="cell markdown">
<h3>Limitations of Text Generation</h3>
<p>Lack of contextual understanding: Text generation models often
struggle with comprehending the broader context and nuances of language.
They generate text based on patterns in the training data without truly
understanding the meaning or intent behind the words. This can lead to
inaccuracies, ambiguity, or nonsensical outputs (DataCamp, 2023).</p>
<p>Overreliance on training data: Text generation models heavily rely on
the quality and diversity of the training data they are exposed to. If
the training data is limited, biased, or doesn't represent the full
range of language variations, the generated text may be biased, lack
diversity, or exhibit other shortcomings (DataCamp, 2023).</p>
<p>Difficulty in handling rare or unseen scenarios: Text generation
models may struggle when faced with uncommon or rare scenarios that were
not well-represented in the training data. They may produce incorrect or
nonsensical responses when encountering unfamiliar or out-of-context
inputs (DataCamp, 2023).</p>
<p>Ethical considerations: Text generation raises ethical concerns,
particularly in relation to misinformation, propaganda, or generating
harmful content. If not carefully monitored and guided, text generation
models can be misused to spread misinformation, amplify biases, or
engage in malicious activities (DataCamp, 2023).</p>
<p>Reference:</p>
<p>DataCamp. (2023). What Is Text Generation? [Blog post]. Retrieved
from <a href="https://www.datacamp.com/blog/what-is-text-generation"
class="uri">https://www.datacamp.com/blog/what-is-text-generation</a></p>
</section>
<section id="exploratory-data-analysis" class="cell markdown">
<h1><strong>EXPLORATORY DATA ANALYSIS</strong></h1>
</section>
<div class="cell markdown">
<p>Reading dataset with <code>pd.read_csv()</code></p>
</div>
<div class="cell code" data-execution_count="298">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>train_csv <span class="op">=</span> pd.read_csv(<span class="st">&quot;./train.csv&quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>train_csv.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="298">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Quotes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Embrace the beauty of every sunrise; it's a fr...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Embrace challenges; they are the stepping ston...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Embrace the rhythm of life and let it dance th...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Embrace kindness, for it has the power to chan...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Embrace the journey, for it leads to the desti...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>Checking for null values</p>
<blockquote>
<p>There are no null values</p>
</blockquote>
</div>
<div class="cell code" data-execution_count="299">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>train_csv.info()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1000 entries, 0 to 999
Data columns (total 1 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   Quotes  1000 non-null   object
dtypes: object(1)
memory usage: 7.9+ KB
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Generating descriptive statistics</p>
<ol>
<li>There are 1000 datapoints</li>
<li>890 of them are unique indicating there are duplicated quotes</li>
<li>The most repeated quote was repeated 5 times</li>
</ol>
<p>Repeated text data can introduce <strong>bias</strong> into the
model. If certain phrases or sentences are overrepresented due to
repetition, the model might learn to favor them and generate similar
outputs excessively.</p>
<p>I want my text generation model to produce diverse and creative
outputs, thus removing repeated data can be beneficial.</p>
<p>FInally, removing repeated data can reduce the training time since
the model won't have to process the same text multiple times.</p>
<p>Thus I will be <strong>removing</strong> repeated quotes</p>
</div>
<div class="cell code" data-execution_count="300">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train_csv.describe()</span></code></pre></div>
<div class="output execute_result" data-execution_count="300">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Quotes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>890</td>
    </tr>
    <tr>
      <th>top</th>
      <td>Radiate acceptance, and find peace in embracin...</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="length-of-quotes" class="cell markdown">
<h3><strong>Length of quotes</strong></h3>
<p>Most quotes have around 68.94 words which tells me that my model does
not need extremely long memory to generate good predictions because
shorter quotes are preferable.</p>
</section>
<div class="cell code" data-execution_count="301">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>num_quotes <span class="op">=</span> <span class="bu">len</span>(train_csv)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>sentence_lengths <span class="op">=</span> train_csv[<span class="st">&quot;Quotes&quot;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>avg_quote_length <span class="op">=</span> sentence_lengths.mean()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Number of quotes: </span><span class="sc">{</span>num_quotes<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average quote length: </span><span class="sc">{</span>avg_quote_length<span class="sc">:.2f}</span><span class="ss"> words&quot;</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.hist(sentence_lengths, bins<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">&quot;#6fc6ab&quot;</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Sentence Length (words)&quot;</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Frequency&quot;</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Sentence Length Distribution&quot;</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Number of quotes: 1000
Average quote length: 68.94 words
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/25ae1652bfa65006019272196d65cae049baf8b5.png" /></p>
</div>
</div>
<section id="word-frequency-analysis" class="cell markdown">
<h3><strong>Word Frequency Analysis</strong></h3>
<p>The distribution of word and phrase frequencies follows common
linguistic patterns e.g. <code>the</code> is the most commonly used word
in this dataset and in the whole english corpus.</p>
<p>Therefore this dataset reflects real world use cases and
frequencies.</p>
</section>
<div class="cell code" data-execution_count="302">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(train_csv[<span class="st">&quot;Quotes&quot;</span>])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>word_count <span class="op">=</span> pd.DataFrame(tokenizer.word_counts.items(), columns<span class="op">=</span>[<span class="st">&quot;word&quot;</span>, <span class="st">&quot;count&quot;</span>])</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>top_20_word_counts <span class="op">=</span> word_count.sort_values(<span class="st">&quot;count&quot;</span>, ascending<span class="op">=</span><span class="va">False</span>)[:<span class="dv">20</span>]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&quot;Top 20 Word Frequencies&quot;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>ax.tick_params(axis<span class="op">=</span><span class="st">&quot;x&quot;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>sns.barplot(</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>top_20_word_counts[<span class="st">&quot;word&quot;</span>],</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>top_20_word_counts[<span class="st">&quot;count&quot;</span>],</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    ax<span class="op">=</span>ax,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">&quot;#6fc6ab&quot;</span>,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/598be21925f066a20e23e3dbdd2bf4f319476d54.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>The least common words occur only once in the whole dataset. The lack
of examples for these words will make it difficult for the model to
preidct them. We can make use of data augmentation to help.</p>
</div>
<div class="cell code" data-execution_count="303">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(train_csv[<span class="st">&quot;Quotes&quot;</span>])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>word_count <span class="op">=</span> pd.DataFrame(tokenizer.word_counts.items(), columns<span class="op">=</span>[<span class="st">&quot;word&quot;</span>, <span class="st">&quot;count&quot;</span>])</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>bottom_20_word_counts <span class="op">=</span> word_count.sort_values(<span class="st">&quot;count&quot;</span>, ascending<span class="op">=</span><span class="va">False</span>)[<span class="op">-</span><span class="dv">20</span>:]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&quot;Bottom 20 Word Frequencies&quot;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>ax.tick_params(axis<span class="op">=</span><span class="st">&quot;x&quot;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>sns.barplot(</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>bottom_20_word_counts[<span class="st">&quot;word&quot;</span>],</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>bottom_20_word_counts[<span class="st">&quot;count&quot;</span>],</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    ax<span class="op">=</span>ax,</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">&quot;#6fc6ab&quot;</span>,</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/12013c3ca920119ce7d43581243a75681436b52f.png" /></p>
</div>
</div>
<section id="n-gram-analysis" class="cell markdown">
<h3><strong>N-gram Analysis</strong></h3>
<p>Certain phrases like "morning let" are particularly prevalent.</p>
<p>The data seems to frequently include references to locations or
themes consistent with Singapore ("lion city", "singapore skyline",
"singapore nature"). In these cases, we can create additional tokens by
combining words frequently found together, phrase chunking.</p>
</section>
<div class="cell code" data-execution_count="304">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to extract n-grams</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_top_ngrams(corpus, n<span class="op">=</span><span class="va">None</span>, top_k<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    vec <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(n, n), stop_words<span class="op">=</span><span class="st">&quot;english&quot;</span>).fit(corpus)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    bag_of_words <span class="op">=</span> vec.transform(corpus)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    sum_words <span class="op">=</span> bag_of_words.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    words_freq <span class="op">=</span> [(word, sum_words[<span class="dv">0</span>, idx]) <span class="cf">for</span> word, idx <span class="kw">in</span> vec.vocabulary_.items()]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    words_freq <span class="op">=</span> <span class="bu">sorted</span>(words_freq, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> words_freq[:top_k]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracting bigrams</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>top_bigrams <span class="op">=</span> get_top_ngrams(train_csv[<span class="st">&quot;Quotes&quot;</span>], <span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracting trigrams</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>top_trigrams <span class="op">=</span> get_top_ngrams(train_csv[<span class="st">&quot;Quotes&quot;</span>], <span class="dv">3</span>, <span class="dv">10</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the results into DataFrames</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>df_bigrams <span class="op">=</span> pd.DataFrame(top_bigrams, columns<span class="op">=</span>[<span class="st">&quot;Bigram&quot;</span>, <span class="st">&quot;Frequency&quot;</span>])</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>df_trigrams <span class="op">=</span> pd.DataFrame(top_trigrams, columns<span class="op">=</span>[<span class="st">&quot;Trigram&quot;</span>, <span class="st">&quot;Frequency&quot;</span>])</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine DataFrames side by side with a gap in the middle</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>combined_df <span class="op">=</span> pd.concat([df_bigrams, df_trigrams], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the combined DataFrame</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>combined_df</span></code></pre></div>
<div class="output execute_result" data-execution_count="304">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Bigram</th>
      <th>Frequency</th>
      <th>Trigram</th>
      <th>Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>morning let</td>
      <td>52</td>
      <td>morning let actions</td>
      <td>13</td>
    </tr>
    <tr>
      <th>1</th>
      <td>embrace power</td>
      <td>25</td>
      <td>heart lion city</td>
      <td>11</td>
    </tr>
    <tr>
      <th>2</th>
      <td>let heart</td>
      <td>21</td>
      <td>morning let kindness</td>
      <td>11</td>
    </tr>
    <tr>
      <th>3</th>
      <td>let kindness</td>
      <td>20</td>
      <td>let heart lead</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>let actions</td>
      <td>17</td>
      <td>heart lead way</td>
      <td>8</td>
    </tr>
    <tr>
      <th>5</th>
      <td>life garden</td>
      <td>15</td>
      <td>life garden flourishes</td>
      <td>8</td>
    </tr>
    <tr>
      <th>6</th>
      <td>dance symphony</td>
      <td>15</td>
      <td>dance chapters growth</td>
      <td>8</td>
    </tr>
    <tr>
      <th>7</th>
      <td>lion city</td>
      <td>15</td>
      <td>tapestry woven threads</td>
      <td>6</td>
    </tr>
    <tr>
      <th>8</th>
      <td>singapore skyline</td>
      <td>15</td>
      <td>singapore nature sanctuary</td>
      <td>6</td>
    </tr>
    <tr>
      <th>9</th>
      <td>singapore nature</td>
      <td>15</td>
      <td>morning let gratitude</td>
      <td>6</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="data-preparation" class="cell markdown">
<h1><strong>DATA PREPARATION</strong></h1>
<p>Punctuation is very important in quotes. Conventionally, tokenizers
filter our special characters. However I am going to keep them as our
model has to learn when to punctuate as well.</p>
</section>
<div class="cell code" data-execution_count="305">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert df to a list</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="bu">list</span>(train_csv[<span class="st">&quot;Quotes&quot;</span>].values)</span></code></pre></div>
</div>
<div class="cell markdown">
<p>To make sure that the punctuation is recognized as its own token, we
need a seperator between the word and the punctuation. A naive approach
is to replace the punctuation in the text with a space +
punctuation.</p>
<p>If you tokenize the example sentence the result would take "cold." as
a token instead of "cold" and ".".</p>
</div>
<div class="cell code" data-execution_count="306">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_space_before_punctuation(data):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    clean_data <span class="op">=</span> []</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text <span class="kw">in</span> data:</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> char <span class="kw">in</span> string.punctuation:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>            text <span class="op">=</span> text.replace(char, <span class="ss">f&#39; </span><span class="sc">{</span>char<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        clean_data.append(text)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clean_data</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>clean_data <span class="op">=</span> add_space_before_punctuation(data)</span></code></pre></div>
</div>
<section id="tokenization" class="cell markdown">
<h3><strong>Tokenization</strong></h3>
</section>
<div class="cell code" data-execution_count="307">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenisation.</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(filters<span class="op">=</span><span class="st">&#39;&#39;</span>)  <span class="co"># Instantiate a Tokenizer object.</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(clean_data)  <span class="co"># Fit your Tokenizer on the dataset</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>tokenizer.word_index</span></code></pre></div>
<div class="output execute_result" data-execution_count="307">
<pre><code>{&#39;.&#39;: 1,
 &#39;the&#39;: 2,
 &#39;of&#39;: 3,
 &#39;,&#39;: 4,
 &#39;your&#39;: 5,
 &#39;and&#39;: 6,
 &#39;a&#39;: 7,
 &quot;&#39;s&quot;: 8,
 &#39;in&#39;: 9,
 &#39;is&#39;: 10,
 &#39;for&#39;: 11,
 &#39;let&#39;: 12,
 &#39;be&#39;: 13,
 &#39;it&#39;: 14,
 &#39;to&#39;: 15,
 &#39;life&#39;: 16,
 &#39;every&#39;: 17,
 &#39;you&#39;: 18,
 &#39;our&#39;: 19,
 &#39;that&#39;: 20,
 &#39;dance&#39;: 21,
 &#39;embrace&#39;: 22,
 &#39;through&#39;: 23,
 &#39;this&#39;: 24,
 &#39;with&#39;: 25,
 &#39;believe&#39;: 26,
 &#39;are&#39;: 27,
 &#39;radiate&#39;: 28,
 &#39;yourself&#39;: 29,
 &#39;morning&#39;: 30,
 &#39;heart&#39;: 31,
 &#39;planet&#39;: 32,
 &#39;singapore&#39;: 33,
 &#39;will&#39;: 34,
 &#39;they&#39;: 35,
 &#39;love&#39;: 36,
 &#39;kindness&#39;: 37,
 &#39;dreams&#39;: 38,
 &#39;symphony&#39;: 39,
 &#39;we&#39;: 40,
 &#39;power&#39;: 41,
 &#39;from&#39;: 42,
 &#39;soul&#39;: 43,
 &#39;;&#39;: 44,
 &#39;find&#39;: 45,
 &#39;world&#39;: 46,
 &#39;gratitude&#39;: 47,
 &#39;light&#39;: 48,
 &#39;act&#39;: 49,
 &#39;nature&#39;: 50,
 &#39;journey&#39;: 51,
 &#39;strength&#39;: 52,
 &#39;joy&#39;: 53,
 &#39;beauty&#39;: 54,
 &#39;way&#39;: 55,
 &#39;canvas&#39;: 56,
 &#39;colors&#39;: 57,
 &#39;whispers&#39;: 58,
 &#39;hope&#39;: 59,
 &#39;spirit&#39;: 60,
 &#39;where&#39;: 61,
 &#39;potential&#39;: 62,
 &#39;resilience&#39;: 63,
 &#39;beacon&#39;: 64,
 &#39;testament&#39;: 65,
 &#39;true&#39;: 66,
 &#39;towards&#39;: 67,
 &#39;hold&#39;: 68,
 &#39;future&#39;: 69,
 &#39;compassion&#39;: 70,
 &#39;hearts&#39;: 71,
 &#39;garden&#39;: 72,
 &#39;step&#39;: 73,
 &#39;change&#39;: 74,
 &#39;promise&#39;: 75,
 &#39;new&#39;: 76,
 &#39;heartbeat&#39;: 77,
 &#39;day&#39;: 78,
 &#39;actions&#39;: 79,
 &#39;wisdom&#39;: 80,
 &#39;moments&#39;: 81,
 &#39;destiny&#39;: 82,
 &#39;force&#39;: 83,
 &#39;others&#39;: 84,
 &#39;story&#39;: 85,
 &#39;compass&#39;: 86,
 &#39;tapestry&#39;: 87,
 &#39;self&#39;: 88,
 &#39;sunrise&#39;: 89,
 &#39;laughter&#39;: 90,
 &#39;legacy&#39;: 91,
 &#39;holds&#39;: 92,
 &#39;chapters&#39;: 93,
 &#39;growth&#39;: 94,
 &#39;an&#39;: 95,
 &#39;gift&#39;: 96,
 &#39;inner&#39;: 97,
 &#39;determination&#39;: 98,
 &#39;on&#39;: 99,
 &#39;purpose&#39;: 100,
 &#39;peace&#39;: 101,
 &#39;each&#39;: 102,
 &#39;city&#39;: 103,
 &#39;melody&#39;: 104,
 &#39;into&#39;: 105,
 &#39;authenticity&#39;: 106,
 &#39;when&#39;: 107,
 &#39;aspirations&#39;: 108,
 &#39;lion&#39;: 109,
 &#39;skyline&#39;: 110,
 &#39;paint&#39;: 111,
 &#39;rhythm&#39;: 112,
 &#39;seeds&#39;: 113,
 &#39;any&#39;: 114,
 &#39;challenge&#39;: 115,
 &#39;tranquility&#39;: 116,
 &#39;leave&#39;: 117,
 &#39;moment&#39;: 118,
 &#39;forgiveness&#39;: 119,
 &#39;empathy&#39;: 120,
 &#39;guiding&#39;: 121,
 &#39;bridge&#39;: 122,
 &#39;opportunities&#39;: 123,
 &#39;watch&#39;: 124,
 &#39;rain&#39;: 125,
 &#39;within&#39;: 126,
 &#39;possibility&#39;: 127,
 &#39;foundation&#39;: 128,
 &#39;serenity&#39;: 129,
 &#39;brings&#39;: 130,
 &#39;open&#39;: 131,
 &#39;universe&#39;: 132,
 &#39;music&#39;: 133,
 &#39;can&#39;: 134,
 &#39;reality&#39;: 135,
 &#39;connection&#39;: 136,
 &#39;overcome&#39;: 137,
 &#39;create&#39;: 138,
 &#39;guides&#39;: 139,
 &#39;path&#39;: 140,
 &#39;progress&#39;: 141,
 &#39;vibrant&#39;: 142,
 &#39;threads&#39;: 143,
 &#39;breath&#39;: 144,
 &#39;us&#39;: 145,
 &#39;blessings&#39;: 146,
 &#39;success&#39;: 147,
 &#39;beginnings&#39;: 148,
 &#39;treasure&#39;: 149,
 &#39;lead&#39;: 150,
 &#39;have&#39;: 151,
 &#39;shape&#39;: 152,
 &#39;positivity&#39;: 153,
 &#39;confidence&#39;: 154,
 &#39;carries&#39;: 155,
 &#39;star&#39;: 156,
 &#39;nurtured&#39;: 157,
 &#39;reminder&#39;: 158,
 &#39;challenges&#39;: 159,
 &#39;has&#39;: 160,
 &#39;joyful&#39;: 161,
 &#39;up&#39;: 162,
 &#39;what&#39;: 163,
 &#39;more&#39;: 164,
 &#39;magic&#39;: 165,
 &#39;transform&#39;: 166,
 &#39;transformation&#39;: 167,
 &#39;brighter&#39;: 168,
 &#39;masterpiece&#39;: 169,
 &#39;behind&#39;: 170,
 &#39;courage&#39;: 171,
 &#39;echoes&#39;: 172,
 &#39;forward&#39;: 173,
 &#39;shine&#39;: 174,
 &#39;source&#39;: 175,
 &#39;painted&#39;: 176,
 &#39;by&#39;: 177,
 &#39;generosity&#39;: 178,
 &#39;opportunity&#39;: 179,
 &#39;park&#39;: 180,
 &#39;hear&#39;: 181,
 &#39;conservation&#39;: 182,
 &#39;fresh&#39;: 183,
 &#39;key&#39;: 184,
 &#39;how&#39;: 185,
 &#39;carry&#39;: 186,
 &#39;smile&#39;: 187,
 &#39;its&#39;: 188,
 &#39;wind&#39;: 189,
 &#39;adventure&#39;: 190,
 &#39;humanity&#39;: 191,
 &#39;passions&#39;: 192,
 &#39;diversity&#39;: 193,
 &#39;touch&#39;: 194,
 &#39;possibilities&#39;: 195,
 &#39;around&#39;: 196,
 &#39;sanctuary&#39;: 197,
 &#39;boundless&#39;: 198,
 &#39;storms&#39;: 199,
 &#39;take&#39;: 200,
 &#39;flourishes&#39;: 201,
 &#39;innovation&#39;: 202,
 &#39;reserve&#39;: 203,
 &#39;stillness&#39;: 204,
 &#39;than&#39;: 205,
 &#39;experience&#39;: 206,
 &#39;song&#39;: 207,
 &#39;energy&#39;: 208,
 &#39;being&#39;: 209,
 &#39;intuition&#39;: 210,
 &#39;truest&#39;: 211,
 &#39;presence&#39;: 212,
 &#39;faith&#39;: 213,
 &#39;inspire&#39;: 214,
 &#39;essence&#39;: 215,
 &#39;mark&#39;: 216,
 &#39;embracing&#39;: 217,
 &#39;ignites&#39;: 218,
 &#39;grace&#39;: 219,
 &#39;spark&#39;: 220,
 &#39;endless&#39;: 221,
 &#39;dream&#39;: 222,
 &#39;woven&#39;: 223,
 &#39;someone&#39;: 224,
 &#39;connects&#39;: 225,
 &#39;sunset&#39;: 226,
 &#39;warmth&#39;: 227,
 &#39;nation&#39;: 228,
 &#39;pulau&#39;: 229,
 &#39;paints&#39;: 230,
 &#39;present&#39;: 231,
 &#39;language&#39;: 232,
 &#39;even&#39;: 233,
 &#39;darkest&#39;: 234,
 &#39;days&#39;: 235,
 &#39;unfold&#39;: 236,
 &#39;turns&#39;: 237,
 &#39;keys&#39;: 238,
 &#39;stories&#39;: 239,
 &#39;word&#39;: 240,
 &#39;inspiration&#39;: 241,
 &#39;souls&#39;: 242,
 &#39;abundance&#39;: 243,
 &#39;armor&#39;: 244,
 &#39;curiosity&#39;: 245,
 &#39;fuel&#39;: 246,
 &#39;greatness&#39;: 247,
 &#39;capable&#39;: 248,
 &#39;flight&#39;: 249,
 &#39;stronger&#39;: 250,
 &#39;soar&#39;: 251,
 &#39;worthy&#39;: 252,
 &#39;experiences&#39;: 253,
 &#39;composed&#39;: 254,
 &#39;made&#39;: 255,
 &#39;understanding&#39;: 256,
 &#39;gardens&#39;: 257,
 &#39;brightens&#39;: 258,
 &#39;tall&#39;: 259,
 &#39;reminds&#39;: 260,
 &#39;dawn&#39;: 261,
 &#39;biodiversity&#39;: 262,
 &#39;protect&#39;: 263,
 &#39;offers&#39;: 264,
 &#39;leads&#39;: 265,
 &#39;words&#39;: 266,
 &#39;simplicity&#39;: 267,
 &#39;imagination&#39;: 268,
 &#39;secrets&#39;: 269,
 &#39;positive&#39;: 270,
 &#39;well&#39;: 271,
 &#39;pages&#39;: 272,
 &#39;stars&#39;: 273,
 &#39;brilliance&#39;: 274,
 &#39;mirror&#39;: 275,
 &#39;creativity&#39;: 276,
 &#39;driving&#39;: 277,
 &#39;-discovery&#39;: 278,
 &#39;voice&#39;: 279,
 &#39;humility&#39;: 280,
 &#39;acceptance&#39;: 281,
 &#39;enthusiasm&#39;: 282,
 &#39;become&#39;: 283,
 &#39;propels&#39;: 284,
 &#39;heals&#39;: 285,
 &#39;echo&#39;: 286,
 &#39;miracles&#39;: 287,
 &#39;obstacle&#39;: 288,
 &#39;discover&#39;: 289,
 &#39;blooms&#39;: 290,
 &#39;pursuit&#39;: 291,
 &#39;strokes&#39;: 292,
 &#39;mosaic&#39;: 293,
 &#39;classroom&#39;: 294,
 &#39;learn&#39;: 295,
 &#39;book&#39;: 296,
 &#39;written&#39;: 297,
 &#39;created&#39;: 298,
 &#39;hues&#39;: 299,
 &#39;resonates&#39;: 300,
 &#39;go&#39;: 301,
 &#39;knows&#39;: 302,
 &#39;no&#39;: 303,
 &#39;letting&#39;: 304,
 &#39;happiness&#39;: 305,
 &#39;creates&#39;: 306,
 &#39;fuels&#39;: 307,
 &#39;wonders&#39;: 308,
 &#39;faced&#39;: 309,
 &#39;creating&#39;: 310,
 &#39;start&#39;: 311,
 &#39;effort&#39;: 312,
 &#39;friendship&#39;: 313,
 &#39;formed&#39;: 314,
 &#39;choice&#39;: 315,
 &#39;brushstroke&#39;: 316,
 &#39;thought&#39;: 317,
 &#39;setup&#39;: 318,
 &#39;liberation&#39;: 319,
 &#39;investment&#39;: 320,
 &#39;tribute&#39;: 321,
 &#39;marvel&#39;: 322,
 &#39;gentle&#39;: 323,
 &#39;diverse&#39;: 324,
 &quot;&#39;&quot;: 325,
 &#39;bukit&#39;: 326,
 &#39;unity&#39;: 327,
 &#39;coastal&#39;: 328,
 &#39;time&#39;: 329,
 &#39;truly&#39;: 330,
 &#39;unlocking&#39;: 331,
 &#39;free&#39;: 332,
 &#39;ever&#39;: 333,
 &#39;beautiful&#39;: 334,
 &#39;genuine&#39;: 335,
 &#39;own&#39;: 336,
 &#39;guide&#39;: 337,
 &#39;-lived&#39;: 338,
 &#39;heal&#39;: 339,
 &#39;flow&#39;: 340,
 &#39;learning&#39;: 341,
 &#39;seasons&#39;: 342,
 &#39;tomorrow&#39;: 343,
 &#39;reflection&#39;: 344,
 &#39;give&#39;: 345,
 &#39;cleanse&#39;: 346,
 &#39;bring&#39;: 347,
 &#39;wounded&#39;: 348,
 &#39;lives&#39;: 349,
 &#39;solace&#39;: 350,
 &#39;precious&#39;: 351,
 &#39;adventures&#39;: 352,
 &#39;expression&#39;: 353,
 &#39;character&#39;: 354,
 &#39;treasures&#39;: 355,
 &#39;good&#39;: 356,
 &#39;lights&#39;: 357,
 &#39;fragrance&#39;: 358,
 &#39;lighthouse&#39;: 359,
 &#39;make&#39;: 360,
 &#39;break&#39;: 361,
 &#39;share&#39;: 362,
 &#39;care&#39;: 363,
 &#39;celebration&#39;: 364,
 &#39;reveal&#39;: 365,
 &#39;tended&#39;: 366,
 &#39;cleanses&#39;: 367,
 &#39;resonate&#39;: 368,
 &#39;write&#39;: 369,
 &#39;intention&#39;: 370,
 &#39;rich&#39;: 371,
 &#39;waters&#39;: 372,
 &#39;transforms&#39;: 373,
 &#39;wings&#39;: 374,
 &#39;waves&#39;: 375,
 &#39;home&#39;: 376,
 &#39;fire&#39;: 377,
 &#39;wonder&#39;: 378,
 &#39;reflects&#39;: 379,
 &#39;soothes&#39;: 380,
 &#39;decision&#39;: 381,
 &#39;beneath&#39;: 382,
 &#39;goodness&#39;: 383,
 &#39;setback&#39;: 384,
 &#39;comeback&#39;: 385,
 &#39;or&#39;: 386,
 &#39;taken&#39;: 387,
 &#39;person&#39;: 388,
 &#39;encouragement&#39;: 389,
 &#39;memories&#39;: 390,
 &#39;chapter&#39;: 391,
 &#39;encounter&#39;: 392,
 &#39;gem&#39;: 393,
 &#39;treasury&#39;: 394,
 &#39;harmony&#39;: 395,
 &#39;charm&#39;: 396,
 &#39;stands&#39;: 397,
 &#39;ubin&#39;: 398,
 &#39;vision&#39;: 399,
 &#39;heritage&#39;: 400,
 &#39;labrador&#39;: 401,
 &#39;vitality&#39;: 402,
 &#39;breathtaking&#39;: 403,
 &#39;lungs&#39;: 404,
 &#39;see&#39;: 405,
 &#39;leaves&#39;: 406,
 &#39;sun&#39;: 407,
 &#39;arms&#39;: 408,
 &#39;welcome&#39;: 409,
 &#39;fill&#39;: 410,
 &#39;stepping&#39;: 411,
 &#39;greatest&#39;: 412,
 &#39;one&#39;: 413,
 &#39;fears&#39;: 414,
 &#39;nourishes&#39;: 415,
 &#39;sets&#39;: 416,
 &#39;past&#39;: 417,
 &#39;louder&#39;: 418,
 &#39;enough&#39;: 419,
 &#39;warm&#39;: 420,
 &#39;wounds&#39;: 421,
 &#39;community&#39;: 422,
 &#39;grateful&#39;: 423,
 &#39;night&#39;: 424,
 &#39;resides&#39;: 425,
 &#39;harmonious&#39;: 426,
 &#39;most&#39;: 427,
 &#39;illuminates&#39;: 428,
 &#39;those&#39;: 429,
 &#39;shields&#39;: 430,
 &#39;bitterness&#39;: 431,
 &#39;haven&#39;: 432,
 &#39;discovery&#39;: 433,
 &#39;like&#39;: 434,
 &#39;gifts&#39;: 435,
 &#39;chorus&#39;: 436,
 &#39;powers&#39;: 437,
 &#39;follow&#39;: 438,
 &#39;many&#39;: 439,
 &#39;extraordinary&#39;: 440,
 &#39;them&#39;: 441,
 &#39;passion&#39;: 442,
 &#39;desires&#39;: 443,
 &#39;existence&#39;: 444,
 &#39;unique&#39;: 445,
 &#39;plays&#39;: 446,
 &#39;sense&#39;: 447,
 &#39;acts&#39;: 448,
 &#39;reminders&#39;: 449,
 &#39;cherish&#39;: 450,
 &#39;full&#39;: 451,
 &#39;leaving&#39;: 452,
 &#39;painting&#39;: 453,
 &#39;renew&#39;: 454,
 &#39;fruits&#39;: 455,
 &#39;pillars&#39;: 456,
 &#39;unveil&#39;: 457,
 &#39;weaving&#39;: 458,
 &#39;their&#39;: 459,
 &#39;north&#39;: 460,
 &#39;sea&#39;: 461,
 &#39;ripple&#39;: 462,
 &#39;sword&#39;: 463,
 &#39;cuts&#39;: 464,
 &#39;action&#39;: 465,
 &#39;seas&#39;: 466,
 &#39;fosters&#39;: 467,
 &#39;currency&#39;: 468,
 &#39;interactions&#39;: 469,
 &#39;unites&#39;: 470,
 &#39;embodiment&#39;: 471,
 &#39;bloom&#39;: 472,
 &#39;whether&#39;: 473,
 &#39;painful&#39;: 474,
 &#39;transitions&#39;: 475,
 &#39;directed&#39;: 476,
 &#39;indomitable&#39;: 477,
 &#39;wave&#39;: 478,
 &#39;witnessing&#39;: 479,
 &#39;cherished&#39;: 480,
 &#39;bay&#39;: 481,
 &#39;ambition&#39;: 482,
 &#39;modernity&#39;: 483,
 &#39;perfect&#39;: 484,
 &#39;chinatown&#39;: 485,
 &#39;pulse&#39;: 486,
 &#39;corner&#39;: 487,
 &#39;waiting&#39;: 488,
 &#39;history&#39;: 489,
 &#39;sky&#39;: 490,
 &#39;quiet&#39;: 491,
 &#39;jurong&#39;: 492,
 &#39;bird&#39;: 493,
 &#39;wildlife&#39;: 494,
 &#39;sungei&#39;: 495,
 &#39;buloh&#39;: 496,
 &#39;wetland&#39;: 497,
 &#39;landscapes&#39;: 498,
 &#39;rugged&#39;: 499,
 &#39;stewardship&#39;: 500,
 &#39;witness&#39;: 501,
 &#39;face&#39;: 502,
 &#39;earth&#39;: 503,
 &#39;ancient&#39;: 504,
 &#39;survival&#39;: 505,
 &#39;species&#39;: 506,
 &#39;reefs&#39;: 507,
 &#39;cradle&#39;: 508,
 &#39;artistry&#39;: 509,
 &#39;-giving&#39;: 510,
 &#39;rustle&#39;: 511,
 &#39;fireflies&#39;: 512,
 &#39;springs&#39;: 513,
 &#39;rising&#39;: 514,
 &#39;fills&#39;: 515,
 &#39;air&#39;: 516,
 &#39;clarity&#39;: 517,
 &#39;chance&#39;: 518,
 &#39;stones&#39;: 519,
 &#39;at&#39;: 520,
 &#39;uniqueness&#39;: 521,
 &#39;only&#39;: 522,
 &#39;multiplies&#39;: 523,
 &#39;chains&#39;: 524,
 &#39;could&#39;: 525,
 &#39;small&#39;: 526,
 &#39;blueprints&#39;: 527,
 &#39;cornerstone&#39;: 528,
 &#39;gateway&#39;: 529,
 &#39;chest&#39;: 530,
 &#39;patience&#39;: 531,
 &#39;far&#39;: 532,
 &#39;birthplace&#39;: 533,
 &#39;thoughts&#39;: 534,
 &#39;nights&#39;: 535,
 &#39;mend&#39;: 536,
 &#39;everyday&#39;: 537,
 &#39;alchemy&#39;: 538,
 &#39;conductor&#39;: 539,
 &#39;cosmos&#39;: 540,
 &#39;kind&#39;: 541,
 &#39;remind&#39;: 542,
 &#39;emotions&#39;: 543,
 &#39;ordinary&#39;: 544,
 &#39;mind&#39;: 545,
 &#39;cycles&#39;: 546,
 &#39;weave&#39;: 547,
 &#39;fulfilled&#39;: 548,
 &#39;burdens&#39;: 549,
 &#39;healing&#39;: 550,
 &#39;bright&#39;: 551,
 &#39;powerful&#39;: 552,
 &quot;&#39;ll&quot;: 553,
 &#39;contagious&#39;: 554,
 &#39;set&#39;: 555,
 &#39;spreads&#39;: 556,
 &#39;wildfire&#39;: 557,
 &#39;sails&#39;: 558,
 &#39;fortress&#39;: 559,
 &#39;balm&#39;: 560,
 &#39;old&#39;: 561,
 &#39;sparks&#39;: 562,
 &#39;transcends&#39;: 563,
 &#39;all&#39;: 564,
 &#39;barriers&#39;: 565,
 &#39;drives&#39;: 566,
 &#39;ages&#39;: 567,
 &#39;conquer&#39;: 568,
 &#39;achieve&#39;: 569,
 &#39;difference&#39;: 570,
 &#39;barrier&#39;: 571,
 &#39;too&#39;: 572,
 &#39;adversity&#39;: 573,
 &#39;things&#39;: 574,
 &#39;agent&#39;: 575,
 &#39;anything&#39;: 576,
 &#39;melodies&#39;: 577,
 &#39;eyes&#39;: 578,
 &#39;deepest&#39;: 579,
 &#39;intentions&#39;: 580,
 &#39;away&#39;: 581,
 &#39;choices&#39;: 582,
 &#39;spread&#39;: 583,
 &#39;receive&#39;: 584,
 &#39;return&#39;: 585,
 &#39;makes&#39;: 586,
 &#39;define&#39;: 587,
 &#39;grow&#39;: 588,
 &#39;watered&#39;: 589,
 &#39;meaning&#39;: 590,
 &#39;renews&#39;: 591,
 &#39;forth&#39;: 592,
 &#39;differences&#39;: 593,
 &#39;sowing&#39;: 594,
 &#39;lies&#39;: 595,
 &#39;freedom&#39;: 596,
 &#39;together&#39;: 597,
 &#39;truth&#39;: 598,
 &#39;speak&#39;: 599,
 &#39;which&#39;: 600,
 &#39;stand&#39;: 601,
 &#39;infectious&#39;: 602,
 &#39;spreading&#39;: 603,
 &#39;fear&#39;: 604,
 &#39;connections&#39;: 605,
 &#39;wellspring&#39;: 606,
 &#39;salve&#39;: 607,
 &#39;turbulent&#39;: 608,
 &#39;protects&#39;: 609,
 &#39;ripples&#39;: 610,
 &#39;preciousness&#39;: 611,
 &#39;uplift&#39;: 612,
 &#39;shapes&#39;: 613,
 &#39;between&#39;: 614,
 &#39;meet&#39;: 615,
 &#39;deserves&#39;: 616,
 &#39;becoming&#39;: 617,
 &#39;blossoming&#39;: 618,
 &#39;alter&#39;: 619,
 &#39;course&#39;: 620,
 &#39;reverberates&#39;: 621,
 &#39;stroke&#39;: 622,
 &#39;brush&#39;: 623,
 &#39;tale&#39;: 624,
 &#39;contentment&#39;: 625,
 &#39;reside&#39;: 626,
 &#39;wide&#39;: 627,
 &#39;comes&#39;: 628,
 &#39;cultures&#39;: 629,
 &#39;bounds&#39;: 630,
 &#39;marina&#39;: 631,
 &#39;orchard&#39;: 632,
 &#39;road&#39;: 633,
 &#39;sentosa&#39;: 634,
 &#39;merlion&#39;: 635,
 &#39;symbol&#39;: 636,
 &#39;timah&#39;: 637,
 &#39;reaches&#39;: 638,
 &#39;island&#39;: 639,
 &#39;defines&#39;: 640,
 &#39;intertwine&#39;: 641,
 &#39;tells&#39;: 642,
 &#39;alive&#39;: 643,
 &#39;tales&#39;: 644,
 &#39;southern&#39;: 645,
 &#39;safari&#39;: 646,
 &#39;mysteries&#39;: 647,
 &#39;endeavor&#39;: 648,
 &#39;excellence&#39;: 649,
 &#39;changi&#39;: 650,
 &#39;river&#39;: 651,
 &#39;vibrancy&#39;: 652,
 &#39;macritchie&#39;: 653,
 &#39;reservoir&#39;: 654,
 &#39;picture&#39;: 655,
 &#39;reaching&#39;: 656,
 &#39;diligence&#39;: 657,
 &#39;views&#39;: 658,
 &#39;embodies&#39;: 659,
 &#39;brand&#39;: 660,
 &#39;lake&#39;: 661,
 &#39;destinies&#39;: 662,
 &#39;chek&#39;: 663,
 &#39;jawa&#39;: 664,
 &#39;unwavering&#39;: 665,
 &#39;batok&#39;: 666,
 &#39;marine&#39;: 667,
 &#39;trove&#39;: 668,
 &#39;flourish&#39;: 669,
 &#39;forests&#39;: 670,
 &#39;breathe&#39;: 671,
 &#39;rivers&#39;: 672,
 &#39;skies&#39;: 673,
 &#39;cities&#39;: 674,
 &#39;footprint&#39;: 675,
 &#39;wetlands&#39;: 676,
 &#39;drop&#39;: 677,
 &#39;savannas&#39;: 678,
 &#39;brushstrokes&#39;: 679,
 &#39;fiery&#39;: 680,
 &#39;pledge&#39;: 681,
 &#39;silent&#39;: 682,
 &#39;grass&#39;: 683,
 &#39;caves&#39;: 684,
 &#39;ray&#39;: 685,
 &#39;sunlight&#39;: 686,
 &#39;endurance&#39;: 687,
 &#39;renewal&#39;: 688,
 &#39;safeguard&#39;: 689,
 &#39;seed&#39;: 690,
 &#39;planted&#39;: 691,
 &#39;gesture&#39;: 692,
 &#39;call&#39;: 693,
 &#39;wild&#39;: 694,
 &#39;delicate&#39;: 695,
 &#39;ecosystems&#39;: 696,
 &#39;nurseries&#39;: 697,
 &#39;sand&#39;: 698,
 &#39;flower&#39;: 699,
 &#39;perfume&#39;: 700,
 &#39;inhale&#39;: 701,
 &#39;veins&#39;: 702,
 &#39;land&#39;: 703,
 &#39;presents&#39;: 704,
 &#39;overflow&#39;: 705,
 &#39;today&#39;: 706,
 &#39;reflect&#39;: 707,
 &#39;chambers&#39;: 708,
 &#39;longer&#39;: 709,
 &#39;serves&#39;: 710,
 &#39;soundtrack&#39;: 711,
 &#39;simple&#39;: 712,
 &#39;joys&#39;: 713,
 &#39;navigate&#39;: 714,
 &#39;illuminate&#39;: 715,
 &#39;fully&#39;: 716,
 &#39;reminding&#39;: 717,
 &#39;victories&#39;: 718,
 &#39;destination&#39;: 719,
 &#39;fingerprint&#39;: 720,
 &#39;exists&#39;: 721,
 &#39;silence&#39;: 722,
 &#39;speaks&#39;: 723,
 &#39;often&#39;: 724,
 &#39;significance&#39;: 725,
 &#39;constant&#39;: 726,
 &#39;chaos&#39;: 727,
 &#39;unknown&#39;: 728,
 &#39;midst&#39;: 729,
 &#39;peaceful&#39;: 730,
 &#39;elders&#39;: 731,
 &#39;allows&#39;: 732,
 &#39;sing&#39;: 733,
 &#39;turn&#39;: 734,
 &#39;-off&#39;: 735,
 &#39;lands&#39;: 736,
 &#39;vulnerability&#39;: 737,
 &#39;ability&#39;: 738,
 &#39;itself&#39;: 739,
 &#39;without&#39;: 740,
 &#39;lullaby&#39;: 741,
 &#39;hug&#39;: 742,
 &#39;shadows&#39;: 743,
 &#39;-expanding&#39;: 744,
 &#39;moon&#39;: 745,
 &#39;loving&#39;: 746,
 &#39;cannot&#39;: 747,
 &#39;affirmations&#39;: 748,
 &#39;solitude&#39;: 749,
 &#39;another&#39;: 750,
 &#39;raindrops&#39;: 751,
 &#39;nurture&#39;: 752,
 &#39;restore&#39;: 753,
 &#39;-love&#39;: 754,
 &#39;-acceptance&#39;: 755,
 &#39;portrait&#39;: 756,
 &#39;radiates&#39;: 757,
 &#39;-care&#39;: 758,
 &#39;nurtures&#39;: 759,
 &#39;body&#39;: 760,
 &#39;deed&#39;: 761,
 &#39;restless&#39;: 762,
 &#39;balance&#39;: 763,
 &#39;-expression&#39;: 764,
 &#39;gloomiest&#39;: 765,
 &#39;trust&#39;: 766,
 &#39;relationships&#39;: 767,
 &#39;minds&#39;: 768,
 &#39;attract&#39;: 769,
 &#39;great&#39;: 770,
 &#39;weight&#39;: 771,
 &#39;grudges&#39;: 772,
 &#39;keeps&#39;: 773,
 &#39;giving&#39;: 774,
 &#39;signature&#39;: 775,
 &#39;simplest&#39;: 776,
 &#39;anchor&#39;: 777,
 &#39;exploration&#39;: 778,
 &#39;smallest&#39;: 779,
 &#39;refuge&#39;: 780,
 &#39;weary&#39;: 781,
 &#39;bedrock&#39;: 782,
 &#39;generations&#39;: 783,
 &#39;unlocks&#39;: 784,
 &#39;mends&#39;: 785,
 &#39;unstoppable&#39;: 786,
 &#39;happen&#39;: 787,
 &#39;impossible&#39;: 788,
 &#39;think&#39;: 789,
 &#39;just&#39;: 790,
 &#39;as&#39;: 791,
 &#39;becomes&#39;: 792,
 &#39;conspire&#39;: 793,
 &#39;favor&#39;: 794,
 &#39;making&#39;: 795,
 &#39;footprints&#39;: 796,
 &#39;forge&#39;: 797,
 &#39;limitless&#39;: 798,
 &#39;unlock&#39;: 799,
 &#39;doors&#39;: 800,
 &#39;destined&#39;: 801,
 &#39;persevere&#39;: 802,
 &#39;darkness&#39;: 803,
 &#39;seek&#39;: 804,
 &#39;work&#39;: 805,
 &#39;art&#39;: 806,
 &#39;amazing&#39;: 807,
 &#39;limitation&#39;: 808,
 &#39;achieving&#39;: 809,
 &#39;architect&#39;: 810,
 &#39;brightest&#39;: 811,
 &#39;followed&#39;: 812,
 &#39;rainbows&#39;: 813,
 &#39;plant&#39;: 814,
 &#39;move&#39;: 815,
 &#39;points&#39;: 816,
 &#39;filled&#39;: 817,
 &#39;blend&#39;: 818,
 &#39;illuminated&#39;: 819,
 &#39;reflected&#39;: 820,
 &#39;multiply&#39;: 821,
 &#39;pieces&#39;: 822,
 &#39;unconditionally&#39;: 823,
 &#39;ink&#39;: 824,
 &#39;deeds&#39;: 825,
 &#39;calling&#39;: 826,
 &#39;disguise&#39;: 827,
 &#39;washes&#39;: 828,
 &#39;forgive&#39;: 829,
 &#39;pen&#39;: 830,
 &#39;nudges&#39;: 831,
 &#39;triumphs&#39;: 832,
 &#39;choose&#39;: 833,
 &#39;found&#39;: 834,
 &#39;ourselves&#39;: 835,
 &#39;guidance&#39;: 836,
 &quot;&#39;ve&quot;: 837,
 &#39;traveled&#39;: 838,
 &#39;hopes&#39;: 839,
 &#39;offer&#39;: 840,
 &#39;pass&#39;: 841,
 &#39;uncertainty&#39;: 842,
 &#39;prelude&#39;: 843,
 &#39;savoring&#39;: 844,
 &#39;other&#39;: 845,
 &#39;side&#39;: 846,
 &#39;sweetest&#39;: 847,
 &#39;twist&#39;: 848,
 &#39;brightness&#39;: 849,
 &#39;notes&#39;: 850,
 &#39;tending&#39;: 851,
 &#39;steps&#39;: 852,
 &#39;limits&#39;: 853,
 &#39;lift&#39;: 854,
 &#39;weaves&#39;: 855,
 &#39;cultivating&#39;: 856,
 &#39;heights&#39;: 857,
 &#39;halls&#39;: 858,
 &#39;so&#39;: 859,
 &#39;brightly&#39;: 860,
 &#39;anthem&#39;: 861,
 &#39;against&#39;: 862,
 &#39;rock&#39;: 863,
 &#39;wildest&#39;: 864,
 &#39;binds&#39;: 865,
 &#39;wherever&#39;: 866,
 &#39;uplifts&#39;: 867,
 &#39;stormy&#39;: 868,
 &#39;achievements&#39;: 869,
 &#39;principles&#39;: 870,
 &#39;engine&#39;: 871,
 &#39;contagion&#39;: 872,
 &#39;doubt&#39;: 873,
 &#39;values&#39;: 874,
 &#39;victory&#39;: 875,
 &#39;shared&#39;: 876,
 &#39;beam&#39;: 877,
 &#39;frees&#39;: 878,
 &#39;spoken&#39;: 879,
 &#39;counts&#39;: 880,
 &#39;thread&#39;: 881,
 &#39;human&#39;: 882,
 &#39;bad&#39;: 883,
 &#39;lesson&#39;: 884,
 &#39;embraced&#39;: 885,
 &#39;contented&#39;: 886,
 &#39;shown&#39;: 887,
 &#39;forged&#39;: 888,
 &#39;miracle&#39;: 889,
 &#39;fate&#39;: 890,
 &#39;mold&#39;: 891,
 &#39;glorious&#39;: 892,
 &#39;narrative&#39;: 893,
 &#39;farewells&#39;: 894,
 &#39;enlightenment&#39;: 895,
 &#39;out&#39;: 896,
 &#39;influence&#39;: 897,
 &#39;triumphant&#39;: 898,
 &#39;determined&#39;: 899,
 &#39;joyous&#39;: 900,
 &#39;worth&#39;: 901,
 &#39;telling&#39;: 902,
 &#39;ebb&#39;: 903,
 &#39;goodbyes&#39;: 904,
 &#39;transcending&#39;: 905,
 &#39;fostering&#39;: 906,
 &#39;serene&#39;: 907,
 &#39;state&#39;: 908,
 &#39;blooming&#39;: 909,
 &#39;significant&#39;: 910,
 &#39;part&#39;: 911,
 &#39;turning&#39;: 912,
 &#39;point&#39;: 913,
 &#39;defy&#39;: 914,
 &#39;logic&#39;: 915,
 &#39;evolving&#39;: 916,
 &#39;wiser&#39;: 917,
 &#39;resilient&#39;: 918,
 &#39;version&#39;: 919,
 &#39;release&#39;: 920,
 &#39;resentment&#39;: 921,
 &#39;providing&#39;: 922,
 &#39;assurance&#39;: 923,
 &#39;not&#39;: 924,
 &#39;alone&#39;: 925,
 &#39;realization&#39;: 926,
 &#39;fulfillment&#39;: 927,
 &#39;richness&#39;: 928,
 &#39;meaningful&#39;: 929,
 &#39;connect&#39;: 930,
 &#39;affection&#39;: 931,
 &#39;deliberate&#39;: 932,
 &#39;shaping&#39;: 933,
 &#39;living&#39;: 934,
 &#39;proof&#39;: 935,
 &#39;conquering&#39;: 936,
 &#39;converge&#39;: 937,
 &#39;roars&#39;: 938,
 &#39;embraces&#39;: 939,
 &#39;traditions&#39;: 940,
 &#39;reach&#39;: 941,
 &#39;beaches&#39;: 942,
 &#39;alleys&#39;: 943,
 &#39;southeast&#39;: 944,
 &#39;asia&#39;: 945,
 &#39;beats&#39;: 946,
 &#39;strong&#39;: 947,
 &#39;raffles&#39;: 948,
 &#39;place&#39;: 949,
 &#39;clarke&#39;: 950,
 &#39;quay&#39;: 951,
 &#39;told&#39;: 952,
 &#39;hawker&#39;: 953,
 &#39;centers&#39;: 954,
 &#39;fine&#39;: 955,
 &#39;dining&#39;: 956,
 &#39;flavors&#39;: 957,
 &#39;delectable&#39;: 958,
 &#39;ideas&#39;: 959,
 &#39;little&#39;: 960,
 &#39;india&#39;: 961,
 &#39;botanic&#39;: 962,
 &#39;glistens&#39;: 963,
 &#39;universal&#39;: 964,
 &#39;studios&#39;: 965,
 &#39;attractions&#39;: 966,
 &#39;-class&#39;: 967,
 &#39;futures&#39;: 968,
 &#39;built&#39;: 969,
 &#39;heavens&#39;: 970,
 &#39;shopping&#39;: 971,
 &#39;shores&#39;: 972,
 &#39;trails&#39;: 973,
 &#39;leap&#39;: 974,
 &#39;sands&#39;: 975,
 &#39;flyer&#39;: 976,
 &#39;landmarks&#39;: 977,
 &#39;lanterns&#39;: 978,
 &#39;kampong&#39;: 979,
 &#39;glam&#39;: 980,
 &#39;-state&#39;: 981,
 &#39;esplanade&#39;: 982,
 &#39;performances&#39;: 983,
 &#39;haw&#39;: 984,
 &#39;par&#39;: 985,
 &#39;villa&#39;: 986,
 &#39;culture&#39;: 987,
 &#39;thriving&#39;: 988,
 &#39;tekong&#39;: 989,
 &#39;islands&#39;: 990,
 &#39;testifies&#39;: 991,
 &#39;fort&#39;: 992,
 &#39;canning&#39;: 993,
 &#39;ruggedness&#39;: 994,
 &#39;dedication&#39;: 995,
 &#39;stone&#39;: 996,
 &#39;stretches&#39;: 997,
 &#39;horizon&#39;: 998,
 &#39;reign&#39;: 999,
 &#39;east&#39;: 1000,
 ...}</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="308">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>VOCABULARY_LEN <span class="op">=</span> <span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span>  <span class="co"># 0 reserved for padding</span></span></code></pre></div>
</div>
<div class="cell markdown">
<p><strong>Texts to sequence of integers / tokens</strong></p>
</div>
<div class="cell code" data-execution_count="309">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>tokenized_sequences <span class="op">=</span> []</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> datapoint <span class="kw">in</span> data:</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    token_list <span class="op">=</span> tokenizer.texts_to_sequences([datapoint])[<span class="dv">0</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    tokenized_sequences.append(token_list)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;Total number of quotes converted from texts to integers: </span><span class="sc">{</span><span class="bu">len</span>(tokenized_sequences)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>tokenized_sequences[:][:<span class="dv">10</span>]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Total number of quotes converted from texts to integers: 1000
</code></pre>
</div>
<div class="output execute_result" data-execution_count="309">
<pre><code>[[22, 2, 54, 3, 17, 7, 183, 518, 15, 111, 5, 46, 25],
 [22, 35, 27, 2, 411, 519, 15, 5, 412],
 [22, 2, 112, 3, 16, 6, 12, 14, 21, 23, 5],
 [22, 11, 14, 160, 2, 41, 15, 74, 2, 46, 413, 31, 520, 7],
 [22, 2, 11, 14, 265, 15, 2, 719, 3, 5],
 [22, 5, 11, 14, 10, 2, 720, 3, 5, 43, 99, 2],
 [22, 2, 231, 11, 14, 10, 2, 522, 413, 20, 330],
 [22, 5, 11, 35, 68, 2, 184, 15, 331, 5, 66],
 [22, 6, 124, 185, 14, 523, 2, 146, 9, 5],
 [22, 2, 11, 14, 415, 2, 113, 3, 5, 69]]</code></pre>
</div>
</div>
<section id="splitting-quotes" class="cell markdown">
<h3><strong>Splitting Quotes</strong></h3>
</section>
<div class="cell markdown">
<p>To generate sequences for the model to learn from, we should split
the given quotes incrementally as rows</p>
</div>
<div class="cell code" data-execution_count="310">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to split the sentence incrementally</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It does not create sentences of 1 word</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_sentence(sentence):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> sentence</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    incremental_splits <span class="op">=</span> [words[: i <span class="op">+</span> <span class="dv">2</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words) <span class="op">-</span> <span class="dv">1</span>)]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> incremental_splits</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>sequence_data <span class="op">=</span> []</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sequence <span class="kw">in</span> tokenized_sequences:</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    incremental_splits <span class="op">=</span> split_sentence(sequence)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    sequence_data.extend(incremental_splits)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>sequence_data[:][:<span class="dv">10</span>]  <span class="co"># First 10 sequences</span></span></code></pre></div>
<div class="output execute_result" data-execution_count="310">
<pre><code>[[22, 2],
 [22, 2, 54],
 [22, 2, 54, 3],
 [22, 2, 54, 3, 17],
 [22, 2, 54, 3, 17, 7],
 [22, 2, 54, 3, 17, 7, 183],
 [22, 2, 54, 3, 17, 7, 183, 518],
 [22, 2, 54, 3, 17, 7, 183, 518, 15],
 [22, 2, 54, 3, 17, 7, 183, 518, 15, 111],
 [22, 2, 54, 3, 17, 7, 183, 518, 15, 111, 5]]</code></pre>
</div>
</div>
<div class="cell markdown">
<p>To ensure the whole dataset has a consistent number of words, we need
to "pad" our sentences</p>
<ul>
<li>There are two ways to pad
<ol>
<li>Before the sentence: 'pre'</li>
<li>After the sentence: 'post'</li>
</ol></li>
</ul>
</div>
<div class="cell code" data-execution_count="311">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>MAX_SEQ_LEN <span class="op">=</span> <span class="bu">max</span>(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    [<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> sequence_data]</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># This is to get the longest sequence of texts.</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>pre_sequence_data <span class="op">=</span> np.array(</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    pad_sequences(sequence_data, maxlen<span class="op">=</span>MAX_SEQ_LEN, padding<span class="op">=</span><span class="st">&quot;pre&quot;</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Keeps all sequences the same length</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>post_sequence_data <span class="op">=</span> np.array(</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    pad_sequences(sequence_data, maxlen<span class="op">=</span>MAX_SEQ_LEN, padding<span class="op">=</span><span class="st">&quot;post&quot;</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Keeps all sequences the same length</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Total number of rows (pre-padded): </span><span class="sc">{</span><span class="bu">len</span>(pre_sequence_data)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Pre-padded Sequence Data: </span><span class="ch">\n</span><span class="sc">{</span>pre_sequence_data<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">&quot;</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Total number of rows (post-padded): </span><span class="sc">{</span><span class="bu">len</span>(post_sequence_data)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Post-padded Sequence Data: </span><span class="ch">\n</span><span class="sc">{</span>post_sequence_data<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Total number of rows (pre-padded): 8795
Pre-padded Sequence Data: 
[[  0   0   0 ...   0  22   2]
 [  0   0   0 ...  22   2  54]
 [  0   0   0 ...   2  54   3]
 ...
 [  0   0   0 ...   3   2 611]
 [  0   0   0 ...   2 611   3]
 [  0   0   0 ... 611   3  17]]


Total number of rows (post-padded): 8795
Post-padded Sequence Data: 
[[ 22   2   0 ...   0   0   0]
 [ 22   2  54 ...   0   0   0]
 [ 22   2  54 ...   0   0   0]
 ...
 [ 25 102  24 ...   0   0   0]
 [ 25 102  24 ...   0   0   0]
 [ 25 102  24 ...   0   0   0]]
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Frequently in recurrent neural networks (RNNs), we typically utilize
the final output or hidden state to make predictions or perform the
desired task.</p>
<p>When we introduce a sequence of 0's to the RNN before extracting the
final output, which is done with 'post' padding, the hidden state of the
network at the last word in the sentence is likely to be influenced or
diminished to some degree by the consecutive zero inputs that follow
that word.</p>
<p>Thus, from an intuitive perspective, this could be a reason why
<strong>pre-padding is often preferred</strong> and tends to be more
effective.</p>
<p>[Reference] <a
href="https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results"
class="uri">https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results</a></p>
<p>This idea is tested in this paper, where they looked at the effects
of pre padding and post padding these were the final results for
LSTM</p>
<p><img src="vertopal_9ec0ba30fb704436854ec086e697b20f/image.png"
alt="image.png" /></p>
<p>[Reference] EFFECTS OF PADDING ON LSTMS AND CNNS, Dwarampudi Mahidhar
Reddy, N V Subba Reddy</p>
</div>
<section id="target-features-validation-split" class="cell markdown">
<h3><strong>Target, Features, Validation split</strong></h3>
</section>
<div class="cell code" data-execution_count="312">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now that we have an array, let us slice the last word as the output.</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The preceding words are the input.</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_features_and_label(data):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> data[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> data[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> get_features_and_label(pre_sequence_data)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Features: </span><span class="ch">\n</span><span class="sc">{</span>X<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">&quot;</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Labels: </span><span class="ch">\n</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Features: 
[[  0   0   0 ...   0   0  22]
 [  0   0   0 ...   0  22   2]
 [  0   0   0 ...  22   2  54]
 ...
 [  0   0   0 ... 145   3   2]
 [  0   0   0 ...   3   2 611]
 [  0   0   0 ...   2 611   3]]


Labels: 
[  2  54   3 ... 611   3  17]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="313">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let us convert y to be a one-hot array.</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>labels_cat <span class="op">=</span> to_categorical(y, num_classes<span class="op">=</span>VOCABULARY_LEN)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> labels_cat</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="314">
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Size of X_train: </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Size of X_val: </span><span class="sc">{</span><span class="bu">len</span>(X_val)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Size of X_train: 7036
Size of X_val: 1759
</code></pre>
</div>
</div>
<section id="data-augmentation" class="cell markdown">
<h2><strong>Data Augmentation</strong></h2>
<p>This process involves using NLTK's part-of-speech tagging to figure
out the role of each word. Then, it transforms these tags into a format
that works well with WordNet. Finally, it selects synonyms considering
the word's specific part of speech, ensuring that the sentence remains
grammatically correct.</p>
</section>
<section id="replacement-functions" class="cell markdown">
<h4>Replacement functions</h4>
</section>
<div class="cell code" data-execution_count="520">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">&quot;punkt&quot;</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">&quot;averaged_perceptron_tagger&quot;</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">&quot;wordnet&quot;</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">&#39;omw-1.4&#39;</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_wordnet_pos(treebank_tag):</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Convert treebank POS tags to WordNet POS tags.&quot;&quot;&quot;</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> treebank_tag.startswith(<span class="st">&quot;J&quot;</span>):</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> wordnet.ADJ</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> treebank_tag.startswith(<span class="st">&quot;V&quot;</span>):</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> wordnet.VERB</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> treebank_tag.startswith(<span class="st">&quot;N&quot;</span>):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> wordnet.NOUN</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> treebank_tag.startswith(<span class="st">&quot;R&quot;</span>):</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> wordnet.ADV</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> synonym_replacement(text, n<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> word_tokenize(text)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    pos_tags <span class="op">=</span> pos_tag(words)</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    augmented_texts <span class="op">=</span> []</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        new_words <span class="op">=</span> words.copy()</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="fl">0.2</span>:  <span class="co"># Probability of replacing a word</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>                wordnet_pos <span class="op">=</span> get_wordnet_pos(pos_tags[i][<span class="dv">1</span>])  <span class="co"># Get WordNet POS tag</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> wordnet_pos:</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>                    synonyms <span class="op">=</span> get_synonyms(word, wordnet_pos)</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> synonyms:</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>                        synonym <span class="op">=</span> random.choice(synonyms)</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>                        new_words[i] <span class="op">=</span> synonym</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>        augmented_texts.append(<span class="st">&quot; &quot;</span>.join(new_words))</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> augmented_texts</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_synonyms(word, pos):</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Get synonyms of a word with the same part of speech.&quot;&quot;&quot;</span></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>    synonyms <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> syn <span class="kw">in</span> wordnet.synsets(word, pos<span class="op">=</span>pos):</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lemma <span class="kw">in</span> syn.lemmas():</span>
<span id="cb30-47"><a href="#cb30-47" aria-hidden="true" tabindex="-1"></a>            synonyms.add(</span>
<span id="cb30-48"><a href="#cb30-48" aria-hidden="true" tabindex="-1"></a>                lemma.name().replace(<span class="st">&quot;_&quot;</span>, <span class="st">&quot; &quot;</span>)</span>
<span id="cb30-49"><a href="#cb30-49" aria-hidden="true" tabindex="-1"></a>            )  <span class="co"># Replace underscores for multi-word synonyms</span></span>
<span id="cb30-50"><a href="#cb30-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-51"><a href="#cb30-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> synonyms:</span>
<span id="cb30-52"><a href="#cb30-52" aria-hidden="true" tabindex="-1"></a>        synonyms.remove(word)</span>
<span id="cb30-53"><a href="#cb30-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-54"><a href="#cb30-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(synonyms)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\zzhen\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     C:\Users\zzhen\AppData\Roaming\nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\zzhen\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to
[nltk_data]     C:\Users\zzhen\AppData\Roaming\nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="521">
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>data_augmented <span class="op">=</span> []</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> quote <span class="kw">in</span> data:</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    original_text <span class="op">=</span> quote</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    augmented_texts <span class="op">=</span> synonym_replacement(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        original_text, n<span class="op">=</span><span class="dv">3</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Generates 3 sentences with different synonyms</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    data_augmented.extend(augmented_texts)  <span class="co"># Keep to one dimension</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>data_augmented_df <span class="op">=</span> pd.DataFrame(data_augmented, columns<span class="op">=</span>[<span class="st">&#39;Quotes&#39;</span>])</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>data_augmented_df.to_csv(<span class="st">&#39;./text_augmented.csv&#39;</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>data_augmented[:<span class="dv">5</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="521">
<pre><code>[&quot;Embrace the beauty of every sunrise ; it &#39;s a fresh chance to paint your world with joy .&quot;,
 &quot;Embrace the beauty of every sunrise ; it &#39;s a fresh chance to paint your globe with joy .&quot;,
 &quot;Embrace the beauty of every break of day ; it &#39;s a refreshing chance to paint your world with joy .&quot;,
 &#39;Embrace challenge ; they are the stepping stones to your greatest victories .&#39;,
 &#39;Embrace challenges ; they are the stepping stones to your greatest victories .&#39;]</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Notice that data augmentation was done before splitting data into
train and validation sets. This is because the use of data augmentation
is to generate more training data, not to ensure the model can
accurately predict "right answers".</p>
</div>
<section id="data-preparation-on-augmented-data" class="cell markdown">
<h3>Data preparation on Augmented Data</h3>
</section>
<div class="cell code" data-execution_count="573">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_preparation_pipeline(data):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenization</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> Tokenizer(filters<span class="op">=</span><span class="st">&#39;&#39;</span>)  <span class="co"># Instantiate a Tokenizer object.</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    clean_data <span class="op">=</span> add_space_before_punctuation(data)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    tokenizer.fit_on_texts(clean_data)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    vocab_len <span class="op">=</span> <span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Converts each of the quotes from texts to integers.</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    tokenized_sequences <span class="op">=</span> []</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> datapoint <span class="kw">in</span> data:</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> tokenizer.texts_to_sequences([datapoint])[<span class="dv">0</span>]</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        tokenized_sequences.append(token_list)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;Total number of quotes converted from texts to integers: </span><span class="sc">{</span><span class="bu">len</span>(tokenized_sequences)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Splitting quotes</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split_sentence(sentence):</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> sentence</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        incremental_splits <span class="op">=</span> [words[: i <span class="op">+</span> <span class="dv">2</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words) <span class="op">-</span> <span class="dv">1</span>)]</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> incremental_splits</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    sequence_data <span class="op">=</span> []</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sequence <span class="kw">in</span> tokenized_sequences:</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        incremental_splits <span class="op">=</span> split_sentence(sequence)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        sequence_data.extend(incremental_splits)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Padding</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>    max_sequence_len <span class="op">=</span> <span class="bu">max</span>(</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        [<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> sequence_data]</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># This is to get the longest sequence of texts.</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>    pre_sequence_data <span class="op">=</span> np.array(</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>        pad_sequences(sequence_data, maxlen<span class="op">=</span>max_sequence_len, padding<span class="op">=</span><span class="st">&quot;pre&quot;</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Keeps all sequences the same length</span></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train, Test, and Validation split</span></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_features_and_label(data):</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> data[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> data[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X, y</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>    features, labels <span class="op">=</span> get_features_and_label(pre_sequence_data)</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> to_categorical(labels, num_classes<span class="op">=</span>vocab_len)</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (vocab_len, max_sequence_len, features, labels, tokenizer)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="574">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>VOCAB_LEN_AUG, MAX_SEQ_LEN_AUG, X_aug, y_aug, augmented_tokenizer <span class="op">=</span> data_preparation_pipeline(data_augmented)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>X_aug_train, X_aug_val, y_aug_train, y_aug_val <span class="op">=</span> train_test_split(X_aug, y_aug, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Size of X_aug_train: </span><span class="sc">{</span><span class="bu">len</span>(X_aug_train)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Size of X_aug_val: </span><span class="sc">{</span><span class="bu">len</span>(X_aug_val)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Number of unique tokens: </span><span class="sc">{</span>VOCAB_LEN_AUG<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Maximum sequence length after augmentation: </span><span class="sc">{</span>MAX_SEQ_LEN_AUG<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Total number of quotes converted from texts to integers: 3000
Size of X_aug_train: 31537
Size of X_aug_val: 7885
Number of unique tokens: 2419
Maximum sequence length after augmentation: 42
</code></pre>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>augmented_tokenizer.texts_to_sequences()</span></code></pre></div>
</div>
<section id="split-by-character" class="cell markdown">
<h2><strong>Split by Character</strong></h2>
<p>Predicting each upcoming word is a task that requires an extremely
large number of examples (&gt;100k). Given our smaller dataset,
character prediction may be an easier task, as there are at most 26
outputs excluding punctuation.</p>
<p><strong>However it greatly limits the length of sentences that can be
generated</strong></p>
<p>Character-level text generation has several challenges, including the
lack of context, longer training times, increased resource requirements,
spelling errors, difficulty with punctuation and formatting, limited
semantic understanding, data requirements, and limited control. To
overcome these issues, hybrid models and post-processing techniques can
be employed.</p>
</section>
<div class="cell code" data-execution_count="318">
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data Cleaning and Preprocessing</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>data_lower <span class="op">=</span> [s.lower() <span class="cf">for</span> s <span class="kw">in</span> data]  <span class="co"># Convert to lowercase (To keep only lower case classes)</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the vectors</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> []</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>next_chars <span class="op">=</span> []</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>chars_window <span class="op">=</span> <span class="dv">50</span> <span class="co"># Determined by the length distribution of data</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sentence <span class="kw">in</span> data_lower:</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get 50 previous chars and next char; then shift by step</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(sentence) <span class="op">-</span> chars_window, step):</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>        sentences.append(sentence[i:i <span class="op">+</span> chars_window])</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        next_chars.append(sentence[i <span class="op">+</span> chars_window])</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a Data Frame with the vectors</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;sentence&#39;</span>: sentences, <span class="st">&#39;next_char&#39;</span>: next_chars})</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the initial rows</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                                            sentence next_char
0  embrace the beauty of every sunrise; it&#39;s a fr...         c
1  mbrace the beauty of every sunrise; it&#39;s a fre...         h
2  brace the beauty of every sunrise; it&#39;s a fres...         a
3  race the beauty of every sunrise; it&#39;s a fresh...         n
4  ace the beauty of every sunrise; it&#39;s a fresh ...         c
</code></pre>
</div>
</div>
<section id="tokenize-every-character" class="cell markdown">
<h4>Tokenize every character</h4>
</section>
<div class="cell code" data-execution_count="319">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>char_tokenizer <span class="op">=</span> Tokenizer(</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    lower<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    filters<span class="op">=</span><span class="st">&#39;&#39;</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    char_level<span class="op">=</span><span class="va">True</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>char_tokenizer.fit_on_texts(data_lower)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>CHAR_N_VOCAB <span class="op">=</span> <span class="bu">len</span>(char_tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>CHAR_WORD_INDEX <span class="op">=</span> char_tokenizer.word_index</span></code></pre></div>
</div>
<section id="encoding-every-sentence-by-character"
class="cell markdown">
<h5>Encoding every sentence by character</h5>
</section>
<div class="cell code" data-execution_count="320">
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>num_seqs <span class="op">=</span> <span class="bu">len</span>(sentences)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>n_vocab <span class="op">=</span> CHAR_N_VOCAB</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>char_to_index <span class="op">=</span> CHAR_WORD_INDEX</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>X_char <span class="op">=</span> np.zeros((num_seqs, chars_window))</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>y_char <span class="op">=</span> np.zeros((num_seqs, n_vocab))</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sentence <span class="kw">in</span> <span class="bu">enumerate</span>(sentences):</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t, char <span class="kw">in</span> <span class="bu">enumerate</span>(sentence):</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        X_char[i, t] <span class="op">=</span> char_to_index[char]</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    y_char[i, char_to_index[next_chars[i]]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the first position of each</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_char[<span class="dv">0</span>], y_char[<span class="dv">0</span>], sep<span class="op">=</span><span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[ 2. 19. 21.  5.  7. 16.  2.  1.  3. 10.  2.  1. 21.  2.  7. 12.  3. 14.
  1.  4. 13.  1.  2. 23.  2.  5. 14.  1.  8. 12.  9.  5.  6.  8.  2. 28.
  1.  6.  3. 25.  8.  1.  7.  1. 13.  5.  2.  8. 10.  1.]
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</code></pre>
</div>
</div>
<section id="train-val-split" class="cell markdown">
<h5>Train, Val split</h5>
</section>
<div class="cell code" data-execution_count="321">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>X_char_train, X_char_val, y_char_train, y_char_val <span class="op">=</span> train_test_split(X_char, y_char, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Size of X_char_train: </span><span class="sc">{</span><span class="bu">len</span>(X_char_train)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Size of X_char_val: </span><span class="sc">{</span><span class="bu">len</span>(X_char_val)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Size of X_char_train: 15205
Size of X_char_val: 3802
</code></pre>
</div>
</div>
<section id="modelling" class="cell markdown">
<h1><strong>MODELLING</strong></h1>
</section>
<section id="evaluation-metrics" class="cell markdown">
<h3><strong>EVALUATION METRICS</strong></h3>
</section>
<section id="text-output-analysis" class="cell markdown">
<h4>Text Output Analysis</h4>
<ol>
<li><strong>Relevance</strong>:
<ul>
<li>Is the content on-topic and related to the subject matter?</li>
</ul></li>
<li><strong>Coherence and Flow</strong>:
<ul>
<li>Does the text have a logical flow of ideas and sentences?</li>
<li>Are there any abrupt transitions or disjointed sentences?</li>
</ul></li>
<li><strong>Grammar and Spelling</strong>:
<ul>
<li>Are there any grammatical errors or misspelled words in the
generated text?</li>
<li>Does the text use proper punctuation and capitalization?</li>
</ul></li>
<li><strong>Consistency</strong>:
<ul>
<li>Is the style and tone of the generated text consistent
throughout?</li>
<li>Are there any sudden shifts in perspective or language?</li>
</ul></li>
<li><strong>Originality</strong>:
<ul>
<li>Does the generated text contain any plagiarized content or closely
resemble existing material?</li>
<li>Is the content sufficiently original and not copied from the
training data or external sources?</li>
</ul></li>
<li><strong>Clarity</strong>:
<ul>
<li>Is the text clear and easily understandable?</li>
<li>Are there any ambiguities or confusing passages?</li>
</ul></li>
</ol>
</section>
<div class="cell markdown">
<p>Required input texts</p>
</div>
<div class="cell code" data-execution_count="456">
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>seed_texts <span class="op">=</span> [</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;embrace each day&quot;</span>,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;radiate some&quot;</span>,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;believe that&quot;</span>,</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;life&#39;s actual purpose is&quot;</span>,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;dance through each and every&quot;</span>,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;let your time and energy&quot;</span>,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;every person is&quot;</span>,</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;our country Singapore is&quot;</span>,</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;planet earth is&quot;</span>,</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;morning and evening would make it&quot;</span>,</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Human generated complete texts</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>seed_texts_references <span class="op">=</span> [</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; with a heart full of gratitude and enthusiasm, and make the most of every opportunity that comes your way.&quot;</span>,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; positive vibes to brighten the lives of those around you, spreading happiness and joy wherever you go.&quot;</span>,</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; the universe conspires to help you achieve your dreams, and with determination, you can overcome any obstacle.&quot;</span>,</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; to explore the beauty of existence, connect with others, and leave a lasting, positive impact on the world.&quot;</span>,</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; moment of your life as if it were a celebration, savoring the rhythm of the journey.&quot;</span>,</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; be invested in things that align with your passions and values, for they are the true treasures of life.&quot;</span>,</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; a complex tapestry of experiences, emotions, and dreams, deserving of empathy, understanding, and respect.&quot;</span>,</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; a vibrant melting pot of cultures, innovation, and progress, making it a remarkable place to call home.&quot;</span>,</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; a precious oasis in the vast cosmos, and it&#39;s our responsibility to protect and preserve its natural wonders.&quot;</span>,</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; easier to establish a consistent routine, allowing you to accomplish your daily goals and find balance in life.&quot;</span></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div>
</div>
<div class="cell markdown">
<p>Probability scaling</p>
<p><strong>Temperature</strong></p>
<blockquote>
<p>a hyperparameter that can be used to control the randomness and
creativity of the generated text in a generative language model. Higher
temperature = more random and creative. Low temperature = less random
and standardized.</p>
</blockquote>
</div>
<div class="cell code" data-execution_count="323">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>np.seterr(divide <span class="op">=</span> <span class="st">&#39;ignore&#39;</span>) </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scale_softmax(softmax_pred, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert predictions to a numpy array and adjust for temperature</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    softmax_pred <span class="op">=</span> np.asarray(softmax_pred).astype(<span class="st">&#39;float64&#39;</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    softmax_pred <span class="op">=</span> np.log(softmax_pred) <span class="op">/</span> temperature</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    exp_softmax_pred <span class="op">=</span> np.exp(softmax_pred)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    softmax_pred <span class="op">=</span> exp_softmax_pred <span class="op">/</span> np.<span class="bu">sum</span>(exp_softmax_pred)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample a prediction from the probability distribution</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    probas <span class="op">=</span> np.random.multinomial(<span class="dv">1</span>, softmax_pred, <span class="dv">1</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmax(probas)</span></code></pre></div>
</div>
<section id="next-word-prediction" class="cell markdown">
<h4><strong>Next word prediction</strong></h4>
</section>
<div class="cell code" data-execution_count="324">
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_next_words(input_text, model, max_sequence_len, no_of_words<span class="op">=</span><span class="dv">10</span>, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer.texts_to_sequences([input_text])[<span class="dv">0</span>] <span class="co"># Convert your input text into integers.</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    predicted_words <span class="op">=</span> []</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(no_of_words):</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        tokens_padded <span class="op">=</span> pad_sequences([tokens], maxlen<span class="op">=</span>max_sequence_len<span class="op">-</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>) <span class="co"># Pad your sequence.</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model.predict(tokens_padded, verbose<span class="op">=</span><span class="dv">0</span>) <span class="co"># Get the prediction as softmax probabilities.</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply temperature scaling and get the new prediction</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        scaled_prediction <span class="op">=</span> scale_softmax(predictions[<span class="dv">0</span>], temperature)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        predicted_word <span class="op">=</span> tokenizer.sequences_to_texts([[scaled_prediction]])[<span class="dv">0</span>] <span class="co"># Convert scaled prediction to word.</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>        predicted_words.append(predicted_word)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update tokens with the new prediction for the next iteration.</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> tokens <span class="op">+</span> [scaled_prediction]</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(tokens) <span class="op">&gt;</span> max_sequence_len<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> tokens[<span class="dv">1</span>:] <span class="co"># Ensure tokens length is within the max_sequence_len.</span></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(predicted_words)</span></code></pre></div>
</div>
<section id="next-character-prediction" class="cell markdown">
<h4><strong>Next character prediction</strong></h4>
</section>
<div class="cell code" data-execution_count="325">
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_next_char(model, input_text, temperature<span class="op">=</span><span class="fl">1.0</span>, no_of_chars<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert seed_text sentences into chars for input</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sentence_to_chars(input_sentence):</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>        data_char <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> char <span class="kw">in</span> input_sentence.lower():</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>            data_char <span class="op">+=</span> char</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> data_char</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> sentence_to_chars(input_text)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">+=</span> sentence</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(no_of_chars):  <span class="co"># number of characters to generate</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        x_pred <span class="op">=</span> np.zeros((<span class="dv">1</span>, chars_window))</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t, char <span class="kw">in</span> <span class="bu">enumerate</span>(sentence):</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>            x_pred[<span class="dv">0</span>, t] <span class="op">=</span> char_tokenizer.word_index.get(</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>                char, <span class="dv">0</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>            )  <span class="co"># Map char to index with default for unknown chars</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model.predict(x_pred, verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>        next_index <span class="op">=</span> scale_softmax(preds, temperature<span class="op">=</span>temperature)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert index back to char</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>        next_char <span class="op">=</span> char_tokenizer.sequences_to_texts([[next_index]])[<span class="dv">0</span>]</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">+=</span> next_char</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update sentence with new character, ensuring its length stays consistent</span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>        sentence <span class="op">=</span> sentence[<span class="dv">1</span>:] <span class="op">+</span> next_char</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated</span></code></pre></div>
</div>
<section id="training-plots" class="cell markdown">
<h4><strong>Training plots</strong></h4>
<p>For evaluating text generation models, we cant use metrics such as
accuracy because we want our model to generate a range of answers and
not get fixated on getting an exact answer each time. We need to
generate sentences and be evaluated by humans, or on the linguistic
level.</p>
</section>
<div class="cell code" data-execution_count="446">
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_history(history):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> history.history[<span class="st">&quot;loss&quot;</span>]</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> history.history[<span class="st">&quot;val_loss&quot;</span>]</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(loss) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss subplot</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, loss, <span class="st">&quot;b-o&quot;</span>, label<span class="op">=</span><span class="st">&quot;Training Loss&quot;</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, val_loss, <span class="st">&quot;g-x&quot;</span>, label<span class="op">=</span><span class="st">&quot;Validation Loss&quot;</span>)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Training and Validation Loss&quot;</span>)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Epoch&quot;</span>)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Loss&quot;</span>)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic x-axis ticks for loss plot</span></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    tick_spacing <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(epochs) <span class="op">//</span> <span class="dv">10</span>)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    plt.xticks(epochs[::tick_spacing])</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    min_val_loss <span class="op">=</span> <span class="bu">min</span>(loss)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>    plt.text(</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(epochs) <span class="op">*</span> <span class="fl">0.8</span>,</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>        min_val_loss <span class="op">*</span> <span class="fl">1.01</span>,</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;Min Loss: </span><span class="sc">{</span>min_val_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>,</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>,</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>all_results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> store_result(modelInfo, linguistic_score, description):</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> modelInfo.history</span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a>    best_val_idx <span class="op">=</span> np.argmin(history[<span class="st">&quot;val_loss&quot;</span>])</span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> {}</span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Model Name&quot;</span>] <span class="op">=</span> modelInfo.model._name</span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Description&quot;</span>] <span class="op">=</span> description</span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Linguistic_score&quot;</span>] <span class="op">=</span> linguistic_score</span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Train Loss&quot;</span>] <span class="op">=</span> history[<span class="st">&quot;loss&quot;</span>][best_val_idx]</span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Val Loss&quot;</span>] <span class="op">=</span> history[<span class="st">&quot;val_loss&quot;</span>][best_val_idx]</span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Epochs&quot;</span>] <span class="op">=</span> <span class="bu">len</span>(history[<span class="st">&quot;loss&quot;</span>])</span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> pd.DataFrame([result])</span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> all_results</span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a>    all_results <span class="op">=</span> pd.concat([all_results, result], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_model(model):</span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model:</span>
<span id="cb49-51"><a href="#cb49-51" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> model</span>
<span id="cb49-52"><a href="#cb49-52" aria-hidden="true" tabindex="-1"></a>    K.backend.clear_session()</span>
<span id="cb49-53"><a href="#cb49-53" aria-hidden="true" tabindex="-1"></a>    tf.compat.v1.reset_default_graph()</span></code></pre></div>
</div>
<section id="callbacks" class="cell markdown">
<h3><strong>Callbacks</strong></h3>
</section>
<div class="cell markdown">
<p><strong>EarlyStopping:</strong></p>
<ul>
<li>Monitors a metric during training and stops the training process if
the metric stops improving. Helps prevent overfitting.</li>
</ul>
<p><strong>ModelCheckpoint:</strong></p>
<ul>
<li>Saves the model's weights at specified intervals during training for
later loading and evaluation.</li>
</ul>
<p><strong>ReduceLROnPlateau:</strong></p>
<ul>
<li>Reduces the learning rate when a metric has stopped improving. Often
used in conjunction with EarlyStopping.</li>
</ul>
</div>
<div class="cell code" data-execution_count="327">
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_checkpoint(<span class="bu">file</span>):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ModelCheckpoint(</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>        filepath<span class="op">=</span><span class="ss">f&quot;./best weights/</span><span class="sc">{</span><span class="bu">file</span><span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>,</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>early_stop <span class="op">=</span> EarlyStopping(</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>, patience<span class="op">=</span><span class="dv">10</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>, mode<span class="op">=</span><span class="st">&quot;min&quot;</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>reduce_lr <span class="op">=</span> ReduceLROnPlateau(</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_loss&quot;</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">3</span>, min_lr<span class="op">=</span><span class="fl">0.0000001</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<section id="base-model" class="cell markdown">
<h2><strong>BASE MODEL</strong></h2>
</section>
<div class="cell code" data-execution_count="480">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> base_rnn(vocab_len, input_length<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">&quot;base_RNN&quot;</span>):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    base_model <span class="op">=</span> Sequential(name<span class="op">=</span>name)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    base_model.add(Embedding(input_dim<span class="op">=</span>vocab_len, output_dim<span class="op">=</span><span class="dv">64</span>, input_length<span class="op">=</span>input_length))</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    base_model.add(SimpleRNN(units<span class="op">=</span><span class="dv">64</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    base_model.add(Dense(vocab_len, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    base_model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&quot;adam&quot;</span>, loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> base_model</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>base_rnn_model <span class="op">=</span> base_rnn(VOCABULARY_LEN)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>base_rnn_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;base_RNN&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_4 (Embedding)     (None, None, 64)          76480     
                                                                 
 simple_rnn (SimpleRNN)      (None, 64)                8256      
                                                                 
 dense_5 (Dense)             (None, 1195)              77675     
                                                                 
=================================================================
Total params: 162411 (634.42 KB)
Trainable params: 162411 (634.42 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="329">
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_rnn_model)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>base_rnn_model <span class="op">=</span> base_rnn(VOCABULARY_LEN)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>base_model_history <span class="op">=</span> base_rnn_model.fit(</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    y_train,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">40</span>,</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr],</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/40
220/220 [==============================] - 4s 12ms/step - loss: 5.2610 - val_loss: 4.8497 - lr: 0.0010
Epoch 2/40
220/220 [==============================] - 3s 12ms/step - loss: 4.3051 - val_loss: 4.1154 - lr: 0.0010
Epoch 3/40
220/220 [==============================] - 4s 17ms/step - loss: 3.5901 - val_loss: 3.6497 - lr: 0.0010
Epoch 4/40
220/220 [==============================] - 3s 16ms/step - loss: 3.0435 - val_loss: 3.3988 - lr: 0.0010
Epoch 5/40
220/220 [==============================] - 3s 14ms/step - loss: 2.6631 - val_loss: 3.3466 - lr: 0.0010
Epoch 6/40
220/220 [==============================] - 3s 13ms/step - loss: 2.3637 - val_loss: 3.2293 - lr: 0.0010
Epoch 7/40
220/220 [==============================] - 3s 13ms/step - loss: 2.1032 - val_loss: 3.2868 - lr: 0.0010
Epoch 8/40
220/220 [==============================] - 3s 13ms/step - loss: 1.8804 - val_loss: 3.2798 - lr: 0.0010
Epoch 9/40
220/220 [==============================] - 3s 13ms/step - loss: 1.6750 - val_loss: 3.4630 - lr: 0.0010
Epoch 10/40
220/220 [==============================] - 3s 14ms/step - loss: 1.4394 - val_loss: 3.5103 - lr: 5.0000e-04
Epoch 11/40
220/220 [==============================] - 3s 14ms/step - loss: 1.3354 - val_loss: 3.5957 - lr: 5.0000e-04
Epoch 12/40
220/220 [==============================] - 3s 14ms/step - loss: 1.2466 - val_loss: 3.7344 - lr: 5.0000e-04
Epoch 13/40
220/220 [==============================] - 3s 14ms/step - loss: 1.1336 - val_loss: 3.8139 - lr: 2.5000e-04
Epoch 14/40
220/220 [==============================] - 3s 14ms/step - loss: 1.0942 - val_loss: 3.9146 - lr: 2.5000e-04
Epoch 15/40
220/220 [==============================] - 3s 14ms/step - loss: 1.0603 - val_loss: 3.9294 - lr: 2.5000e-04
Epoch 16/40
220/220 [==============================] - 3s 14ms/step - loss: 1.0020 - val_loss: 4.1035 - lr: 1.2500e-04
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/d53ed006daf9dbc8496e751b1b94f89941427b9b.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="457">
<div class="sourceCode" id="cb55"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>generated_sentences <span class="op">=</span> []</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    generated_sentence <span class="op">=</span> predict_next_words(</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>        text, base_rnn_model, MAX_SEQ_LEN, temperature<span class="op">=</span><span class="dv">2</span>, no_of_words<span class="op">=</span><span class="dv">10</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    generated_sentences.append(generated_sentence)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentence<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: by kindness be the chaos are make this cities bring
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: dreams by it holds lullaby free and across opportunities share
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: act of can enough trail your action lies any blessings
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: find is every inspire ebb flow brighter be a than
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: taken source our go they towards nurtured and marine wisdom
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: renew believe in and brighter hearts forged burdens love true
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: you has every leave start mold is fruits the towards
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: an tribute on the vibrant woven you way progress the
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: the liberation towards filled seeds the canvas of into joyful
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: be your past darkest across restless dreams the spreading possibilities
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>1/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>0/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>9/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>10/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>20/60</strong></td>
</tr>
</tbody>
</table>
</div>
<div class="cell code" data-execution_count="467">
<div class="sourceCode" id="cb57"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>store_result(base_model_history, <span class="dv">25</span>, <span class="st">&#39;Base RNN model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="467">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>base_RNN</td>
      <td>Base RNN model</td>
      <td>25</td>
      <td>2.363711</td>
      <td>3.22929</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="base-model-augmented" class="cell markdown">
<h2><strong>BASE MODEL</strong> (augmented)</h2>
</section>
<div class="cell code" data-execution_count="519">
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_rnn_model)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>base_rnn_model_aug <span class="op">=</span> base_rnn(VOCAB_LEN_AUG)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>base_model_aug_history <span class="op">=</span> base_rnn_model_aug.fit(</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    X_aug,</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    y_aug,</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">60</span>,</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_aug_val, y_aug_val),</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr],</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(base_model_aug_history, description=&quot;augmented&quot;)</span></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_aug_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/60
1232/1232 [==============================] - 17s 12ms/step - loss: 4.6113 - val_loss: 3.4271 - lr: 0.0010
Epoch 2/60
1232/1232 [==============================] - 20s 16ms/step - loss: 3.1171 - val_loss: 2.6889 - lr: 0.0010
Epoch 3/60
1232/1232 [==============================] - 20s 17ms/step - loss: 2.5249 - val_loss: 2.2031 - lr: 0.0010
Epoch 4/60
1232/1232 [==============================] - 20s 16ms/step - loss: 2.0945 - val_loss: 1.8141 - lr: 0.0010
Epoch 5/60
1232/1232 [==============================] - 20s 16ms/step - loss: 1.7615 - val_loss: 1.4968 - lr: 0.0010
Epoch 6/60
1232/1232 [==============================] - 20s 16ms/step - loss: 1.5046 - val_loss: 1.2982 - lr: 0.0010
Epoch 7/60
1232/1232 [==============================] - 20s 16ms/step - loss: 1.3159 - val_loss: 1.1419 - lr: 0.0010
Epoch 8/60
1232/1232 [==============================] - 20s 17ms/step - loss: 1.1760 - val_loss: 1.0090 - lr: 0.0010
Epoch 9/60
1232/1232 [==============================] - 22s 18ms/step - loss: 1.0665 - val_loss: 0.9374 - lr: 0.0010
Epoch 10/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.9850 - val_loss: 0.8453 - lr: 0.0010
Epoch 11/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.9279 - val_loss: 0.8009 - lr: 0.0010
Epoch 12/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.8757 - val_loss: 0.7799 - lr: 0.0010
Epoch 13/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.8367 - val_loss: 0.7212 - lr: 0.0010
Epoch 14/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.8061 - val_loss: 0.6959 - lr: 0.0010
Epoch 15/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.7791 - val_loss: 0.6881 - lr: 0.0010
Epoch 16/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.7585 - val_loss: 0.6628 - lr: 0.0010
Epoch 17/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.7350 - val_loss: 0.6604 - lr: 0.0010
Epoch 18/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.7195 - val_loss: 0.6576 - lr: 0.0010
Epoch 19/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.7097 - val_loss: 0.6184 - lr: 0.0010
Epoch 20/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.6903 - val_loss: 0.6054 - lr: 0.0010
Epoch 21/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.6791 - val_loss: 0.6057 - lr: 0.0010
Epoch 22/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.6703 - val_loss: 0.5984 - lr: 0.0010
Epoch 23/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.6613 - val_loss: 0.5757 - lr: 0.0010
Epoch 24/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.6527 - val_loss: 0.5841 - lr: 0.0010
Epoch 25/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.6452 - val_loss: 0.5729 - lr: 0.0010
Epoch 26/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.6356 - val_loss: 0.5596 - lr: 0.0010
Epoch 27/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.6321 - val_loss: 0.5667 - lr: 0.0010
Epoch 28/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.6253 - val_loss: 0.5618 - lr: 0.0010
Epoch 29/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.6166 - val_loss: 0.5491 - lr: 0.0010
Epoch 30/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.6179 - val_loss: 0.5536 - lr: 0.0010
Epoch 31/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.6096 - val_loss: 0.5305 - lr: 0.0010
Epoch 32/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.6054 - val_loss: 0.5446 - lr: 0.0010
Epoch 33/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.6007 - val_loss: 0.5433 - lr: 0.0010
Epoch 34/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.5964 - val_loss: 0.5389 - lr: 0.0010
Epoch 35/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.5247 - val_loss: 0.4736 - lr: 5.0000e-04
Epoch 36/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.5117 - val_loss: 0.4697 - lr: 5.0000e-04
Epoch 37/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.5075 - val_loss: 0.4674 - lr: 5.0000e-04
Epoch 38/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.5064 - val_loss: 0.4590 - lr: 5.0000e-04
Epoch 39/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.5037 - val_loss: 0.4623 - lr: 5.0000e-04
Epoch 40/60
1232/1232 [==============================] - 22s 18ms/step - loss: 0.5008 - val_loss: 0.4648 - lr: 5.0000e-04
Epoch 41/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4993 - val_loss: 0.4563 - lr: 5.0000e-04
Epoch 42/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4970 - val_loss: 0.4569 - lr: 5.0000e-04
Epoch 43/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4949 - val_loss: 0.4532 - lr: 5.0000e-04
Epoch 44/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.4937 - val_loss: 0.4579 - lr: 5.0000e-04
Epoch 45/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.4929 - val_loss: 0.4487 - lr: 5.0000e-04
Epoch 46/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4942 - val_loss: 0.4459 - lr: 5.0000e-04
Epoch 47/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4883 - val_loss: 0.4451 - lr: 5.0000e-04
Epoch 48/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4875 - val_loss: 0.4478 - lr: 5.0000e-04
Epoch 49/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4853 - val_loss: 0.4506 - lr: 5.0000e-04
Epoch 50/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.4856 - val_loss: 0.4420 - lr: 5.0000e-04
Epoch 51/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.4843 - val_loss: 0.4513 - lr: 5.0000e-04
Epoch 52/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4832 - val_loss: 0.4401 - lr: 5.0000e-04
Epoch 53/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4833 - val_loss: 0.4415 - lr: 5.0000e-04
Epoch 54/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4801 - val_loss: 0.4430 - lr: 5.0000e-04
Epoch 55/60
1232/1232 [==============================] - 21s 17ms/step - loss: 0.4804 - val_loss: 0.4490 - lr: 5.0000e-04
Epoch 56/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.4498 - val_loss: 0.4208 - lr: 2.5000e-04
Epoch 57/60
1232/1232 [==============================] - 20s 16ms/step - loss: 0.4448 - val_loss: 0.4222 - lr: 2.5000e-04
Epoch 58/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.4445 - val_loss: 0.4203 - lr: 2.5000e-04
Epoch 59/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.4441 - val_loss: 0.4159 - lr: 2.5000e-04
Epoch 60/60
1232/1232 [==============================] - 20s 17ms/step - loss: 0.4435 - val_loss: 0.4155 - lr: 2.5000e-04
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/18323381460d542ab243edcd13c24e8bff3e9006.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="468">
<div class="sourceCode" id="cb60"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_words(text, base_rnn_model_aug, MAX_SEQ_LEN)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: faith your planet to the ripples . your tapestry .
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: determination , is let for the reflect  in every
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: &#39;s curiosity be landscapes &#39;s humanity of love and big
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: let when the lullaby . our morning &#39;s pulse .
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: &#39;s solitude you be your speak  . . .
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: of the light be lead your beginnings and  your
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: let for the day sunrise of our morning . .
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: true be change and future anthem . . beacon a
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: a  , a true be the bird in your
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: reality a brings that creating and jurong . most and
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>0/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>0/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>9/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>10/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>19/60</strong></td>
</tr>
</tbody>
</table>
</div>
<div class="cell code" data-execution_count="481">
<div class="sourceCode" id="cb62"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>store_result(base_model_aug_history, <span class="dv">19</span>, <span class="st">&#39;Base Augmented RNN model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="481">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>base_RNN</td>
      <td>Base Augmented RNN model</td>
      <td>19</td>
      <td>0.474447</td>
      <td>0.432215</td>
      <td>60</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>With augmentation, loss managed to decrease by 0.02, which may not be
that significant. But the texts generated with out augmented model
produced way more punctuation, which is what I wanted the quotes to
have. Thus augmentation produced better results, despite its sentences
being less legible.</p>
</div>
<section id="base-model-character-prediction" class="cell markdown">
<h2><strong>BASE MODEL</strong> (character prediction)</h2>
</section>
<div class="cell code" data-execution_count="333">
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_rnn_model)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>base_rnn_char_model <span class="op">=</span> base_rnn(CHAR_N_VOCAB, chars_window)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>base_model_char_history <span class="op">=</span> base_rnn_char_model.fit(X_char, </span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>                                                y_char, </span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>                                                validation_data<span class="op">=</span>(X_char_val, y_char_val),</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>                                                batch_size<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>                                                epochs<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(base_model_char_history, description=&quot;character prediction&quot;)</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_char_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/50
149/149 [==============================] - 5s 24ms/step - loss: 2.9742 - val_loss: 2.6124
Epoch 2/50
149/149 [==============================] - 3s 22ms/step - loss: 2.4136 - val_loss: 2.2428
Epoch 3/50
149/149 [==============================] - 3s 23ms/step - loss: 2.1648 - val_loss: 2.0594
Epoch 4/50
149/149 [==============================] - 3s 22ms/step - loss: 2.0086 - val_loss: 1.9199
Epoch 5/50
149/149 [==============================] - 3s 21ms/step - loss: 1.8952 - val_loss: 1.8370
Epoch 6/50
149/149 [==============================] - 3s 23ms/step - loss: 1.7996 - val_loss: 1.7309
Epoch 7/50
149/149 [==============================] - 3s 23ms/step - loss: 1.7239 - val_loss: 1.6740
Epoch 8/50
149/149 [==============================] - 3s 19ms/step - loss: 1.6633 - val_loss: 1.6135
Epoch 9/50
149/149 [==============================] - 3s 22ms/step - loss: 1.6108 - val_loss: 1.5862
Epoch 10/50
149/149 [==============================] - 3s 22ms/step - loss: 1.5681 - val_loss: 1.5301
Epoch 11/50
149/149 [==============================] - 3s 23ms/step - loss: 1.5331 - val_loss: 1.4870
Epoch 12/50
149/149 [==============================] - 3s 23ms/step - loss: 1.4954 - val_loss: 1.4620
Epoch 13/50
149/149 [==============================] - 3s 22ms/step - loss: 1.4672 - val_loss: 1.4343
Epoch 14/50
149/149 [==============================] - 3s 23ms/step - loss: 1.4437 - val_loss: 1.4020
Epoch 15/50
149/149 [==============================] - 3s 23ms/step - loss: 1.4154 - val_loss: 1.3858
Epoch 16/50
149/149 [==============================] - 3s 22ms/step - loss: 1.3951 - val_loss: 1.3565
Epoch 17/50
149/149 [==============================] - 3s 23ms/step - loss: 1.3757 - val_loss: 1.3339
Epoch 18/50
149/149 [==============================] - 3s 23ms/step - loss: 1.3526 - val_loss: 1.3223
Epoch 19/50
149/149 [==============================] - 3s 23ms/step - loss: 1.3378 - val_loss: 1.2991
Epoch 20/50
149/149 [==============================] - 3s 22ms/step - loss: 1.3131 - val_loss: 1.2711
Epoch 21/50
149/149 [==============================] - 3s 22ms/step - loss: 1.2992 - val_loss: 1.2697
Epoch 22/50
149/149 [==============================] - 3s 23ms/step - loss: 1.2778 - val_loss: 1.2573
Epoch 23/50
149/149 [==============================] - 3s 22ms/step - loss: 1.2639 - val_loss: 1.2359
Epoch 24/50
149/149 [==============================] - 3s 23ms/step - loss: 1.2485 - val_loss: 1.2271
Epoch 25/50
149/149 [==============================] - 3s 23ms/step - loss: 1.2357 - val_loss: 1.2098
Epoch 26/50
149/149 [==============================] - 3s 22ms/step - loss: 1.2226 - val_loss: 1.1802
Epoch 27/50
149/149 [==============================] - 3s 21ms/step - loss: 1.2096 - val_loss: 1.1869
Epoch 28/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1986 - val_loss: 1.1593
Epoch 29/50
149/149 [==============================] - 3s 23ms/step - loss: 1.1865 - val_loss: 1.1517
Epoch 30/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1760 - val_loss: 1.1499
Epoch 31/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1615 - val_loss: 1.1230
Epoch 32/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1559 - val_loss: 1.1148
Epoch 33/50
149/149 [==============================] - 3s 23ms/step - loss: 1.1460 - val_loss: 1.1021
Epoch 34/50
149/149 [==============================] - 3s 23ms/step - loss: 1.1346 - val_loss: 1.0864
Epoch 35/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1261 - val_loss: 1.1065
Epoch 36/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1195 - val_loss: 1.0910
Epoch 37/50
149/149 [==============================] - 3s 23ms/step - loss: 1.1155 - val_loss: 1.0656
Epoch 38/50
149/149 [==============================] - 3s 22ms/step - loss: 1.1032 - val_loss: 1.0615
Epoch 39/50
149/149 [==============================] - 3s 23ms/step - loss: 1.0960 - val_loss: 1.0621
Epoch 40/50
149/149 [==============================] - 3s 23ms/step - loss: 1.0917 - val_loss: 1.0575
Epoch 41/50
149/149 [==============================] - 3s 22ms/step - loss: 1.0851 - val_loss: 1.0458
Epoch 42/50
149/149 [==============================] - 3s 22ms/step - loss: 1.0790 - val_loss: 1.0371
Epoch 43/50
149/149 [==============================] - 3s 22ms/step - loss: 1.0730 - val_loss: 1.0299
Epoch 44/50
149/149 [==============================] - 3s 21ms/step - loss: 1.0674 - val_loss: 1.0285
Epoch 45/50
149/149 [==============================] - 3s 21ms/step - loss: 1.0584 - val_loss: 1.0201
Epoch 46/50
149/149 [==============================] - 3s 22ms/step - loss: 1.0503 - val_loss: 1.0066
Epoch 47/50
149/149 [==============================] - 3s 21ms/step - loss: 1.0502 - val_loss: 1.0171
Epoch 48/50
149/149 [==============================] - 3s 22ms/step - loss: 1.0420 - val_loss: 1.0150
Epoch 49/50
149/149 [==============================] - 3s 23ms/step - loss: 1.0400 - val_loss: 0.9864
Epoch 50/50
149/149 [==============================] - 3s 21ms/step - loss: 1.0301 - val_loss: 1.0043
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/50df170a7651e5133372bd8adab05e3a6ce00298.png" /></p>
</div>
</div>
<section id="text-generation" class="cell markdown">
<h3>Text generation</h3>
</section>
<div class="cell code" data-execution_count="579">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_char(</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>        base_rnn_char_model, text, </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        temperature<span class="op">=</span><span class="fl">2.0</span>, no_of_chars<span class="op">=</span><span class="dv">20</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: embrace each day.,qfjasgtdzde.zasjme
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: radiate someaniblzs&quot;icmlirdfc&#39;bv
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: believe that,rftovy.rpwkr-.iqt&#39;k
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: life&#39;s actual purpose is&quot;iya&#39;t bzf&quot; a eaylxd
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: dance through each and everystwtgtofzcsis-rmmsrs
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: let your time and energymxudp&quot;iosy&quot;.m&#39;xerzv;
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: every person isqhnmtttcr .zyszgpm.r
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: our country singapore issri-ckealltvfy-aflt.
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: planet earth isl&#39;,vsjz.mm.iwlpdbsf.
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: morning and evening would make itrnwuci bebn.ne&#39;whom,
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>0/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>0/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>5/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>10/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>15/60</strong></td>
</tr>
</tbody>
</table>
</div>
<div class="cell code" data-execution_count="482">
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>store_result(base_model_char_history, <span class="dv">15</span>, <span class="st">&#39;Base Character RNN model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="482">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>base_RNN</td>
      <td>Base Character RNN model</td>
      <td>15</td>
      <td>1.039982</td>
      <td>0.986383</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>The model predicts characters that form words that are illegible.
This may be because we are using a RNN with no gates. Predicting by
words removes the risk of incorrect spelling; unless training data
contains mispelled words.</p>
</div>
<section id="comparison-of-gru-and-lstm" class="cell markdown">
<h3><strong>Comparison of GRU and LSTM</strong></h3>
<p><strong>Architecture</strong>:</p>
<ul>
<li><em>LSTM</em>: Three gate layers (input, forget, output gates).</li>
<li><em>GRU</em>: Two gate layers (update and reset gates).</li>
</ul>
<p><strong>Computational Complexity</strong>:</p>
<ul>
<li><em>LSTM</em>: More computationally intensive due to extra gate
layers.</li>
<li><em>GRU</em>: Less computationally intensive, faster to train.</li>
</ul>
<p><strong>Parameter Size</strong>:</p>
<ul>
<li><em>LSTM</em>: Typically has more parameters.</li>
<li><em>GRU</em>: Fewer parameters, advantageous with limited data.</li>
</ul>
<p><strong>Capturing Long-Term Dependencies</strong>:</p>
<ul>
<li><em>LSTM</em>: Effective at capturing long-term dependencies.</li>
<li><em>GRU</em>: Can capture dependencies, slightly less effective with
very long sequences.</li>
</ul>
<p><strong>Training Speed</strong>:</p>
<ul>
<li><em>GRU</em>: Converges faster during training due to simpler
architecture.</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li><em>LSTM</em>: Preferred for tasks with complex, long-range
dependencies (e.g., NLP, speech recognition).</li>
<li><em>GRU</em>: Suitable for tasks prioritizing computational
efficiency or shorter-term dependencies.</li>
</ul>
<p>Because of time and resoure constraint, I need a model that is less
computationally intensive, with fewer parameters and converges faster.
Also during EDA earlier, we concluded that quotes are not that long.
Therefore we do not need a model that can capture very long-term
dependencies.</p>
<p>So I will be using GRU (I will still do a quick comparison with
LSTM)</p>
</section>
<section id="lstm-model" class="cell markdown">
<h2><strong>LSTM MODEL</strong></h2>
</section>
<div class="cell code" data-execution_count="335">
<div class="sourceCode" id="cb68"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_lstm(vocab_len, input_length<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">&quot;lstm&quot;</span>):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build model</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential(name<span class="op">=</span>name)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    model.add(Embedding(input_dim<span class="op">=</span>vocab_len, output_dim<span class="op">=</span><span class="dv">128</span>, input_length<span class="op">=</span>input_length))</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    model.add(LSTM(units<span class="op">=</span><span class="dv">128</span>, return_sequences<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    model.add(LSTM(units<span class="op">=</span><span class="dv">128</span>, return_sequences<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(vocab_len, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>, optimizer<span class="op">=</span><span class="st">&quot;adam&quot;</span>)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>lstm_model <span class="op">=</span> instantiate_lstm(VOCABULARY_LEN)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>lstm_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;lstm&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_1 (Embedding)     (None, None, 128)         152960    
                                                                 
 lstm (LSTM)                 (None, None, 128)         131584    
                                                                 
 lstm_1 (LSTM)               (None, 128)               131584    
                                                                 
 dense_1 (Dense)             (None, 1195)              154155    
                                                                 
=================================================================
Total params: 570283 (2.18 MB)
Trainable params: 570283 (2.18 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="336">
<div class="sourceCode" id="cb70"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>reset_model(lstm_model)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>lstm_model <span class="op">=</span> instantiate_lstm(VOCABULARY_LEN)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>lstm_model_history <span class="op">=</span> lstm_model.fit(</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    X,</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    y,</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr],</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(lstm_model_history)</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>plot_history(lstm_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/50
69/69 [==============================] - 31s 257ms/step - loss: 5.4100 - val_loss: 4.8581 - lr: 0.0010
Epoch 2/50
69/69 [==============================] - 16s 229ms/step - loss: 4.9281 - val_loss: 4.8274 - lr: 0.0010
Epoch 3/50
69/69 [==============================] - 17s 240ms/step - loss: 4.9136 - val_loss: 4.8314 - lr: 0.0010
Epoch 4/50
69/69 [==============================] - 17s 241ms/step - loss: 4.8870 - val_loss: 4.7452 - lr: 0.0010
Epoch 5/50
69/69 [==============================] - 16s 238ms/step - loss: 4.6378 - val_loss: 4.4557 - lr: 0.0010
Epoch 6/50
69/69 [==============================] - 16s 236ms/step - loss: 4.3858 - val_loss: 4.2472 - lr: 0.0010
Epoch 7/50
69/69 [==============================] - 17s 247ms/step - loss: 4.1989 - val_loss: 4.0745 - lr: 0.0010
Epoch 8/50
69/69 [==============================] - 17s 240ms/step - loss: 4.0458 - val_loss: 3.9357 - lr: 0.0010
Epoch 9/50
69/69 [==============================] - 18s 254ms/step - loss: 3.9170 - val_loss: 3.8094 - lr: 0.0010
Epoch 10/50
69/69 [==============================] - 17s 245ms/step - loss: 3.8021 - val_loss: 3.7087 - lr: 0.0010
Epoch 11/50
69/69 [==============================] - 17s 252ms/step - loss: 3.6921 - val_loss: 3.5983 - lr: 0.0010
Epoch 12/50
69/69 [==============================] - 17s 246ms/step - loss: 3.5907 - val_loss: 3.4933 - lr: 0.0010
Epoch 13/50
69/69 [==============================] - 17s 245ms/step - loss: 3.4878 - val_loss: 3.3822 - lr: 0.0010
Epoch 14/50
69/69 [==============================] - 17s 249ms/step - loss: 3.3879 - val_loss: 3.2860 - lr: 0.0010
Epoch 15/50
69/69 [==============================] - 17s 242ms/step - loss: 3.2909 - val_loss: 3.1985 - lr: 0.0010
Epoch 16/50
69/69 [==============================] - 17s 250ms/step - loss: 3.1994 - val_loss: 3.0956 - lr: 0.0010
Epoch 17/50
69/69 [==============================] - 17s 248ms/step - loss: 3.1054 - val_loss: 3.0013 - lr: 0.0010
Epoch 18/50
69/69 [==============================] - 18s 256ms/step - loss: 3.0168 - val_loss: 2.9192 - lr: 0.0010
Epoch 19/50
69/69 [==============================] - 16s 237ms/step - loss: 2.9383 - val_loss: 2.8366 - lr: 0.0010
Epoch 20/50
69/69 [==============================] - 17s 252ms/step - loss: 2.8613 - val_loss: 2.7645 - lr: 0.0010
Epoch 21/50
69/69 [==============================] - 17s 248ms/step - loss: 2.7916 - val_loss: 2.6958 - lr: 0.0010
Epoch 22/50
69/69 [==============================] - 17s 249ms/step - loss: 2.7266 - val_loss: 2.6365 - lr: 0.0010
Epoch 23/50
69/69 [==============================] - 17s 249ms/step - loss: 2.6678 - val_loss: 2.5747 - lr: 0.0010
Epoch 24/50
69/69 [==============================] - 17s 249ms/step - loss: 2.6045 - val_loss: 2.5157 - lr: 0.0010
Epoch 25/50
69/69 [==============================] - 17s 251ms/step - loss: 2.5462 - val_loss: 2.4613 - lr: 0.0010
Epoch 26/50
69/69 [==============================] - 17s 249ms/step - loss: 2.4936 - val_loss: 2.4093 - lr: 0.0010
Epoch 27/50
69/69 [==============================] - 18s 257ms/step - loss: 2.4432 - val_loss: 2.3559 - lr: 0.0010
Epoch 28/50
69/69 [==============================] - 17s 246ms/step - loss: 2.3914 - val_loss: 2.3036 - lr: 0.0010
Epoch 29/50
69/69 [==============================] - 17s 242ms/step - loss: 2.3446 - val_loss: 2.2628 - lr: 0.0010
Epoch 30/50
69/69 [==============================] - 17s 254ms/step - loss: 2.3029 - val_loss: 2.2113 - lr: 0.0010
Epoch 31/50
69/69 [==============================] - 17s 247ms/step - loss: 2.2584 - val_loss: 2.1733 - lr: 0.0010
Epoch 32/50
69/69 [==============================] - 18s 255ms/step - loss: 2.2132 - val_loss: 2.1353 - lr: 0.0010
Epoch 33/50
69/69 [==============================] - 17s 239ms/step - loss: 2.1765 - val_loss: 2.0945 - lr: 0.0010
Epoch 34/50
69/69 [==============================] - 17s 252ms/step - loss: 2.1351 - val_loss: 2.0533 - lr: 0.0010
Epoch 35/50
69/69 [==============================] - 17s 244ms/step - loss: 2.0991 - val_loss: 2.0257 - lr: 0.0010
Epoch 36/50
69/69 [==============================] - 16s 232ms/step - loss: 2.0633 - val_loss: 1.9918 - lr: 0.0010
Epoch 37/50
69/69 [==============================] - 17s 253ms/step - loss: 2.0286 - val_loss: 1.9573 - lr: 0.0010
Epoch 38/50
69/69 [==============================] - 18s 260ms/step - loss: 1.9937 - val_loss: 1.9141 - lr: 0.0010
Epoch 39/50
69/69 [==============================] - 18s 254ms/step - loss: 1.9558 - val_loss: 1.8842 - lr: 0.0010
Epoch 40/50
69/69 [==============================] - 17s 245ms/step - loss: 1.9232 - val_loss: 1.8563 - lr: 0.0010
Epoch 41/50
69/69 [==============================] - 18s 254ms/step - loss: 1.8905 - val_loss: 1.8199 - lr: 0.0010
Epoch 42/50
69/69 [==============================] - 18s 260ms/step - loss: 1.8586 - val_loss: 1.7905 - lr: 0.0010
Epoch 43/50
69/69 [==============================] - 17s 251ms/step - loss: 1.8264 - val_loss: 1.7637 - lr: 0.0010
Epoch 44/50
69/69 [==============================] - 17s 246ms/step - loss: 1.7976 - val_loss: 1.7294 - lr: 0.0010
Epoch 45/50
69/69 [==============================] - 18s 254ms/step - loss: 1.7653 - val_loss: 1.7019 - lr: 0.0010
Epoch 46/50
69/69 [==============================] - 17s 249ms/step - loss: 1.7368 - val_loss: 1.6724 - lr: 0.0010
Epoch 47/50
69/69 [==============================] - 18s 254ms/step - loss: 1.7076 - val_loss: 1.6480 - lr: 0.0010
Epoch 48/50
69/69 [==============================] - 17s 245ms/step - loss: 1.6809 - val_loss: 1.6192 - lr: 0.0010
Epoch 49/50
69/69 [==============================] - 18s 255ms/step - loss: 1.6553 - val_loss: 1.5950 - lr: 0.0010
Epoch 50/50
69/69 [==============================] - 17s 251ms/step - loss: 1.6287 - val_loss: 1.5651 - lr: 0.0010
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/d83717d9f9f3f3f83f13ba677e603362a6251203.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="580">
<div class="sourceCode" id="cb72"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_words(text, lstm_model, MAX_SEQ_LEN)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: is a energy to the journey of progress and every
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: for you are be every corner you leave should many
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: light for a planet welcome the strength to new and
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: rich for you are through for it is a bountiful
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: hold the reminders of your and compassion knows the fills
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: be the armor that soothes biodiversity has every work for
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: a symphony of the life for joy to the boundless
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: a holds to savoring your industry of the beauty of
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: a reflection of gratitude and own more dreams and hawker
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: can canvases of our sense of your wildest creating your
-------------------------------------------------- 

</code></pre>
</div>
</div>
<section id="gru-model" class="cell markdown">
<h2><strong>GRU MODEL</strong></h2>
</section>
<div class="cell code" data-execution_count="338">
<div class="sourceCode" id="cb74"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_gru(vocab_len, input_length<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">&quot;gru&quot;</span>):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build model</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential(name<span class="op">=</span>name)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    model.add(Embedding(input_dim<span class="op">=</span>vocab_len, output_dim<span class="op">=</span><span class="dv">128</span>, input_length<span class="op">=</span>input_length))</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    model.add(GRU(units<span class="op">=</span><span class="dv">128</span>, return_sequences<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    model.add(GRU(units<span class="op">=</span><span class="dv">128</span>, return_sequences<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(vocab_len, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>, optimizer<span class="op">=</span><span class="st">&quot;adam&quot;</span>)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>gru_model <span class="op">=</span> instantiate_gru(VOCABULARY_LEN)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>gru_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;gru&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_1 (Embedding)     (None, None, 128)         152960    
                                                                 
 gru (GRU)                   (None, None, 128)         99072     
                                                                 
 gru_1 (GRU)                 (None, 128)               99072     
                                                                 
 dense_1 (Dense)             (None, 1195)              154155    
                                                                 
=================================================================
Total params: 505259 (1.93 MB)
Trainable params: 505259 (1.93 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="339">
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>reset_model(gru_model)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>gru_model <span class="op">=</span> instantiate_gru(VOCABULARY_LEN)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>gru_model_history <span class="op">=</span> gru_model.fit(</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    X,</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    y,</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_val, y_val),</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr]</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(gru_model_history)</span></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>plot_history(gru_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/50
275/275 [==============================] - 27s 77ms/step - loss: 5.1898 - val_loss: 4.8545 - lr: 0.0010
Epoch 2/50
275/275 [==============================] - 20s 73ms/step - loss: 4.6156 - val_loss: 4.2570 - lr: 0.0010
Epoch 3/50
275/275 [==============================] - 21s 75ms/step - loss: 4.0711 - val_loss: 3.6172 - lr: 0.0010
Epoch 4/50
275/275 [==============================] - 21s 75ms/step - loss: 3.4372 - val_loss: 3.0544 - lr: 0.0010
Epoch 5/50
275/275 [==============================] - 20s 73ms/step - loss: 2.9610 - val_loss: 2.6230 - lr: 0.0010
Epoch 6/50
275/275 [==============================] - 20s 74ms/step - loss: 2.5865 - val_loss: 2.2843 - lr: 0.0010
Epoch 7/50
275/275 [==============================] - 20s 74ms/step - loss: 2.2727 - val_loss: 2.0089 - lr: 0.0010
Epoch 8/50
275/275 [==============================] - 20s 74ms/step - loss: 2.0169 - val_loss: 1.7646 - lr: 0.0010
Epoch 9/50
275/275 [==============================] - 21s 75ms/step - loss: 1.7954 - val_loss: 1.5724 - lr: 0.0010
Epoch 10/50
275/275 [==============================] - 20s 74ms/step - loss: 1.6158 - val_loss: 1.4231 - lr: 0.0010
Epoch 11/50
275/275 [==============================] - 19s 70ms/step - loss: 1.4695 - val_loss: 1.2861 - lr: 0.0010
Epoch 12/50
275/275 [==============================] - 20s 74ms/step - loss: 1.3428 - val_loss: 1.1651 - lr: 0.0010
Epoch 13/50
275/275 [==============================] - 20s 72ms/step - loss: 1.2327 - val_loss: 1.0675 - lr: 0.0010
Epoch 14/50
275/275 [==============================] - 19s 71ms/step - loss: 1.1362 - val_loss: 0.9957 - lr: 0.0010
Epoch 15/50
275/275 [==============================] - 21s 75ms/step - loss: 1.0583 - val_loss: 0.9189 - lr: 0.0010
Epoch 16/50
275/275 [==============================] - 20s 74ms/step - loss: 0.9904 - val_loss: 0.8574 - lr: 0.0010
Epoch 17/50
275/275 [==============================] - 20s 73ms/step - loss: 0.9294 - val_loss: 0.8295 - lr: 0.0010
Epoch 18/50
275/275 [==============================] - 20s 74ms/step - loss: 0.8786 - val_loss: 0.7656 - lr: 0.0010
Epoch 19/50
275/275 [==============================] - 20s 74ms/step - loss: 0.8392 - val_loss: 0.7526 - lr: 0.0010
Epoch 20/50
275/275 [==============================] - 20s 72ms/step - loss: 0.7929 - val_loss: 0.7010 - lr: 0.0010
Epoch 21/50
275/275 [==============================] - 20s 75ms/step - loss: 0.7614 - val_loss: 0.6693 - lr: 0.0010
Epoch 22/50
275/275 [==============================] - 20s 74ms/step - loss: 0.7320 - val_loss: 0.6480 - lr: 0.0010
Epoch 23/50
275/275 [==============================] - 19s 71ms/step - loss: 0.7120 - val_loss: 0.6344 - lr: 0.0010
Epoch 24/50
275/275 [==============================] - 20s 74ms/step - loss: 0.6941 - val_loss: 0.6092 - lr: 0.0010
Epoch 25/50
275/275 [==============================] - 20s 74ms/step - loss: 0.6700 - val_loss: 0.5959 - lr: 0.0010
Epoch 26/50
275/275 [==============================] - 20s 72ms/step - loss: 0.6563 - val_loss: 0.5961 - lr: 0.0010
Epoch 27/50
275/275 [==============================] - 20s 74ms/step - loss: 0.6484 - val_loss: 0.5867 - lr: 0.0010
Epoch 28/50
275/275 [==============================] - 20s 74ms/step - loss: 0.6330 - val_loss: 0.5711 - lr: 0.0010
Epoch 29/50
275/275 [==============================] - 19s 71ms/step - loss: 0.6214 - val_loss: 0.5581 - lr: 0.0010
Epoch 30/50
275/275 [==============================] - 20s 72ms/step - loss: 0.6074 - val_loss: 0.5535 - lr: 0.0010
Epoch 31/50
275/275 [==============================] - 20s 73ms/step - loss: 0.6064 - val_loss: 0.5411 - lr: 0.0010
Epoch 32/50
275/275 [==============================] - 19s 71ms/step - loss: 0.5962 - val_loss: 0.5349 - lr: 0.0010
Epoch 33/50
275/275 [==============================] - 20s 73ms/step - loss: 0.5841 - val_loss: 0.5358 - lr: 0.0010
Epoch 34/50
275/275 [==============================] - 20s 73ms/step - loss: 0.5800 - val_loss: 0.5236 - lr: 0.0010
Epoch 35/50
275/275 [==============================] - 20s 72ms/step - loss: 0.5728 - val_loss: 0.5199 - lr: 0.0010
Epoch 36/50
275/275 [==============================] - 20s 73ms/step - loss: 0.5679 - val_loss: 0.5069 - lr: 0.0010
Epoch 37/50
275/275 [==============================] - 20s 73ms/step - loss: 0.5649 - val_loss: 0.5182 - lr: 0.0010
Epoch 38/50
275/275 [==============================] - 20s 72ms/step - loss: 0.5627 - val_loss: 0.5185 - lr: 0.0010
Epoch 39/50
275/275 [==============================] - 20s 74ms/step - loss: 0.5653 - val_loss: 0.5093 - lr: 0.0010
Epoch 40/50
275/275 [==============================] - 20s 74ms/step - loss: 0.5225 - val_loss: 0.4831 - lr: 5.0000e-04
Epoch 41/50
275/275 [==============================] - 20s 71ms/step - loss: 0.5082 - val_loss: 0.4791 - lr: 5.0000e-04
Epoch 42/50
275/275 [==============================] - 20s 74ms/step - loss: 0.5064 - val_loss: 0.4797 - lr: 5.0000e-04
Epoch 43/50
275/275 [==============================] - 20s 73ms/step - loss: 0.5047 - val_loss: 0.4743 - lr: 5.0000e-04
Epoch 44/50
275/275 [==============================] - 20s 74ms/step - loss: 0.5028 - val_loss: 0.4760 - lr: 5.0000e-04
Epoch 45/50
275/275 [==============================] - 19s 70ms/step - loss: 0.5024 - val_loss: 0.4732 - lr: 5.0000e-04
Epoch 46/50
275/275 [==============================] - 20s 74ms/step - loss: 0.5006 - val_loss: 0.4728 - lr: 5.0000e-04
Epoch 47/50
275/275 [==============================] - 20s 71ms/step - loss: 0.4988 - val_loss: 0.4723 - lr: 5.0000e-04
Epoch 48/50
275/275 [==============================] - 20s 74ms/step - loss: 0.4992 - val_loss: 0.4694 - lr: 5.0000e-04
Epoch 49/50
275/275 [==============================] - 21s 76ms/step - loss: 0.4986 - val_loss: 0.4694 - lr: 5.0000e-04
Epoch 50/50
275/275 [==============================] - 20s 73ms/step - loss: 0.4980 - val_loss: 0.4685 - lr: 5.0000e-04
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/b5281eaa1f1bc9fdbfb719b819118a530afdfc29.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="581">
<div class="sourceCode" id="cb78"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_words(text, gru_model, MAX_SEQ_LEN)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: brings your for you be the only constant in refreshment
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: and let it light up the lives of those around
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: sunrise be a beacon of light for the old for
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: a testament to the beauty of our witness the fiery
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: power to lift you are the wide heartbeats in arid
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: be the salve that soothes wounded wounded the world around
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: carries a story worthy of your of every dreams in
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: a reminder of the hope and spirit in the world
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: a canvas for new more powerful and let the true
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: from hope of the wonder of a fresh start and
-------------------------------------------------- 

</code></pre>
</div>
</div>
<section id="comparing-lstm-to-gru-there-are-a-few-differences"
class="cell markdown">
<h3>Comparing LSTM to GRU, there are a few differences</h3>
<p>Model training for GRU went more smoothly than LSTM; Between epochs
24 to 31, there was a spike in loss for LSTM</p>
<table>
<colgroup>
<col style="width: 35%" />
<col style="width: 14%" />
<col style="width: 35%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>GRU</strong></th>
<th></th>
<th><strong>LSTM</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Criteria</td>
<td>Score</td>
<td>Criteria</td>
<td>Score</td>
</tr>
<tr class="even">
<td>----------------------</td>
<td>---------</td>
<td>----------------------</td>
<td>---------</td>
</tr>
<tr class="odd">
<td>Relevance</td>
<td>5/10</td>
<td>Relevance</td>
<td>3/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>8/10</td>
<td>Coherence and Flow</td>
<td>3/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>5/10</td>
<td>Grammar and Spelling</td>
<td>5/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>10/10</td>
<td>Consistency</td>
<td>10/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>9/10</td>
<td>Originality</td>
<td>9/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>1/10</td>
<td>Clarity</td>
<td>1/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>38/60</strong></td>
<td><strong>Total</strong></td>
<td><strong>31/60</strong></td>
</tr>
</tbody>
</table>
<p>Overall, GRU performed better in our technical analysis and language
analysis. Thus <strong>we will be focusing on GRU</strong>.</p>
</section>
<div class="cell code" data-execution_count="483">
<div class="sourceCode" id="cb80"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>store_result(lstm_model_history, <span class="dv">31</span>, <span class="st">&#39;LSTM model&#39;</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>store_result(gru_model_history, <span class="dv">38</span>, <span class="st">&#39;GRU model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="483">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>gru</td>
      <td>GRU model</td>
      <td>38</td>
      <td>0.497999</td>
      <td>0.468535</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="gru-model-augmented" class="cell markdown">
<h2><strong>GRU MODEL</strong> (augmented)</h2>
</section>
<div class="cell code" data-execution_count="342">
<div class="sourceCode" id="cb81"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>reset_model(gru_model)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>gru_model_aug <span class="op">=</span> instantiate_gru(VOCAB_LEN_AUG)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>gru_model_aug_history <span class="op">=</span> gru_model_aug.fit(</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    X_aug,</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    y_aug,</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_aug_val, y_aug_val),</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">40</span>,</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr]</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(gru_model_aug_history)</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>plot_history(gru_model_aug_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/40
308/308 [==============================] - 96s 294ms/step - loss: 5.5541 - val_loss: 5.3072 - lr: 0.0010
Epoch 2/40
308/308 [==============================] - 87s 283ms/step - loss: 5.3460 - val_loss: 5.0951 - lr: 0.0010
Epoch 3/40
308/308 [==============================] - 84s 272ms/step - loss: 4.9619 - val_loss: 4.5789 - lr: 0.0010
Epoch 4/40
308/308 [==============================] - 80s 260ms/step - loss: 4.0524 - val_loss: 3.4212 - lr: 0.0010
Epoch 5/40
308/308 [==============================] - 82s 268ms/step - loss: 3.1848 - val_loss: 2.7635 - lr: 0.0010
Epoch 6/40
308/308 [==============================] - 88s 284ms/step - loss: 2.6606 - val_loss: 2.3352 - lr: 0.0010
Epoch 7/40
308/308 [==============================] - 84s 272ms/step - loss: 2.2829 - val_loss: 2.0137 - lr: 0.0010
Epoch 8/40
308/308 [==============================] - 82s 265ms/step - loss: 1.9942 - val_loss: 1.7671 - lr: 0.0010
Epoch 9/40
308/308 [==============================] - 81s 263ms/step - loss: 1.7676 - val_loss: 1.5744 - lr: 0.0010
Epoch 10/40
308/308 [==============================] - 80s 259ms/step - loss: 1.5866 - val_loss: 1.4010 - lr: 0.0010
Epoch 11/40
308/308 [==============================] - 78s 254ms/step - loss: 1.4365 - val_loss: 1.2745 - lr: 0.0010
Epoch 12/40
308/308 [==============================] - 78s 254ms/step - loss: 1.3125 - val_loss: 1.1691 - lr: 0.0010
Epoch 13/40
308/308 [==============================] - 77s 251ms/step - loss: 1.2076 - val_loss: 1.0729 - lr: 0.0010
Epoch 14/40
308/308 [==============================] - 76s 248ms/step - loss: 1.1204 - val_loss: 1.0002 - lr: 0.0010
Epoch 15/40
308/308 [==============================] - 80s 259ms/step - loss: 1.0482 - val_loss: 0.9352 - lr: 0.0010
Epoch 16/40
308/308 [==============================] - 79s 257ms/step - loss: 0.9854 - val_loss: 0.8781 - lr: 0.0010
Epoch 17/40
308/308 [==============================] - 79s 255ms/step - loss: 0.9313 - val_loss: 0.8289 - lr: 0.0010
Epoch 18/40
308/308 [==============================] - 80s 261ms/step - loss: 0.8872 - val_loss: 0.7932 - lr: 0.0010
Epoch 19/40
308/308 [==============================] - 80s 258ms/step - loss: 0.8471 - val_loss: 0.7562 - lr: 0.0010
Epoch 20/40
308/308 [==============================] - 82s 267ms/step - loss: 0.8126 - val_loss: 0.7275 - lr: 0.0010
Epoch 21/40
308/308 [==============================] - 80s 261ms/step - loss: 0.7826 - val_loss: 0.6986 - lr: 0.0010
Epoch 22/40
308/308 [==============================] - 80s 259ms/step - loss: 0.7566 - val_loss: 0.6711 - lr: 0.0010
Epoch 23/40
308/308 [==============================] - 80s 261ms/step - loss: 0.7327 - val_loss: 0.6530 - lr: 0.0010
Epoch 24/40
308/308 [==============================] - 78s 254ms/step - loss: 0.7122 - val_loss: 0.6371 - lr: 0.0010
Epoch 25/40
308/308 [==============================] - 82s 265ms/step - loss: 0.6942 - val_loss: 0.6204 - lr: 0.0010
Epoch 26/40
308/308 [==============================] - 83s 269ms/step - loss: 0.6777 - val_loss: 0.6061 - lr: 0.0010
Epoch 27/40
308/308 [==============================] - 75s 244ms/step - loss: 0.6627 - val_loss: 0.5951 - lr: 0.0010
Epoch 28/40
308/308 [==============================] - 70s 227ms/step - loss: 0.6507 - val_loss: 0.5822 - lr: 0.0010
Epoch 29/40
308/308 [==============================] - 71s 229ms/step - loss: 0.6379 - val_loss: 0.5673 - lr: 0.0010
Epoch 30/40
308/308 [==============================] - 69s 223ms/step - loss: 0.6296 - val_loss: 0.5685 - lr: 0.0010
Epoch 31/40
308/308 [==============================] - 69s 223ms/step - loss: 0.6196 - val_loss: 0.5602 - lr: 0.0010
Epoch 32/40
308/308 [==============================] - 69s 223ms/step - loss: 0.6071 - val_loss: 0.5509 - lr: 0.0010
Epoch 33/40
308/308 [==============================] - 69s 226ms/step - loss: 0.6019 - val_loss: 0.5439 - lr: 0.0010
Epoch 34/40
308/308 [==============================] - 69s 223ms/step - loss: 0.5932 - val_loss: 0.5323 - lr: 0.0010
Epoch 35/40
308/308 [==============================] - 68s 222ms/step - loss: 0.5857 - val_loss: 0.5277 - lr: 0.0010
Epoch 36/40
308/308 [==============================] - 69s 223ms/step - loss: 0.5785 - val_loss: 0.5210 - lr: 0.0010
Epoch 37/40
308/308 [==============================] - 69s 223ms/step - loss: 0.5726 - val_loss: 0.5142 - lr: 0.0010
Epoch 38/40
308/308 [==============================] - 69s 223ms/step - loss: 0.5691 - val_loss: 0.5051 - lr: 0.0010
Epoch 39/40
308/308 [==============================] - 69s 223ms/step - loss: 0.5609 - val_loss: 0.5075 - lr: 0.0010
Epoch 40/40
308/308 [==============================] - 69s 223ms/step - loss: 0.5543 - val_loss: 0.5019 - lr: 0.0010
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/332d01836b7243c93a436387860b783a413fdffe.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>GRU got a higher loss with augmented data (0.55) than non-augmented
data (0.49)</p>
</div>
<div class="cell code" data-execution_count="582">
<div class="sourceCode" id="cb83"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_words(text, gru_model_aug, MAX_SEQ_LEN)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: limits , yourself &#39;s  singapore the rich  wings
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: the chambers of the southeast symphony they paint the harmony
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: blessings your blooms rain is the light be brilliance .
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: your hope &#39;s strength to the peace you nurtured bridge
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: , life knows story hear tranquility be  . .
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: for the resonate colors truly of our morning &#39;s canvas
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: of we , is let for the seeds of a
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: let for the brings of a someone become . love
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: your ; reality the serenity of love and peace .
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: let reality the innovation of every &#39;s become . .
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<p>As expected, our augmented dataset allowed our GRU model to produce
punctuations. However, the sentences generated are alot worse than the
sentences generated without augmentation.</p>
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>1/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>2/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>2/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>10/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>9/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>24/60</strong></td>
</tr>
</tbody>
</table>
<p>This is likely due to the large number of new words introduced from
augmentation. It also gave way to more punctuation examples, allowing
our model to be more confident predicting punctuation, albeit badly.</p>
<p>There seems to be repetition in the punctuation, so let us explore if
a sequence processing model that consists of two GRUs, one taking the
input in a forward direction, and the other in a backwards direction,
helps.</p>
</div>
<div class="cell code" data-execution_count="484">
<div class="sourceCode" id="cb85"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>store_result(gru_model_aug_history, <span class="dv">24</span>, <span class="st">&#39;GRU augmented model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="484">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>gru</td>
      <td>GRU augmented model</td>
      <td>24</td>
      <td>0.554282</td>
      <td>0.501867</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="gru-model-bidirectional" class="cell markdown">
<h2><strong>GRU MODEL</strong> (Bidirectional)</h2>
</section>
<div class="cell code" data-execution_count="344">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bidirectional_gru_model(vocab_len, input_length<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">&quot;bi-gru&quot;</span>):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential([</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>        Embedding(input_dim<span class="op">=</span>vocab_len, output_dim<span class="op">=</span><span class="dv">128</span>, input_length<span class="op">=</span>input_length),</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>        Bidirectional(GRU(units<span class="op">=</span><span class="dv">64</span>, return_sequences<span class="op">=</span><span class="va">True</span>)),</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>        Bidirectional(GRU(units<span class="op">=</span><span class="dv">64</span>)),</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>        Dense(units<span class="op">=</span>vocab_len, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    ], name<span class="op">=</span>name)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>bidirectional_gru <span class="op">=</span> bidirectional_gru_model(VOCAB_LEN_AUG, MAX_SEQ_LEN_AUG)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>bidirectional_gru.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;bi-gru&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_1 (Embedding)     (None, 43, 128)           305920    
                                                                 
 bidirectional (Bidirection  (None, 43, 128)           74496     
 al)                                                             
                                                                 
 bidirectional_1 (Bidirecti  (None, 128)               74496     
 onal)                                                           
                                                                 
 dense_1 (Dense)             (None, 2390)              308310    
                                                                 
=================================================================
Total params: 763222 (2.91 MB)
Trainable params: 763222 (2.91 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="345">
<div class="sourceCode" id="cb88"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>bidirectional_gru <span class="op">=</span> bidirectional_gru_model(VOCAB_LEN_AUG)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>bidirectional_gru_history <span class="op">=</span> bidirectional_gru.fit(</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    X_aug,</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    y_aug,</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_aug_val, y_aug_val),</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr]</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(bidirectional_gru_history)</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>plot_history(bidirectional_gru_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/50
308/308 [==============================] - 83s 243ms/step - loss: 5.3606 - val_loss: 4.7615 - lr: 0.0010
Epoch 2/50
308/308 [==============================] - 74s 241ms/step - loss: 4.4835 - val_loss: 4.0206 - lr: 0.0010
Epoch 3/50
308/308 [==============================] - 74s 240ms/step - loss: 3.8084 - val_loss: 3.4406 - lr: 0.0010
Epoch 4/50
308/308 [==============================] - 74s 240ms/step - loss: 3.3355 - val_loss: 3.0546 - lr: 0.0010
Epoch 5/50
308/308 [==============================] - 75s 242ms/step - loss: 2.9936 - val_loss: 2.7611 - lr: 0.0010
Epoch 6/50
308/308 [==============================] - 74s 240ms/step - loss: 2.7270 - val_loss: 2.5151 - lr: 0.0010
Epoch 7/50
308/308 [==============================] - 73s 239ms/step - loss: 2.5060 - val_loss: 2.3335 - lr: 0.0010
Epoch 8/50
308/308 [==============================] - 74s 240ms/step - loss: 2.3278 - val_loss: 2.1659 - lr: 0.0010
Epoch 9/50
308/308 [==============================] - 74s 242ms/step - loss: 2.1662 - val_loss: 2.0170 - lr: 0.0010
Epoch 10/50
308/308 [==============================] - 74s 240ms/step - loss: 2.0282 - val_loss: 1.8960 - lr: 0.0010
Epoch 11/50
308/308 [==============================] - 74s 239ms/step - loss: 1.9062 - val_loss: 1.7814 - lr: 0.0010
Epoch 12/50
308/308 [==============================] - 74s 240ms/step - loss: 1.7975 - val_loss: 1.7229 - lr: 0.0010
Epoch 13/50
308/308 [==============================] - 75s 243ms/step - loss: 1.7150 - val_loss: 1.5802 - lr: 0.0010
Epoch 14/50
308/308 [==============================] - 74s 240ms/step - loss: 1.6001 - val_loss: 1.4790 - lr: 0.0010
Epoch 15/50
308/308 [==============================] - 74s 240ms/step - loss: 1.5027 - val_loss: 1.3923 - lr: 0.0010
Epoch 16/50
308/308 [==============================] - 74s 239ms/step - loss: 1.4386 - val_loss: 1.3573 - lr: 0.0010
Epoch 17/50
308/308 [==============================] - 74s 240ms/step - loss: 1.3694 - val_loss: 1.3301 - lr: 0.0010
Epoch 18/50
308/308 [==============================] - 74s 240ms/step - loss: 1.3162 - val_loss: 1.2201 - lr: 0.0010
Epoch 19/50
308/308 [==============================] - 73s 238ms/step - loss: 1.2328 - val_loss: 1.1399 - lr: 0.0010
Epoch 20/50
308/308 [==============================] - 74s 240ms/step - loss: 1.1747 - val_loss: 1.0839 - lr: 0.0010
Epoch 21/50
308/308 [==============================] - 74s 240ms/step - loss: 1.1165 - val_loss: 1.0301 - lr: 0.0010
Epoch 22/50
308/308 [==============================] - 74s 241ms/step - loss: 1.0720 - val_loss: 0.9877 - lr: 0.0010
Epoch 23/50
308/308 [==============================] - 74s 239ms/step - loss: 1.0318 - val_loss: 0.9547 - lr: 0.0010
Epoch 24/50
308/308 [==============================] - 74s 240ms/step - loss: 0.9934 - val_loss: 0.9166 - lr: 0.0010
Epoch 25/50
308/308 [==============================] - 74s 241ms/step - loss: 0.9652 - val_loss: 0.8871 - lr: 0.0010
Epoch 26/50
308/308 [==============================] - 74s 239ms/step - loss: 0.9267 - val_loss: 0.9061 - lr: 0.0010
Epoch 27/50
308/308 [==============================] - 74s 239ms/step - loss: 0.9257 - val_loss: 0.8663 - lr: 0.0010
Epoch 28/50
308/308 [==============================] - 74s 239ms/step - loss: 0.8954 - val_loss: 0.8329 - lr: 0.0010
Epoch 29/50
308/308 [==============================] - 74s 241ms/step - loss: 0.8566 - val_loss: 0.7878 - lr: 0.0010
Epoch 30/50
308/308 [==============================] - 74s 240ms/step - loss: 0.8288 - val_loss: 0.7575 - lr: 0.0010
Epoch 31/50
308/308 [==============================] - 74s 239ms/step - loss: 0.8026 - val_loss: 0.7391 - lr: 0.0010
Epoch 32/50
308/308 [==============================] - 74s 239ms/step - loss: 0.7872 - val_loss: 0.7223 - lr: 0.0010
Epoch 33/50
308/308 [==============================] - 74s 240ms/step - loss: 0.7756 - val_loss: 0.7179 - lr: 0.0010
Epoch 34/50
308/308 [==============================] - 73s 239ms/step - loss: 0.7559 - val_loss: 0.6904 - lr: 0.0010
Epoch 35/50
308/308 [==============================] - 75s 242ms/step - loss: 0.7415 - val_loss: 0.6812 - lr: 0.0010
Epoch 36/50
308/308 [==============================] - 74s 242ms/step - loss: 0.7268 - val_loss: 0.6664 - lr: 0.0010
Epoch 37/50
308/308 [==============================] - 75s 242ms/step - loss: 0.7108 - val_loss: 0.6446 - lr: 0.0010
Epoch 38/50
308/308 [==============================] - 73s 239ms/step - loss: 0.6972 - val_loss: 0.6497 - lr: 0.0010
Epoch 39/50
308/308 [==============================] - 74s 241ms/step - loss: 0.6887 - val_loss: 0.6277 - lr: 0.0010
Epoch 40/50
308/308 [==============================] - 74s 240ms/step - loss: 0.6747 - val_loss: 0.6589 - lr: 0.0010
Epoch 41/50
308/308 [==============================] - 74s 241ms/step - loss: 0.7084 - val_loss: 0.6685 - lr: 0.0010
Epoch 42/50
308/308 [==============================] - 77s 251ms/step - loss: 0.6968 - val_loss: 0.6119 - lr: 0.0010
Epoch 43/50
308/308 [==============================] - 79s 256ms/step - loss: 0.6542 - val_loss: 0.5947 - lr: 0.0010
Epoch 44/50
308/308 [==============================] - 73s 236ms/step - loss: 0.6395 - val_loss: 0.5791 - lr: 0.0010
Epoch 45/50
308/308 [==============================] - 73s 238ms/step - loss: 0.6300 - val_loss: 0.5819 - lr: 0.0010
Epoch 46/50
308/308 [==============================] - 74s 239ms/step - loss: 0.6246 - val_loss: 0.5661 - lr: 0.0010
Epoch 47/50
308/308 [==============================] - 74s 240ms/step - loss: 0.6160 - val_loss: 0.5579 - lr: 0.0010
Epoch 48/50
308/308 [==============================] - 74s 239ms/step - loss: 0.6103 - val_loss: 0.5593 - lr: 0.0010
Epoch 49/50
308/308 [==============================] - 74s 241ms/step - loss: 0.6106 - val_loss: 0.5665 - lr: 0.0010
Epoch 50/50
308/308 [==============================] - 74s 240ms/step - loss: 0.6022 - val_loss: 0.5490 - lr: 0.0010
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/3aa86ad173a730db4ebaf70207764432839a8df2.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="584">
<div class="sourceCode" id="cb90"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_words(text, bidirectional_gru, MAX_SEQ_LEN)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: comes you reality the  of your understanding glorious carry
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: the power of flavors symphony they lighthouse the hope and
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: nurseries for a delicate of nature is the garden of
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: the positivity , universe behind of the attract , a
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: and wounds and watered inspire in ordinary warm . .
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: challenges the innovation be find in courage &#39;s reminder .
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: music carries the its that embrace heart beacon a canvas
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: could , created of yourself &#39;s gratitude for a storms
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: the   for a testament of find and a
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: your gardens universe the fuel that chorus you threads .
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>1/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>3/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>10/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>10/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>0/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>24/60</strong></td>
</tr>
</tbody>
</table>
</div>
<div class="cell code" data-execution_count="485">
<div class="sourceCode" id="cb92"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>store_result(bidirectional_gru_history, <span class="dv">24</span>, <span class="st">&#39;Bi-GRU augmented model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="485">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>bi-gru</td>
      <td>Bi-GRU augmented model</td>
      <td>24</td>
      <td>0.602184</td>
      <td>0.549019</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>Using a Bidirectional GRU seems to reduce the number of repeated
tokens, such as trailing full stops.</p>
</div>
<section id="gru-model-character-prediction" class="cell markdown">
<h2><strong>GRU MODEL</strong> (character prediction)</h2>
</section>
<div class="cell code" data-execution_count="347">
<div class="sourceCode" id="cb93"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>reset_model(gru_model)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>gru_char_model <span class="op">=</span> instantiate_gru(CHAR_N_VOCAB, chars_window)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>gru_char_model_history <span class="op">=</span> gru_char_model.fit(</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    X_char,</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    y_char,</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>(X_char_val, y_char_val),</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr]</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a><span class="co"># store_result(gru_char_model_history)</span></span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>plot_history(gru_char_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/30
149/149 [==============================] - 41s 246ms/step - loss: 2.7640 - val_loss: 2.2572 - lr: 0.0010
Epoch 2/30
149/149 [==============================] - 36s 242ms/step - loss: 2.0701 - val_loss: 1.8748 - lr: 0.0010
Epoch 3/30
149/149 [==============================] - 36s 242ms/step - loss: 1.7789 - val_loss: 1.6192 - lr: 0.0010
Epoch 4/30
149/149 [==============================] - 36s 243ms/step - loss: 1.5564 - val_loss: 1.4246 - lr: 0.0010
Epoch 5/30
149/149 [==============================] - 36s 241ms/step - loss: 1.3905 - val_loss: 1.2686 - lr: 0.0010
Epoch 6/30
149/149 [==============================] - 37s 248ms/step - loss: 1.2566 - val_loss: 1.1410 - lr: 0.0010
Epoch 7/30
149/149 [==============================] - 36s 245ms/step - loss: 1.1463 - val_loss: 1.0299 - lr: 0.0010
Epoch 8/30
149/149 [==============================] - 36s 241ms/step - loss: 1.0481 - val_loss: 0.9496 - lr: 0.0010
Epoch 9/30
149/149 [==============================] - 36s 241ms/step - loss: 0.9673 - val_loss: 0.8648 - lr: 0.0010
Epoch 10/30
149/149 [==============================] - 36s 242ms/step - loss: 0.8893 - val_loss: 0.7978 - lr: 0.0010
Epoch 11/30
149/149 [==============================] - 36s 240ms/step - loss: 0.8218 - val_loss: 0.7244 - lr: 0.0010
Epoch 12/30
149/149 [==============================] - 36s 242ms/step - loss: 0.7594 - val_loss: 0.6594 - lr: 0.0010
Epoch 13/30
149/149 [==============================] - 38s 253ms/step - loss: 0.7028 - val_loss: 0.6162 - lr: 0.0010
Epoch 14/30
149/149 [==============================] - 39s 263ms/step - loss: 0.6525 - val_loss: 0.5506 - lr: 0.0010
Epoch 15/30
149/149 [==============================] - 39s 263ms/step - loss: 0.6001 - val_loss: 0.5171 - lr: 0.0010
Epoch 16/30
149/149 [==============================] - 37s 247ms/step - loss: 0.5559 - val_loss: 0.4836 - lr: 0.0010
Epoch 17/30
149/149 [==============================] - 37s 247ms/step - loss: 0.5138 - val_loss: 0.4441 - lr: 0.0010
Epoch 18/30
149/149 [==============================] - 37s 247ms/step - loss: 0.4747 - val_loss: 0.3945 - lr: 0.0010
Epoch 19/30
149/149 [==============================] - 37s 246ms/step - loss: 0.4388 - val_loss: 0.3627 - lr: 0.0010
Epoch 20/30
149/149 [==============================] - 37s 246ms/step - loss: 0.4041 - val_loss: 0.3322 - lr: 0.0010
Epoch 21/30
149/149 [==============================] - 37s 249ms/step - loss: 0.3704 - val_loss: 0.3072 - lr: 0.0010
Epoch 22/30
149/149 [==============================] - 38s 253ms/step - loss: 0.3413 - val_loss: 0.2761 - lr: 0.0010
Epoch 23/30
149/149 [==============================] - 37s 248ms/step - loss: 0.3181 - val_loss: 0.2550 - lr: 0.0010
Epoch 24/30
149/149 [==============================] - 37s 247ms/step - loss: 0.2905 - val_loss: 0.2355 - lr: 0.0010
Epoch 25/30
149/149 [==============================] - 36s 242ms/step - loss: 0.2674 - val_loss: 0.2186 - lr: 0.0010
Epoch 26/30
149/149 [==============================] - 36s 242ms/step - loss: 0.2468 - val_loss: 0.1976 - lr: 0.0010
Epoch 27/30
149/149 [==============================] - 36s 244ms/step - loss: 0.2239 - val_loss: 0.1801 - lr: 0.0010
Epoch 28/30
149/149 [==============================] - 36s 243ms/step - loss: 0.2095 - val_loss: 0.1659 - lr: 0.0010
Epoch 29/30
149/149 [==============================] - 36s 242ms/step - loss: 0.1942 - val_loss: 0.1574 - lr: 0.0010
Epoch 30/30
149/149 [==============================] - 36s 243ms/step - loss: 0.1821 - val_loss: 0.1424 - lr: 0.0010
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/bfa3fae639b152fd946fc6e741cbc0ec49b41f19.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="585">
<div class="sourceCode" id="cb95"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_char(gru_char_model, text, temperature<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: embrace each dayaaaa  yeg.
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: radiate someeaieaaiyaa
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: believe thateaaaaaaeea
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: life&#39;s actual purpose isiesaaayaai
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: dance through each and everya ei,eeiya
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: let your time and energyyyaeayxeya
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: every person isiaiaiiaaaa
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: our country singapore isa eeaeaa o
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: planet earth iseaiayayeye
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: morning and evening would make itetiaay&#39;.ee
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<p>With character prediction, there are a lot of repeated predictions.
These letters tend to be <code>a</code> and <code>e</code>. These two
letters are the top 2 most used letters in the english language. Our
model predicts them frequently because there are more examples of
<code>a</code> and <code>e</code>, which makes it more "comfortable" to
predict them. We can see difference by adjusting
<code>temperature</code></p>
</div>
<div class="cell code" data-execution_count="586">
<div class="sourceCode" id="cb97"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Temperature = 2</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> predict_next_char(gru_char_model, text, temperature<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: embrace each day.n&quot;ayssbkf
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: radiate some yiv.aep,s
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: believe that .eoo.za;o
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: life&#39;s actual purpose isiaetyg&#39;y-o
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: dance through each and everyy,,za y.ci
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: let your time and energyqivtv,aeag
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: every person isi,aziytiae
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: our country singapore is tttnefayg
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: planet earth ismyeaoaaiwe
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: morning and evening would make ite e, oayy&#39;
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<p>There is less repetition now</p>
</div>
<section id="transformers" class="cell markdown">
<h1><strong>Transformers</strong></h1>
</section>
<div class="cell markdown">
<p>A Transformer is a deep learning model architecture designed for
various natural language processing tasks, such as machine translation
and text generation. It relies on a <strong>self-attention
mechanism</strong> to capture relationships between different words or
elements in a sequence, allowing it to process input data in parallel
and at different positions, making it highly efficient for long
sequences, and getting great performance.</p>
<p><img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="350" height="500"></p>
</div>
<section id="transformer-utils" class="cell markdown">
<h4>Transformer utils</h4>
<ol>
<li>Attention mask</li>
<li>Transformer block</li>
<li>TokenAndPositionEmbedding</li>
<li>Sandardize text (handles punctuation, etc...)</li>
</ol>
</section>
<div class="cell code" data-execution_count="474">
<div class="sourceCode" id="cb99"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="at">@K.saving.register_keras_serializable</span>()</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(Layer):</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>        embed_dim,</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>        num_heads,</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>        ff_dim,</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>        rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff_dim <span class="op">=</span> ff_dim</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rate <span class="op">=</span> rate</span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.att <span class="op">=</span> MultiHeadAttention(num_heads, embed_dim)</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> Sequential(</span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>                Dense(ff_dim, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>),</span>
<span id="cb99-20"><a href="#cb99-20" aria-hidden="true" tabindex="-1"></a>                Dense(embed_dim),</span>
<span id="cb99-21"><a href="#cb99-21" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb99-22"><a href="#cb99-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-23"><a href="#cb99-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layernorm1 <span class="op">=</span> LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb99-24"><a href="#cb99-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layernorm2 <span class="op">=</span> LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb99-25"><a href="#cb99-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> Dropout(rate)</span>
<span id="cb99-26"><a href="#cb99-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> Dropout(rate)</span>
<span id="cb99-27"><a href="#cb99-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-28"><a href="#cb99-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> causal_attention_mask(<span class="va">self</span>, batch_size, n_dest, n_src, dtype):</span>
<span id="cb99-29"><a href="#cb99-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb99-30"><a href="#cb99-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Mask the upper half of the dot product matrix in self attention.</span></span>
<span id="cb99-31"><a href="#cb99-31" aria-hidden="true" tabindex="-1"></a><span class="co">        This prevents flow of information from future tokens to current token.</span></span>
<span id="cb99-32"><a href="#cb99-32" aria-hidden="true" tabindex="-1"></a><span class="co">        1&#39;s in the lower triangle, counting from the lower right corner.</span></span>
<span id="cb99-33"><a href="#cb99-33" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb99-34"><a href="#cb99-34" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> tf.<span class="bu">range</span>(n_dest)[:, <span class="va">None</span>]</span>
<span id="cb99-35"><a href="#cb99-35" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> tf.<span class="bu">range</span>(n_src)</span>
<span id="cb99-36"><a href="#cb99-36" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> i <span class="op">&gt;=</span> j <span class="op">-</span> n_src <span class="op">+</span> n_dest</span>
<span id="cb99-37"><a href="#cb99-37" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> tf.cast(m, dtype)</span>
<span id="cb99-38"><a href="#cb99-38" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> tf.reshape(mask, [<span class="dv">1</span>, n_dest, n_src])</span>
<span id="cb99-39"><a href="#cb99-39" aria-hidden="true" tabindex="-1"></a>        mult <span class="op">=</span> tf.concat(</span>
<span id="cb99-40"><a href="#cb99-40" aria-hidden="true" tabindex="-1"></a>            [tf.expand_dims(batch_size, <span class="op">-</span><span class="dv">1</span>), tf.constant([<span class="dv">1</span>, <span class="dv">1</span>], dtype<span class="op">=</span>tf.int32)], <span class="dv">0</span></span>
<span id="cb99-41"><a href="#cb99-41" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb99-42"><a href="#cb99-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.tile(mask, mult)</span>
<span id="cb99-43"><a href="#cb99-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-44"><a href="#cb99-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb99-45"><a href="#cb99-45" aria-hidden="true" tabindex="-1"></a>        input_shape <span class="op">=</span> tf.shape(inputs)</span>
<span id="cb99-46"><a href="#cb99-46" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> input_shape[<span class="dv">0</span>]</span>
<span id="cb99-47"><a href="#cb99-47" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> input_shape[<span class="dv">1</span>]</span>
<span id="cb99-48"><a href="#cb99-48" aria-hidden="true" tabindex="-1"></a>        causal_mask <span class="op">=</span> <span class="va">self</span>.causal_attention_mask(batch_size, seq_len, seq_len, tf.<span class="bu">bool</span>)</span>
<span id="cb99-49"><a href="#cb99-49" aria-hidden="true" tabindex="-1"></a>        attention_output <span class="op">=</span> <span class="va">self</span>.att(inputs, inputs, attention_mask<span class="op">=</span>causal_mask)</span>
<span id="cb99-50"><a href="#cb99-50" aria-hidden="true" tabindex="-1"></a>        attention_output <span class="op">=</span> <span class="va">self</span>.dropout1(attention_output)</span>
<span id="cb99-51"><a href="#cb99-51" aria-hidden="true" tabindex="-1"></a>        out1 <span class="op">=</span> <span class="va">self</span>.layernorm1(inputs <span class="op">+</span> attention_output)</span>
<span id="cb99-52"><a href="#cb99-52" aria-hidden="true" tabindex="-1"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(out1)</span>
<span id="cb99-53"><a href="#cb99-53" aria-hidden="true" tabindex="-1"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.dropout2(ffn_output)</span>
<span id="cb99-54"><a href="#cb99-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layernorm2(out1 <span class="op">+</span> ffn_output)</span>
<span id="cb99-55"><a href="#cb99-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-56"><a href="#cb99-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb99-57"><a href="#cb99-57" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb99-58"><a href="#cb99-58" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;embed_dim&quot;</span>] <span class="op">=</span> <span class="va">self</span>.embed_dim</span>
<span id="cb99-59"><a href="#cb99-59" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;num_heads&quot;</span>] <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb99-60"><a href="#cb99-60" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;ff_dim&quot;</span>] <span class="op">=</span> <span class="va">self</span>.ff_dim</span>
<span id="cb99-61"><a href="#cb99-61" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;rate&quot;</span>] <span class="op">=</span> <span class="va">self</span>.rate</span>
<span id="cb99-62"><a href="#cb99-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb99-63"><a href="#cb99-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-64"><a href="#cb99-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-65"><a href="#cb99-65" aria-hidden="true" tabindex="-1"></a><span class="at">@K.saving.register_keras_serializable</span>()</span>
<span id="cb99-66"><a href="#cb99-66" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TokenAndPositionEmbedding(Layer):</span>
<span id="cb99-67"><a href="#cb99-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, maxlen, vocab_size, embed_dim, <span class="op">**</span>kwargs):</span>
<span id="cb99-68"><a href="#cb99-68" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb99-69"><a href="#cb99-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maxlen <span class="op">=</span> maxlen</span>
<span id="cb99-70"><a href="#cb99-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb99-71"><a href="#cb99-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb99-72"><a href="#cb99-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_emb <span class="op">=</span> Embedding(input_dim<span class="op">=</span>vocab_size, output_dim<span class="op">=</span>embed_dim)</span>
<span id="cb99-73"><a href="#cb99-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_emb <span class="op">=</span> Embedding(input_dim<span class="op">=</span>maxlen, output_dim<span class="op">=</span>embed_dim)</span>
<span id="cb99-74"><a href="#cb99-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-75"><a href="#cb99-75" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb99-76"><a href="#cb99-76" aria-hidden="true" tabindex="-1"></a>        maxlen <span class="op">=</span> tf.shape(x)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb99-77"><a href="#cb99-77" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> tf.<span class="bu">range</span>(start<span class="op">=</span><span class="dv">0</span>, limit<span class="op">=</span>maxlen, delta<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb99-78"><a href="#cb99-78" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> <span class="va">self</span>.pos_emb(positions)</span>
<span id="cb99-79"><a href="#cb99-79" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_emb(x)</span>
<span id="cb99-80"><a href="#cb99-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> positions</span>
<span id="cb99-81"><a href="#cb99-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-82"><a href="#cb99-82" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb99-83"><a href="#cb99-83" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb99-84"><a href="#cb99-84" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;maxlen&quot;</span>] <span class="op">=</span> <span class="va">self</span>.maxlen</span>
<span id="cb99-85"><a href="#cb99-85" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;vocab_size&quot;</span>] <span class="op">=</span> <span class="va">self</span>.vocab_size</span>
<span id="cb99-86"><a href="#cb99-86" aria-hidden="true" tabindex="-1"></a>        config[<span class="st">&quot;embed_dim&quot;</span>] <span class="op">=</span> <span class="va">self</span>.embed_dim</span>
<span id="cb99-87"><a href="#cb99-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb99-88"><a href="#cb99-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-89"><a href="#cb99-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-90"><a href="#cb99-90" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_standardization(input_string):</span>
<span id="cb99-91"><a href="#cb99-91" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Remove line-break tags and handle punctuation&quot;&quot;&quot;</span></span>
<span id="cb99-92"><a href="#cb99-92" aria-hidden="true" tabindex="-1"></a>    lowercased <span class="op">=</span> tf.strings.lower(input_string)</span>
<span id="cb99-93"><a href="#cb99-93" aria-hidden="true" tabindex="-1"></a>    stripped_html <span class="op">=</span> tf.strings.regex_replace(lowercased, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="st">&quot; &quot;</span>)</span>
<span id="cb99-94"><a href="#cb99-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.strings.regex_replace(stripped_html, <span class="ss">f&quot;([</span><span class="sc">{</span>string<span class="sc">.</span>punctuation<span class="sc">}</span><span class="ss">])&quot;</span>, <span class="vs">r&quot; \1&quot;</span>)</span></code></pre></div>
</div>
<section id="loading-data" class="cell markdown">
<h4>Loading data</h4>
<p>I am using augmented data as transformers requires a large number of
data and it has shown to promote the prediction of punctuation.</p>
</section>
<div class="cell code" data-execution_count="475">
<div class="sourceCode" id="cb100"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>text_ds <span class="op">=</span> tf.data.TextLineDataset(<span class="st">&#39;./text_augmented.csv&#39;</span>)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>text_ds <span class="op">=</span> text_ds.shuffle(buffer_size<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>DATASET_SIZE <span class="op">=</span> pd.read_csv(<span class="st">&#39;./text_augmented.csv&#39;</span>).shape[<span class="dv">0</span>]</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.80</span> <span class="op">*</span> DATASET_SIZE)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> text_ds.take(train_size)</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.batch(batch_size)</span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.20</span> <span class="op">*</span> DATASET_SIZE)</span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> text_ds.skip(train_size)</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> text_ds.take(val_size)</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> val_ds.batch(batch_size)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="476">
<div class="sourceCode" id="cb101"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> VOCAB_LEN_AUG  <span class="co"># Only consider the top 20k words</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>maxlen <span class="op">=</span> MAX_SEQ_LEN_AUG  <span class="co"># Max sequence size</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Embedding size for each token</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Number of attention heads</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>feed_forward_dim <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Hidden layer size in feed forward network inside transformer</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20</span></span></code></pre></div>
</div>
<section id="vectorization" class="cell markdown">
<h4>Vectorization</h4>
</section>
<div class="cell code" data-execution_count="354">
<div class="sourceCode" id="cb102"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a vectorization layer and adapt it to the text</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>vectorize_layer <span class="op">=</span> TextVectorization(</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>custom_standardization,</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>vocab_size <span class="op">-</span> <span class="dv">1</span>,</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">&quot;int&quot;</span>,</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span>maxlen <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>vectorize_layer.adapt(train_ds)</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> vectorize_layer.get_vocabulary()  <span class="co"># To get words back from token indices</span></span></code></pre></div>
</div>
<section id="data-preparation" class="cell markdown">
<h4>Data Preparation</h4>
</section>
<div class="cell code" data-execution_count="429">
<div class="sourceCode" id="cb103"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_lm_inputs_labels(text):</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shift word sequences by 1 position so that the target for position (i) is</span></span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a><span class="co">    word at position (i+1). The model will use all words up till position (i)</span></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a><span class="co">    to predict the next word.</span></span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> tf.expand_dims(text, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>    tokenized_sentences <span class="op">=</span> vectorize_layer(text)</span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tokenized_sentences[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> tokenized_sentences[:, <span class="dv">1</span>:]</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, y</span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a>shift_train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(prepare_lm_inputs_labels)</span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a>train_ds_autotuned <span class="op">=</span> shift_train_ds.prefetch(tf.data.AUTOTUNE)</span></code></pre></div>
</div>
<section id="build-model" class="cell markdown">
<h4>Build model</h4>
</section>
<div class="cell code" data-execution_count="438">
<div class="sourceCode" id="cb104"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model():</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> Input(shape<span class="op">=</span>(maxlen,), dtype<span class="op">=</span>tf.int32)</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>    embedding_layer <span class="op">=</span> TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> embedding_layer(inputs)</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>    transformer_block <span class="op">=</span> TransformerBlock(embed_dim, num_heads, feed_forward_dim)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> transformer_block(x)</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> Dense(vocab_size)(x)</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>[outputs, x])</span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span>[loss_fn, <span class="va">None</span>],</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># No loss and optimization based on word embeddings from transformer block</span></span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<section id="training-transformer" class="cell markdown">
<h3>Training Transformer</h3>
</section>
<div class="cell code" data-execution_count="430">
<div class="sourceCode" id="cb105"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare validation data</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>shift_train_ds <span class="op">=</span> val_ds.<span class="bu">map</span>(prepare_lm_inputs_labels)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>val_ds_autotuned <span class="op">=</span> shift_train_ds.prefetch(tf.data.AUTOTUNE)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="439">
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>transformer_model <span class="op">=</span> create_model()</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>transformer_model_history <span class="op">=</span> transformer_model.fit(</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>    train_ds_autotuned, validation_data<span class="op">=</span>val_ds_autotuned, epochs<span class="op">=</span><span class="dv">20</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>plot_history(transformer_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/20
19/19 [==============================] - 16s 660ms/step - loss: 3.0838 - dense_5_loss: 3.0838 - val_loss: 1.6013 - val_dense_5_loss: 1.6013
Epoch 2/20
19/19 [==============================] - 13s 669ms/step - loss: 1.5619 - dense_5_loss: 1.5619 - val_loss: 1.1916 - val_dense_5_loss: 1.1916
Epoch 3/20
19/19 [==============================] - 12s 611ms/step - loss: 1.2496 - dense_5_loss: 1.2496 - val_loss: 1.0805 - val_dense_5_loss: 1.0805
Epoch 4/20
19/19 [==============================] - 12s 623ms/step - loss: 1.0610 - dense_5_loss: 1.0610 - val_loss: 0.8548 - val_dense_5_loss: 0.8548
Epoch 5/20
19/19 [==============================] - 12s 643ms/step - loss: 0.9072 - dense_5_loss: 0.9072 - val_loss: 0.8069 - val_dense_5_loss: 0.8069
Epoch 6/20
19/19 [==============================] - 12s 653ms/step - loss: 0.8021 - dense_5_loss: 0.8021 - val_loss: 0.6895 - val_dense_5_loss: 0.6895
Epoch 7/20
19/19 [==============================] - 13s 689ms/step - loss: 0.7133 - dense_5_loss: 0.7133 - val_loss: 0.6472 - val_dense_5_loss: 0.6472
Epoch 8/20
19/19 [==============================] - 13s 689ms/step - loss: 0.6562 - dense_5_loss: 0.6562 - val_loss: 0.6167 - val_dense_5_loss: 0.6167
Epoch 9/20
19/19 [==============================] - 14s 722ms/step - loss: 0.6080 - dense_5_loss: 0.6080 - val_loss: 0.5790 - val_dense_5_loss: 0.5790
Epoch 10/20
19/19 [==============================] - 14s 761ms/step - loss: 0.5698 - dense_5_loss: 0.5698 - val_loss: 0.5367 - val_dense_5_loss: 0.5367
Epoch 11/20
19/19 [==============================] - 13s 670ms/step - loss: 0.5434 - dense_5_loss: 0.5434 - val_loss: 0.5374 - val_dense_5_loss: 0.5374
Epoch 12/20
19/19 [==============================] - 13s 671ms/step - loss: 0.5210 - dense_5_loss: 0.5210 - val_loss: 0.5096 - val_dense_5_loss: 0.5096
Epoch 13/20
19/19 [==============================] - 13s 696ms/step - loss: 0.4919 - dense_5_loss: 0.4919 - val_loss: 0.4910 - val_dense_5_loss: 0.4910
Epoch 14/20
19/19 [==============================] - 13s 667ms/step - loss: 0.4733 - dense_5_loss: 0.4733 - val_loss: 0.4663 - val_dense_5_loss: 0.4663
Epoch 15/20
19/19 [==============================] - 13s 689ms/step - loss: 0.4547 - dense_5_loss: 0.4547 - val_loss: 0.4541 - val_dense_5_loss: 0.4541
Epoch 16/20
19/19 [==============================] - 14s 750ms/step - loss: 0.4275 - dense_5_loss: 0.4275 - val_loss: 0.4466 - val_dense_5_loss: 0.4466
Epoch 17/20
19/19 [==============================] - 17s 870ms/step - loss: 0.4143 - dense_5_loss: 0.4143 - val_loss: 0.4520 - val_dense_5_loss: 0.4520
Epoch 18/20
19/19 [==============================] - 16s 834ms/step - loss: 0.4068 - dense_5_loss: 0.4068 - val_loss: 0.4158 - val_dense_5_loss: 0.4158
Epoch 19/20
19/19 [==============================] - 14s 741ms/step - loss: 0.3916 - dense_5_loss: 0.3916 - val_loss: 0.4115 - val_dense_5_loss: 0.4115
Epoch 20/20
19/19 [==============================] - 16s 829ms/step - loss: 0.3775 - dense_5_loss: 0.3775 - val_loss: 0.3722 - val_dense_5_loss: 0.3722
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/776279b3219aaa48ce671878b4a7c6607c2d944d.png" /></p>
</div>
</div>
<section id="text-generation" class="cell markdown">
<h3>Text Generation</h3>
</section>
<div class="cell code" data-execution_count="420">
<div class="sourceCode" id="cb108"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformer_text_generator(start_tokens, transformer_model, max_tokens<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_from(logits):</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>        logits, indices <span class="op">=</span> tf.math.top_k(logits, k<span class="op">=</span><span class="dv">10</span>, <span class="bu">sorted</span><span class="op">=</span><span class="va">True</span>)</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.asarray(indices).astype(<span class="st">&quot;int32&quot;</span>)</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> softmax(tf.expand_dims(logits, <span class="dv">0</span>))[<span class="dv">0</span>]</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> np.asarray(preds).astype(<span class="st">&quot;float32&quot;</span>)</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(indices, p<span class="op">=</span>preds)</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> detokenize(number):</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> vocab[number]</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>    start_tokens <span class="op">=</span> [_ <span class="cf">for</span> _ <span class="kw">in</span> start_tokens]</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>    num_tokens_generated <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>    tokens_generated <span class="op">=</span> []</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> num_tokens_generated <span class="op">&lt;</span> max_tokens:</span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>        pad_len <span class="op">=</span> maxlen <span class="op">-</span> <span class="bu">len</span>(start_tokens)</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a>        sample_index <span class="op">=</span> <span class="bu">len</span>(start_tokens) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pad_len <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> start_tokens[:maxlen]</span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>            sample_index <span class="op">=</span> maxlen <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> pad_len <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> start_tokens <span class="op">+</span> [<span class="dv">0</span>] <span class="op">*</span> pad_len</span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> start_tokens</span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.array([x])</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>        y, _ <span class="op">=</span> transformer_model.predict(x, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>        sample_token <span class="op">=</span> sample_from(y[<span class="dv">0</span>][sample_index])</span>
<span id="cb108-29"><a href="#cb108-29" aria-hidden="true" tabindex="-1"></a>        tokens_generated.append(sample_token)</span>
<span id="cb108-30"><a href="#cb108-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sample_token <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb108-31"><a href="#cb108-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb108-32"><a href="#cb108-32" aria-hidden="true" tabindex="-1"></a>        start_tokens.append(sample_token)</span>
<span id="cb108-33"><a href="#cb108-33" aria-hidden="true" tabindex="-1"></a>        num_tokens_generated <span class="op">=</span> <span class="bu">len</span>(tokens_generated)</span>
<span id="cb108-34"><a href="#cb108-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-35"><a href="#cb108-35" aria-hidden="true" tabindex="-1"></a>    txt <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(</span>
<span id="cb108-36"><a href="#cb108-36" aria-hidden="true" tabindex="-1"></a>        [detokenize(_) <span class="cf">for</span> _ <span class="kw">in</span> tokens_generated]</span>
<span id="cb108-37"><a href="#cb108-37" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb108-38"><a href="#cb108-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-39"><a href="#cb108-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> txt</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="432">
<div class="sourceCode" id="cb109"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>word_to_index <span class="op">=</span> {}</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab):</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>    word_to_index[word] <span class="op">=</span> index</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>    start_tokens <span class="op">=</span> [word_to_index.get(_, <span class="dv">1</span>) <span class="cf">for</span> _ <span class="kw">in</span> text.split()]</span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> transformer_text_generator(start_tokens, transformer_model, max_tokens<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: a gift , a gem , the music of a reminder of nature , a treasure and gratitude , a
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: , and watch how it ignites the way forward .
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: heart and permit it be a sanctuary within you .
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: the fire that connects hearts and farewells .
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: endeavor equal a step we &#39;ve traveled .
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: that be a force that shields your heart .
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: a fresh canvas ; they are the blueprints of your destiny .
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: the potential ; it represent a truly genuine soul .
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: the power to the spirit free from bitterness .
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: personify the beauty in the way for others .
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>4/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>7/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>6/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>10/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>10/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>7/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>44/60</strong></td>
</tr>
</tbody>
</table>
</div>
<div class="cell code" data-execution_count="486">
<div class="sourceCode" id="cb111"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>store_result(transformer_model_history, <span class="dv">44</span>, <span class="st">&#39;Transformer augmented model&#39;</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="486">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>model_1</td>
      <td>Transformer augmented model</td>
      <td>44</td>
      <td>0.377505</td>
      <td>0.37224</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>Despite training for only 20 epochs, the sentences generated by
transformers already performed way better compared to our GRU and other
previous models. This is likely due to the introduction of attention
masks.</p>
</div>
<section id="model-selection" class="cell markdown">
<h1><strong>MODEL SELECTION</strong></h1>
</section>
<div class="cell code" data-execution_count="506">
<div class="sourceCode" id="cb112"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>all_results.to_csv(<span class="st">&#39;RNN_models_results.csv&#39;</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="508">
<div class="sourceCode" id="cb113"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>all_results_loaded <span class="op">=</span> pd.read_csv(<span class="st">&#39;RNN_models_results.csv&#39;</span>)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>all_results_loaded.sort_values(<span class="st">&#39;Linguistic_score&#39;</span>, ascending<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="508">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Linguistic_score</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>model_1</td>
      <td>Transformer augmented model</td>
      <td>44</td>
      <td>0.377505</td>
      <td>0.372240</td>
      <td>20</td>
    </tr>
    <tr>
      <th>4</th>
      <td>gru</td>
      <td>GRU model</td>
      <td>38</td>
      <td>0.497999</td>
      <td>0.468535</td>
      <td>50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>lstm</td>
      <td>LSTM model</td>
      <td>31</td>
      <td>1.628731</td>
      <td>1.565074</td>
      <td>50</td>
    </tr>
    <tr>
      <th>0</th>
      <td>base_RNN</td>
      <td>Base RNN model</td>
      <td>25</td>
      <td>2.363711</td>
      <td>3.229290</td>
      <td>16</td>
    </tr>
    <tr>
      <th>5</th>
      <td>gru</td>
      <td>GRU augmented model</td>
      <td>24</td>
      <td>0.554282</td>
      <td>0.501867</td>
      <td>40</td>
    </tr>
    <tr>
      <th>6</th>
      <td>bi-gru</td>
      <td>Bi-GRU augmented model</td>
      <td>24</td>
      <td>0.602184</td>
      <td>0.549019</td>
      <td>50</td>
    </tr>
    <tr>
      <th>1</th>
      <td>base_RNN</td>
      <td>Base Augmented RNN model</td>
      <td>19</td>
      <td>0.474447</td>
      <td>0.432215</td>
      <td>60</td>
    </tr>
    <tr>
      <th>2</th>
      <td>base_RNN</td>
      <td>Base Character RNN model</td>
      <td>15</td>
      <td>1.039982</td>
      <td>0.986383</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>Our <code>Transformer augmented model</code> got the highest
linguistic score, which is a subjective metric measured by human. But it
also has the lowest train and validation loss. Although a low loss does
not necessarily mean a higher linguistic score as demonstrated by our
<code>GRU augmented model</code></p>
</div>
<section id="model-improvement" class="cell markdown">
<h1><strong>MODEL IMPROVEMENT</strong></h1>
</section>
<section id="transformer-hypermodel" class="cell markdown">
<h2><strong>TRANSFORMER HYPERMODEL</strong></h2>
</section>
<div class="cell code" data-execution_count="440">
<div class="sourceCode" id="cb114"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_model(hp):</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hyperparameters to tune</span></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>    embed_dim <span class="op">=</span> hp.Choice(<span class="st">&#39;embed_dim&#39;</span>, [<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>])</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>    num_heads <span class="op">=</span> hp.Choice(<span class="st">&#39;num_heads&#39;</span>, [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>    feed_forward_dim <span class="op">=</span> hp.Choice(<span class="st">&#39;feed_forward_dim&#39;</span>, [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>])</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> Input(shape<span class="op">=</span>(maxlen,), dtype<span class="op">=</span>tf.int32)</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>    embedding_layer <span class="op">=</span> TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)</span>
<span id="cb114-9"><a href="#cb114-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> embedding_layer(inputs)</span>
<span id="cb114-10"><a href="#cb114-10" aria-hidden="true" tabindex="-1"></a>    transformer_block <span class="op">=</span> TransformerBlock(embed_dim, num_heads, feed_forward_dim)</span>
<span id="cb114-11"><a href="#cb114-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> transformer_block(x)</span>
<span id="cb114-12"><a href="#cb114-12" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> Dense(vocab_size)(x)</span>
<span id="cb114-13"><a href="#cb114-13" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>[outputs, x])</span>
<span id="cb114-14"><a href="#cb114-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-15"><a href="#cb114-15" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb114-16"><a href="#cb114-16" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, loss<span class="op">=</span>[loss_fn, <span class="va">None</span>]) </span>
<span id="cb114-17"><a href="#cb114-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-18"><a href="#cb114-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb114-19"><a href="#cb114-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-20"><a href="#cb114-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Keras Tuner</span></span>
<span id="cb114-21"><a href="#cb114-21" aria-hidden="true" tabindex="-1"></a>tuner <span class="op">=</span> RandomSearch(</span>
<span id="cb114-22"><a href="#cb114-22" aria-hidden="true" tabindex="-1"></a>    build_model,</span>
<span id="cb114-23"><a href="#cb114-23" aria-hidden="true" tabindex="-1"></a>    objective<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>,</span>
<span id="cb114-24"><a href="#cb114-24" aria-hidden="true" tabindex="-1"></a>    max_trials<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb114-25"><a href="#cb114-25" aria-hidden="true" tabindex="-1"></a>    executions_per_trial<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb114-26"><a href="#cb114-26" aria-hidden="true" tabindex="-1"></a>    directory<span class="op">=</span><span class="st">&#39;hyperparameter&#39;</span>,</span>
<span id="cb114-27"><a href="#cb114-27" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">&#39;transformer_hypermodel&#39;</span></span>
<span id="cb114-28"><a href="#cb114-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb114-29"><a href="#cb114-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-30"><a href="#cb114-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have your training data prepared</span></span>
<span id="cb114-31"><a href="#cb114-31" aria-hidden="true" tabindex="-1"></a>tuner.search(train_ds_autotuned, validation_data<span class="op">=</span>val_ds_autotuned, epochs<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Trial 10 Complete [00h 03m 50s]
val_loss: 0.700661857922872

Best val_loss So Far: 0.5387389659881592
Total elapsed time: 00h 45m 22s
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="443">
<div class="sourceCode" id="cb116"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>tf.get_logger().setLevel(<span class="st">&#39;ERROR&#39;</span>)</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>best_hps <span class="op">=</span> tuner.get_best_hyperparameters(num_trials<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> tuner.get_best_models()[<span class="dv">0</span>]</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>best_model_history <span class="op">=</span> best_model.fit(</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>    train_ds_autotuned, </span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>val_ds_autotuned,</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr]</span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true" tabindex="-1"></a>best_model.save(<span class="st">&#39;best_rnn_model.keras&#39;</span>) <span class="co"># Saving best model</span></span>
<span id="cb116-13"><a href="#cb116-13" aria-hidden="true" tabindex="-1"></a>plot_history(best_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/100
19/19 [==============================] - 21s 977ms/step - loss: 0.5330 - dense_2_loss: 0.5330 - val_loss: 0.5198 - val_dense_2_loss: 0.5198 - lr: 0.0010
Epoch 2/100
19/19 [==============================] - 15s 791ms/step - loss: 0.5163 - dense_2_loss: 0.5163 - val_loss: 0.5018 - val_dense_2_loss: 0.5018 - lr: 0.0010
Epoch 3/100
19/19 [==============================] - 14s 718ms/step - loss: 0.5058 - dense_2_loss: 0.5058 - val_loss: 0.5127 - val_dense_2_loss: 0.5127 - lr: 0.0010
Epoch 4/100
19/19 [==============================] - 13s 691ms/step - loss: 0.4936 - dense_2_loss: 0.4936 - val_loss: 0.4800 - val_dense_2_loss: 0.4800 - lr: 0.0010
Epoch 5/100
19/19 [==============================] - 13s 698ms/step - loss: 0.4777 - dense_2_loss: 0.4777 - val_loss: 0.4860 - val_dense_2_loss: 0.4860 - lr: 0.0010
Epoch 6/100
19/19 [==============================] - 13s 673ms/step - loss: 0.4519 - dense_2_loss: 0.4519 - val_loss: 0.4715 - val_dense_2_loss: 0.4715 - lr: 0.0010
Epoch 7/100
19/19 [==============================] - 14s 743ms/step - loss: 0.4252 - dense_2_loss: 0.4252 - val_loss: 0.4321 - val_dense_2_loss: 0.4321 - lr: 0.0010
Epoch 8/100
19/19 [==============================] - 13s 702ms/step - loss: 0.3970 - dense_2_loss: 0.3970 - val_loss: 0.3941 - val_dense_2_loss: 0.3941 - lr: 0.0010
Epoch 9/100
19/19 [==============================] - 13s 706ms/step - loss: 0.3660 - dense_2_loss: 0.3660 - val_loss: 0.3758 - val_dense_2_loss: 0.3758 - lr: 0.0010
Epoch 10/100
19/19 [==============================] - 13s 668ms/step - loss: 0.3473 - dense_2_loss: 0.3473 - val_loss: 0.3598 - val_dense_2_loss: 0.3598 - lr: 0.0010
Epoch 11/100
19/19 [==============================] - 13s 679ms/step - loss: 0.3412 - dense_2_loss: 0.3412 - val_loss: 0.3638 - val_dense_2_loss: 0.3638 - lr: 0.0010
Epoch 12/100
19/19 [==============================] - 13s 696ms/step - loss: 0.3302 - dense_2_loss: 0.3302 - val_loss: 0.3638 - val_dense_2_loss: 0.3638 - lr: 0.0010
Epoch 13/100
19/19 [==============================] - 13s 689ms/step - loss: 0.3198 - dense_2_loss: 0.3198 - val_loss: 0.3195 - val_dense_2_loss: 0.3195 - lr: 0.0010
Epoch 14/100
19/19 [==============================] - 13s 680ms/step - loss: 0.3012 - dense_2_loss: 0.3012 - val_loss: 0.3101 - val_dense_2_loss: 0.3101 - lr: 0.0010
Epoch 15/100
19/19 [==============================] - 13s 699ms/step - loss: 0.2920 - dense_2_loss: 0.2920 - val_loss: 0.2990 - val_dense_2_loss: 0.2990 - lr: 0.0010
Epoch 16/100
19/19 [==============================] - 13s 697ms/step - loss: 0.2728 - dense_2_loss: 0.2728 - val_loss: 0.2951 - val_dense_2_loss: 0.2951 - lr: 0.0010
Epoch 17/100
19/19 [==============================] - 13s 689ms/step - loss: 0.2653 - dense_2_loss: 0.2653 - val_loss: 0.2656 - val_dense_2_loss: 0.2656 - lr: 0.0010
Epoch 18/100
19/19 [==============================] - 13s 661ms/step - loss: 0.2460 - dense_2_loss: 0.2460 - val_loss: 0.2656 - val_dense_2_loss: 0.2656 - lr: 0.0010
Epoch 19/100
19/19 [==============================] - 17s 891ms/step - loss: 0.2428 - dense_2_loss: 0.2428 - val_loss: 0.2749 - val_dense_2_loss: 0.2749 - lr: 0.0010
Epoch 20/100
19/19 [==============================] - 15s 769ms/step - loss: 0.2391 - dense_2_loss: 0.2391 - val_loss: 0.2488 - val_dense_2_loss: 0.2488 - lr: 0.0010
Epoch 21/100
19/19 [==============================] - 19s 1s/step - loss: 0.2362 - dense_2_loss: 0.2362 - val_loss: 0.2393 - val_dense_2_loss: 0.2393 - lr: 0.0010
Epoch 22/100
19/19 [==============================] - 14s 745ms/step - loss: 0.2266 - dense_2_loss: 0.2266 - val_loss: 0.2312 - val_dense_2_loss: 0.2312 - lr: 0.0010
Epoch 23/100
19/19 [==============================] - 14s 747ms/step - loss: 0.2189 - dense_2_loss: 0.2189 - val_loss: 0.2128 - val_dense_2_loss: 0.2128 - lr: 0.0010
Epoch 24/100
19/19 [==============================] - 14s 736ms/step - loss: 0.2103 - dense_2_loss: 0.2103 - val_loss: 0.2080 - val_dense_2_loss: 0.2080 - lr: 0.0010
Epoch 25/100
19/19 [==============================] - 14s 755ms/step - loss: 0.2034 - dense_2_loss: 0.2034 - val_loss: 0.1944 - val_dense_2_loss: 0.1944 - lr: 0.0010
Epoch 26/100
19/19 [==============================] - 14s 728ms/step - loss: 0.1925 - dense_2_loss: 0.1925 - val_loss: 0.1931 - val_dense_2_loss: 0.1931 - lr: 0.0010
Epoch 27/100
19/19 [==============================] - 14s 724ms/step - loss: 0.1876 - dense_2_loss: 0.1876 - val_loss: 0.1901 - val_dense_2_loss: 0.1901 - lr: 0.0010
Epoch 28/100
19/19 [==============================] - 14s 721ms/step - loss: 0.1848 - dense_2_loss: 0.1848 - val_loss: 0.1797 - val_dense_2_loss: 0.1797 - lr: 0.0010
Epoch 29/100
19/19 [==============================] - 14s 718ms/step - loss: 0.1802 - dense_2_loss: 0.1802 - val_loss: 0.1800 - val_dense_2_loss: 0.1800 - lr: 0.0010
Epoch 30/100
19/19 [==============================] - 12s 649ms/step - loss: 0.1801 - dense_2_loss: 0.1801 - val_loss: 0.1839 - val_dense_2_loss: 0.1839 - lr: 0.0010
Epoch 31/100
19/19 [==============================] - 14s 720ms/step - loss: 0.1787 - dense_2_loss: 0.1787 - val_loss: 0.1844 - val_dense_2_loss: 0.1844 - lr: 0.0010
Epoch 32/100
19/19 [==============================] - 14s 713ms/step - loss: 0.1712 - dense_2_loss: 0.1712 - val_loss: 0.1615 - val_dense_2_loss: 0.1615 - lr: 5.0000e-04
Epoch 33/100
19/19 [==============================] - 14s 738ms/step - loss: 0.1584 - dense_2_loss: 0.1584 - val_loss: 0.1517 - val_dense_2_loss: 0.1517 - lr: 5.0000e-04
Epoch 34/100
19/19 [==============================] - 14s 728ms/step - loss: 0.1512 - dense_2_loss: 0.1512 - val_loss: 0.1462 - val_dense_2_loss: 0.1462 - lr: 5.0000e-04
Epoch 35/100
19/19 [==============================] - 13s 702ms/step - loss: 0.1480 - dense_2_loss: 0.1480 - val_loss: 0.1458 - val_dense_2_loss: 0.1458 - lr: 5.0000e-04
Epoch 36/100
19/19 [==============================] - 13s 712ms/step - loss: 0.1458 - dense_2_loss: 0.1458 - val_loss: 0.1438 - val_dense_2_loss: 0.1438 - lr: 5.0000e-04
Epoch 37/100
19/19 [==============================] - 13s 713ms/step - loss: 0.1448 - dense_2_loss: 0.1448 - val_loss: 0.1432 - val_dense_2_loss: 0.1432 - lr: 5.0000e-04
Epoch 38/100
19/19 [==============================] - 14s 767ms/step - loss: 0.1448 - dense_2_loss: 0.1448 - val_loss: 0.1414 - val_dense_2_loss: 0.1414 - lr: 5.0000e-04
Epoch 39/100
19/19 [==============================] - 12s 657ms/step - loss: 0.1430 - dense_2_loss: 0.1430 - val_loss: 0.1411 - val_dense_2_loss: 0.1411 - lr: 5.0000e-04
Epoch 40/100
19/19 [==============================] - 12s 654ms/step - loss: 0.1423 - dense_2_loss: 0.1423 - val_loss: 0.1392 - val_dense_2_loss: 0.1392 - lr: 5.0000e-04
Epoch 41/100
19/19 [==============================] - 13s 677ms/step - loss: 0.1411 - dense_2_loss: 0.1411 - val_loss: 0.1398 - val_dense_2_loss: 0.1398 - lr: 5.0000e-04
Epoch 42/100
19/19 [==============================] - 13s 684ms/step - loss: 0.1410 - dense_2_loss: 0.1410 - val_loss: 0.1376 - val_dense_2_loss: 0.1376 - lr: 5.0000e-04
Epoch 43/100
19/19 [==============================] - 13s 686ms/step - loss: 0.1405 - dense_2_loss: 0.1405 - val_loss: 0.1398 - val_dense_2_loss: 0.1398 - lr: 5.0000e-04
Epoch 44/100
19/19 [==============================] - 13s 671ms/step - loss: 0.1401 - dense_2_loss: 0.1401 - val_loss: 0.1373 - val_dense_2_loss: 0.1373 - lr: 5.0000e-04
Epoch 45/100
19/19 [==============================] - 13s 703ms/step - loss: 0.1399 - dense_2_loss: 0.1399 - val_loss: 0.1379 - val_dense_2_loss: 0.1379 - lr: 5.0000e-04
Epoch 46/100
19/19 [==============================] - 13s 670ms/step - loss: 0.1403 - dense_2_loss: 0.1403 - val_loss: 0.1365 - val_dense_2_loss: 0.1365 - lr: 5.0000e-04
Epoch 47/100
19/19 [==============================] - 13s 665ms/step - loss: 0.1381 - dense_2_loss: 0.1381 - val_loss: 0.1379 - val_dense_2_loss: 0.1379 - lr: 5.0000e-04
Epoch 48/100
19/19 [==============================] - 13s 692ms/step - loss: 0.1382 - dense_2_loss: 0.1382 - val_loss: 0.1357 - val_dense_2_loss: 0.1357 - lr: 5.0000e-04
Epoch 49/100
19/19 [==============================] - 13s 672ms/step - loss: 0.1377 - dense_2_loss: 0.1377 - val_loss: 0.1376 - val_dense_2_loss: 0.1376 - lr: 5.0000e-04
Epoch 50/100
19/19 [==============================] - 13s 688ms/step - loss: 0.1382 - dense_2_loss: 0.1382 - val_loss: 0.1358 - val_dense_2_loss: 0.1358 - lr: 5.0000e-04
Epoch 51/100
19/19 [==============================] - 13s 668ms/step - loss: 0.1367 - dense_2_loss: 0.1367 - val_loss: 0.1363 - val_dense_2_loss: 0.1363 - lr: 5.0000e-04
Epoch 52/100
19/19 [==============================] - 13s 677ms/step - loss: 0.1365 - dense_2_loss: 0.1365 - val_loss: 0.1326 - val_dense_2_loss: 0.1326 - lr: 2.5000e-04
Epoch 53/100
19/19 [==============================] - 13s 666ms/step - loss: 0.1336 - dense_2_loss: 0.1336 - val_loss: 0.1316 - val_dense_2_loss: 0.1316 - lr: 2.5000e-04
Epoch 54/100
19/19 [==============================] - 12s 656ms/step - loss: 0.1332 - dense_2_loss: 0.1332 - val_loss: 0.1314 - val_dense_2_loss: 0.1314 - lr: 2.5000e-04
Epoch 55/100
19/19 [==============================] - 13s 670ms/step - loss: 0.1329 - dense_2_loss: 0.1329 - val_loss: 0.1314 - val_dense_2_loss: 0.1314 - lr: 2.5000e-04
Epoch 56/100
19/19 [==============================] - 12s 659ms/step - loss: 0.1328 - dense_2_loss: 0.1328 - val_loss: 0.1315 - val_dense_2_loss: 0.1315 - lr: 2.5000e-04
Epoch 57/100
19/19 [==============================] - 13s 701ms/step - loss: 0.1319 - dense_2_loss: 0.1319 - val_loss: 0.1309 - val_dense_2_loss: 0.1309 - lr: 2.5000e-04
Epoch 58/100
19/19 [==============================] - 13s 678ms/step - loss: 0.1315 - dense_2_loss: 0.1315 - val_loss: 0.1300 - val_dense_2_loss: 0.1300 - lr: 2.5000e-04
Epoch 59/100
19/19 [==============================] - 13s 680ms/step - loss: 0.1321 - dense_2_loss: 0.1321 - val_loss: 0.1297 - val_dense_2_loss: 0.1297 - lr: 2.5000e-04
Epoch 60/100
19/19 [==============================] - 13s 673ms/step - loss: 0.1314 - dense_2_loss: 0.1314 - val_loss: 0.1302 - val_dense_2_loss: 0.1302 - lr: 2.5000e-04
Epoch 61/100
19/19 [==============================] - 13s 681ms/step - loss: 0.1314 - dense_2_loss: 0.1314 - val_loss: 0.1299 - val_dense_2_loss: 0.1299 - lr: 2.5000e-04
Epoch 62/100
19/19 [==============================] - 13s 677ms/step - loss: 0.1318 - dense_2_loss: 0.1318 - val_loss: 0.1290 - val_dense_2_loss: 0.1290 - lr: 2.5000e-04
Epoch 63/100
19/19 [==============================] - 13s 684ms/step - loss: 0.1311 - dense_2_loss: 0.1311 - val_loss: 0.1291 - val_dense_2_loss: 0.1291 - lr: 2.5000e-04
Epoch 64/100
19/19 [==============================] - 13s 682ms/step - loss: 0.1307 - dense_2_loss: 0.1307 - val_loss: 0.1281 - val_dense_2_loss: 0.1281 - lr: 2.5000e-04
Epoch 65/100
19/19 [==============================] - 13s 674ms/step - loss: 0.1309 - dense_2_loss: 0.1309 - val_loss: 0.1290 - val_dense_2_loss: 0.1290 - lr: 2.5000e-04
Epoch 66/100
19/19 [==============================] - 13s 674ms/step - loss: 0.1301 - dense_2_loss: 0.1301 - val_loss: 0.1289 - val_dense_2_loss: 0.1289 - lr: 2.5000e-04
Epoch 67/100
19/19 [==============================] - 13s 669ms/step - loss: 0.1309 - dense_2_loss: 0.1309 - val_loss: 0.1283 - val_dense_2_loss: 0.1283 - lr: 2.5000e-04
Epoch 68/100
19/19 [==============================] - 13s 676ms/step - loss: 0.1295 - dense_2_loss: 0.1295 - val_loss: 0.1279 - val_dense_2_loss: 0.1279 - lr: 1.2500e-04
Epoch 69/100
19/19 [==============================] - 13s 678ms/step - loss: 0.1291 - dense_2_loss: 0.1291 - val_loss: 0.1283 - val_dense_2_loss: 0.1283 - lr: 1.2500e-04
Epoch 70/100
19/19 [==============================] - 13s 665ms/step - loss: 0.1284 - dense_2_loss: 0.1284 - val_loss: 0.1276 - val_dense_2_loss: 0.1276 - lr: 1.2500e-04
Epoch 71/100
19/19 [==============================] - 13s 671ms/step - loss: 0.1285 - dense_2_loss: 0.1285 - val_loss: 0.1276 - val_dense_2_loss: 0.1276 - lr: 1.2500e-04
Epoch 72/100
19/19 [==============================] - 13s 665ms/step - loss: 0.1291 - dense_2_loss: 0.1291 - val_loss: 0.1275 - val_dense_2_loss: 0.1275 - lr: 1.2500e-04
Epoch 73/100
19/19 [==============================] - 13s 685ms/step - loss: 0.1291 - dense_2_loss: 0.1291 - val_loss: 0.1276 - val_dense_2_loss: 0.1276 - lr: 1.2500e-04
Epoch 74/100
19/19 [==============================] - 13s 676ms/step - loss: 0.1282 - dense_2_loss: 0.1282 - val_loss: 0.1269 - val_dense_2_loss: 0.1269 - lr: 6.2500e-05
Epoch 75/100
19/19 [==============================] - 13s 668ms/step - loss: 0.1281 - dense_2_loss: 0.1281 - val_loss: 0.1274 - val_dense_2_loss: 0.1274 - lr: 6.2500e-05
Epoch 76/100
19/19 [==============================] - 13s 680ms/step - loss: 0.1284 - dense_2_loss: 0.1284 - val_loss: 0.1275 - val_dense_2_loss: 0.1275 - lr: 6.2500e-05
Epoch 77/100
19/19 [==============================] - 13s 666ms/step - loss: 0.1285 - dense_2_loss: 0.1285 - val_loss: 0.1270 - val_dense_2_loss: 0.1270 - lr: 6.2500e-05
Epoch 78/100
19/19 [==============================] - 13s 685ms/step - loss: 0.1290 - dense_2_loss: 0.1290 - val_loss: 0.1271 - val_dense_2_loss: 0.1271 - lr: 3.1250e-05
Epoch 79/100
19/19 [==============================] - 13s 685ms/step - loss: 0.1280 - dense_2_loss: 0.1280 - val_loss: 0.1272 - val_dense_2_loss: 0.1272 - lr: 3.1250e-05
Epoch 80/100
19/19 [==============================] - 13s 667ms/step - loss: 0.1281 - dense_2_loss: 0.1281 - val_loss: 0.1269 - val_dense_2_loss: 0.1269 - lr: 3.1250e-05
Epoch 81/100
19/19 [==============================] - 13s 671ms/step - loss: 0.1279 - dense_2_loss: 0.1279 - val_loss: 0.1268 - val_dense_2_loss: 0.1268 - lr: 1.5625e-05
Epoch 82/100
19/19 [==============================] - 13s 685ms/step - loss: 0.1277 - dense_2_loss: 0.1277 - val_loss: 0.1274 - val_dense_2_loss: 0.1274 - lr: 1.5625e-05
Epoch 83/100
19/19 [==============================] - 13s 675ms/step - loss: 0.1276 - dense_2_loss: 0.1276 - val_loss: 0.1270 - val_dense_2_loss: 0.1270 - lr: 1.5625e-05
Epoch 84/100
19/19 [==============================] - 13s 669ms/step - loss: 0.1278 - dense_2_loss: 0.1278 - val_loss: 0.1272 - val_dense_2_loss: 0.1272 - lr: 1.5625e-05
Epoch 85/100
19/19 [==============================] - 13s 664ms/step - loss: 0.1278 - dense_2_loss: 0.1278 - val_loss: 0.1272 - val_dense_2_loss: 0.1272 - lr: 7.8125e-06
Epoch 86/100
19/19 [==============================] - 14s 754ms/step - loss: 0.1282 - dense_2_loss: 0.1282 - val_loss: 0.1271 - val_dense_2_loss: 0.1271 - lr: 7.8125e-06
Epoch 87/100
19/19 [==============================] - 13s 689ms/step - loss: 0.1275 - dense_2_loss: 0.1275 - val_loss: 0.1273 - val_dense_2_loss: 0.1273 - lr: 7.8125e-06
Epoch 88/100
19/19 [==============================] - 13s 678ms/step - loss: 0.1271 - dense_2_loss: 0.1271 - val_loss: 0.1272 - val_dense_2_loss: 0.1272 - lr: 3.9063e-06
Epoch 89/100
19/19 [==============================] - 13s 677ms/step - loss: 0.1285 - dense_2_loss: 0.1285 - val_loss: 0.1272 - val_dense_2_loss: 0.1272 - lr: 3.9063e-06
Epoch 90/100
19/19 [==============================] - 13s 664ms/step - loss: 0.1278 - dense_2_loss: 0.1278 - val_loss: 0.1270 - val_dense_2_loss: 0.1270 - lr: 3.9063e-06
Epoch 91/100
19/19 [==============================] - 13s 681ms/step - loss: 0.1277 - dense_2_loss: 0.1277 - val_loss: 0.1271 - val_dense_2_loss: 0.1271 - lr: 1.9531e-06
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_9ec0ba30fb704436854ec086e697b20f/49eead03c242158ffe56f71f14fcd7f05c1d1914.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="444">
<div class="sourceCode" id="cb118"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>best_model_loaded <span class="op">=</span> load_model(<span class="st">&#39;best_rnn_model.keras&#39;</span>) <span class="co"># Saving best model</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> seed_texts:</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>    start_tokens <span class="op">=</span> [word_to_index.get(_, <span class="dv">1</span>) <span class="cf">for</span> _ <span class="kw">in</span> text.split()]</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>    generated_sentences <span class="op">=</span> transformer_text_generator(start_tokens, best_model_loaded, max_tokens<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Input text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Generated text: </span><span class="sc">{</span>generated_sentences<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">50</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--------------------------------------------------
Input text: embrace each day
Generated text: brings a new day filled with endless possibilities .
-------------------------------------------------- 

--------------------------------------------------
Input text: radiate some
Generated text: story &#39;s garden , for it is the only constant in life into reality .
-------------------------------------------------- 

--------------------------------------------------
Input text: believe that
Generated text: you will create a world becomes your canvas .
-------------------------------------------------- 

--------------------------------------------------
Input text: life&#39;s actual purpose is
Generated text: the fire that fuels your success .
-------------------------------------------------- 

--------------------------------------------------
Input text: dance through each and every
Generated text: moment into present moment that powers your journey .
-------------------------------------------------- 

--------------------------------------------------
Input text: let your time and energy
Generated text: be the universe will lift you up built .
-------------------------------------------------- 

--------------------------------------------------
Input text: every person is
Generated text: brighter when we share them .
-------------------------------------------------- 

--------------------------------------------------
Input text: our country Singapore is
Generated text: the symphony of laugh and joyfulness .
-------------------------------------------------- 

--------------------------------------------------
Input text: planet earth is
Generated text: the life &#39;s beauty knows no bounds .
-------------------------------------------------- 

--------------------------------------------------
Input text: morning and evening would make it
Generated text: story , a symphony of hope brightness level , the quietness of the heart , for they hold the turning
-------------------------------------------------- 

</code></pre>
</div>
</div>
<div class="cell markdown">
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>6/10</td>
</tr>
<tr class="even">
<td>Coherence and Flow</td>
<td>6/10</td>
</tr>
<tr class="odd">
<td>Grammar and Spelling</td>
<td>6/10</td>
</tr>
<tr class="even">
<td>Consistency</td>
<td>10/10</td>
</tr>
<tr class="odd">
<td>Originality</td>
<td>10/10</td>
</tr>
<tr class="even">
<td>Clarity</td>
<td>7/10</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>45/60</strong></td>
</tr>
</tbody>
</table>
</div>
<section id="further-improvements" class="cell markdown">
<h2>Further improvements</h2>
</section>
<section id="subword-tokenization" class="cell markdown">
<h3>Subword tokenization</h3>
<p>Subword tokenization is a text processing method that breaks words
into smaller units, such as characters or character sequences, to create
a flexible vocabulary for encoding and decoding text. It is particularly
useful for handling languages with complex word forms, dealing with
out-of-vocabulary words, and compressing text efficiently. Common
algorithms for subword tokenization include Byte-Pair Encoding (BPE),
SentencePiece, and WordPiece.</p>
<h4 id="bpe-wordpiece-or-sentence-piece">BPE, WordPiece, or Sentence
Piece?</h4>
<p>BPE, WordPiece, and SentencePiece are subword tokenization techniques
used in natural language processing.</p>
<ul>
<li><p>BPE iteratively merges character or character pairs, is popular,
but may not handle word boundaries well.</p></li>
<li><p>WordPiece, similar to BPE, treats spaces as tokens and is used in
models like BERT.</p></li>
<li><p>SentencePiece is language-agnostic, adaptable, and suitable for
various languages and custom vocabularies.</p></li>
</ul>
</section>
<section id="nlp-metrics" class="cell markdown">
<h3>NLP Metrics</h3>
<p>You can’t train a good model if you don’t have the right evaluation
metric, and you can’t explain your model if you don’t understand the
metric you’re using. This task of generating text renders many commonly
used evaluation metrics useless due to its nature. We want our models to
come up with robust sentences, and not fixed to a "right answer". Thus
metrics such as <code>accuracy</code>, <code>precision</code>,
<code>recall</code>, <code>f1 score</code>, would not be useful even
though classification is used. However there are other metrics that do
fit the nature of text generation. <strong>Perplexity</strong>.</p>
<h4 id="perplexity">Perplexity</h4>
<p>Perplexity calculates the confidence of a model to predict the next
word. This metric works in this case because we want a variety of
answers for the same prompts, but still be coheret.</p>
<p>However, it is hard to make "apples-to-apples" comparisons across
datasets with different context lengths, vocabulary sizes, words,
character-based models, etc, because of how much it is affected by
vocabulary size.</p>
<p><a
href="https://surge-ai.medium.com/evaluating-language-models-an-introduction-to-perplexity-in-nlp-f6019f7fb914"
class="uri">https://surge-ai.medium.com/evaluating-language-models-an-introduction-to-perplexity-in-nlp-f6019f7fb914</a></p>
<p>Other metrics such as BLEU are used for measuring a model's accuracy.
Its use case is mainly on machine translation tasks</p>
</section>
</body>
</html>
