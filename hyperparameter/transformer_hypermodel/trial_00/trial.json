{"trial_id": "00", "hyperparameters": {"space": [{"class_name": "Choice", "config": {"name": "embed_dim", "default": 32, "conditions": [], "values": [32, 64, 128, 256], "ordered": true}}, {"class_name": "Choice", "config": {"name": "num_heads", "default": 1, "conditions": [], "values": [1, 2, 3], "ordered": true}}, {"class_name": "Choice", "config": {"name": "feed_forward_dim", "default": 64, "conditions": [], "values": [64, 128, 256], "ordered": true}}], "values": {"embed_dim": 64, "num_heads": 3, "feed_forward_dim": 256}}, "metrics": {"metrics": {}}, "score": null, "best_step": 0, "status": "FAILED", "message": "Traceback (most recent call last):\n  File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\zzhen\\AppData\\Local\\Temp\\__autograph_generated_file2johpljx.py\", line 15, in tf__train_function\n    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n  File \"C:\\Users\\zzhen\\AppData\\Local\\Temp\\__autograph_generated_filensfyjkvu.py\", line 14, in tf__call\n    attention_output = ag__.converted_call(ag__.ld(self).att, (ag__.ld(inputs), ag__.ld(inputs)), dict(attention_mask=ag__.ld(causal_mask)), fscope)\nValueError: in user code:\n\n    File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\zzhen\\AppData\\Local\\Temp\\__autograph_generated_filensfyjkvu.py\", line 14, in tf__call\n        attention_output = ag__.converted_call(ag__.ld(self).att, (ag__.ld(inputs), ag__.ld(inputs)), dict(attention_mask=ag__.ld(causal_mask)), fscope)\n\n    ValueError: Exception encountered when calling layer 'transformer_block' (type TransformerBlock).\n    \n    in user code:\n    \n        File \"C:\\Users\\zzhen\\AppData\\Local\\Temp\\ipykernel_19824\\3923772477.py\", line 49, in call  *\n            attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        File \"c:\\Users\\zzhen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'query' (type EinsumDense).\n        \n        Shape must be rank 3 but is rank 2\n        \t for 0th input and equation: abc,cde->abde for '{{node model/transformer_block/multi_head_attention/query/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](model/token_and_position_embedding/add, model/transformer_block/multi_head_attention/query/einsum/Einsum/ReadVariableOp)' with input shapes: [?,64], [64,3,64].\n        \n        Call arguments received by layer 'query' (type EinsumDense):\n          \u2022 inputs=tf.Tensor(shape=(None, 64), dtype=float32)\n    \n    \n    Call arguments received by layer 'transformer_block' (type TransformerBlock):\n      \u2022 inputs=tf.Tensor(shape=(None, 64), dtype=float32)\n\n"}